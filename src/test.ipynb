{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.74s/it]\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.68s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, truncation_side='left', padding_side='right')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0},use_flash_attention_2=True)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0})\n",
    "flash_attn_model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0},use_flash_attention_2=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, truncation_side='left', padding_side='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "flash_attn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/tokenization_utils_base.py:2630: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_prompt = \"I've got a lovely bunch of coconuts do do do dooo\"\n",
    "random = \" \".join([\"do\" for _ in range(50)])\n",
    "batched_input_prompt = [\"I've got a lovely bunch of coconuts do do do dooo\", \" \".join([\"do\" for _ in range(50)])]\n",
    "input_prompt_tokenized = tokenizer(input_prompt, return_tensors=\"pt\").to('cuda')\n",
    "input_prompt_padded_tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=\"max_length\", max_length=50).to('cuda')\n",
    "batched_input_prompt_tokenized = tokenizer(batched_input_prompt, return_tensors=\"pt\", padding=True, max_length=50).to('cuda')\n",
    "random_tokenized = tokenizer(random, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "print(input_prompt_tokenized)\n",
    "print(input_prompt_padded_tokenized)\n",
    "print(batched_input_prompt_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Model + Non-Padded Input\n",
      "Loss (no padding): 2.8300979137420654\n",
      "Logits (no padding): tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.3750, -9.8125, -0.3691,  ..., -3.4844, -8.0000, -2.8594],\n",
      "         [-3.7812, -2.7656,  4.0000,  ..., -1.4297, -2.8125, -0.3926],\n",
      "         ...,\n",
      "         [-3.4688, -3.0625,  8.3125,  ...,  0.2559, -2.6875, -2.8125],\n",
      "         [-3.0000, -3.0938,  8.7500,  ...,  0.0806, -2.5156, -2.7656],\n",
      "         [-2.6562, -1.6953,  7.7812,  ..., -0.6484, -3.1562, -2.5000]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids_masked = torch.zeros(input_prompt_padded_tokenized.input_ids.shape, dtype=torch.int64).to('cuda')\n",
    "torch.where(input_prompt_padded_tokenized.input_ids == tokenizer.pad_token_id,\n",
    "            torch.tensor(-100, dtype=torch.int64),\n",
    "            input_prompt_padded_tokenized.input_ids,\n",
    "            out=input_ids_masked)\n",
    "\n",
    "loss1 = model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits1 = model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(f\"Normal Model + Non-Padded Input\")\n",
    "print(f\"Loss (no padding): {loss1}\")\n",
    "print(f\"Logits (no padding): {logits1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attn Model + Non-Padded Input\n",
      "Loss (no padding) with flash attn: 2.8371522426605225\n",
      "Logits (no padding) with flash attn: tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.4375, -9.8750, -0.3555,  ..., -3.5000, -8.0625, -2.9062],\n",
      "         [-3.7812, -2.7812,  4.0625,  ..., -1.3828, -2.7969, -0.3438],\n",
      "         ...,\n",
      "         [-3.4844, -3.0781,  8.3750,  ...,  0.2871, -2.6719, -2.8125],\n",
      "         [-3.0625, -3.1250,  8.6875,  ...,  0.0928, -2.5781, -2.7812],\n",
      "         [-2.6562, -1.6719,  7.7812,  ..., -0.6914, -3.1406, -2.5156]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n",
      "Loss (no padding) with flash attn: 8.834760665893555\n"
     ]
    }
   ],
   "source": [
    "loss1_falsh_attn = flash_attn_model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits1_falsh_attn = flash_attn_model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "loss_random = model(\n",
    "    random_tokenized.input_ids,\n",
    "    attention_mask=random_tokenized.attention_mask,\n",
    "    labels=random_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "print(f\"Flash Attn Model + Non-Padded Input\")\n",
    "print(f\"Loss (no padding) with flash attn: {loss1_falsh_attn}\")\n",
    "print(f\"Logits (no padding) with flash attn: {logits1_falsh_attn}\")\n",
    "print(f\"Loss (no padding) with flash attn: {loss_random + loss1_falsh_attn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 32000])\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'''\n",
      "Normal Model + Padded Input\n",
      "Loss (padding): 8.399846076965332\n",
      "Logits (padding): tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.3750, -9.8125, -0.3691,  ..., -3.4844, -8.0000, -2.8594],\n",
      "         [-3.7812, -2.7656,  4.0000,  ..., -1.4297, -2.8125, -0.3926],\n",
      "         ...,\n",
      "         [-4.9062,  8.0000,  5.2500,  ..., -2.2812, -2.2812, -2.7344],\n",
      "         [-5.4062,  3.4219,  5.3125,  ..., -2.6094, -1.1250, -3.4062],\n",
      "         [-5.3125,  2.5312,  5.9375,  ..., -2.2656, -0.9961, -3.6719]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n",
      "Decoded Logits (padding): []\n"
     ]
    }
   ],
   "source": [
    "loss2 = model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2 = model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(logits2.shape)\n",
    "# decode the logits\n",
    "print(tokenizer.decode(logits2.argmax(dim=-1)[0]))\n",
    "\n",
    "print(f\"Normal Model + Padded Input\")\n",
    "print(f\"Loss (padding): {loss2}\")\n",
    "print(f\"Logits (padding): {logits2}\")\n",
    "print(f\"Decoded Logits (padding): {decoded_logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "Flash Attn Model + Padded Input\n",
      "Loss (padding) with flash attn: 16.86910057067871\n",
      "Logits (padding) with flash attn: tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.4375, -9.8750, -0.3555,  ..., -3.5000, -8.0625, -2.9062],\n",
      "         [-3.7812, -2.7812,  4.0625,  ..., -1.3828, -2.7969, -0.3438],\n",
      "         ...,\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250],\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250],\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss2_falsh_attn = flash_attn_model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2_falsh_attn = flash_attn_model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(tokenizer.decode(logits2_falsh_attn.argmax(dim=-1)[0]))\n",
    "\n",
    "print(f\"Flash Attn Model + Padded Input\")\n",
    "print(f\"Loss (padding) with flash attn: {loss2_falsh_attn}\")\n",
    "print(f\"Logits (padding) with flash attn: {logits2_falsh_attn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Logits\n",
      "------------------\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>''''\n",
      "------------------\n",
      "\n",
      "Normal Model + Padded Input\n",
      "Loss (with padding): 7.247009754180908\n",
      "Logits (with padding): tensor([[ 0.0608, -0.1953,  0.3203,  ...,  1.3281,  1.8359,  0.6094],\n",
      "        [-8.3750, -9.8125, -0.3574,  ..., -3.5000, -8.0625, -2.8750],\n",
      "        [-3.8281, -2.8281,  4.0000,  ..., -1.4375, -2.8438, -0.4102],\n",
      "        ...,\n",
      "        [-5.3438,  3.5625,  5.4062,  ..., -2.5781, -1.0469, -3.3594],\n",
      "        [-5.3750,  2.4375,  5.8438,  ..., -2.2969, -1.0547, -3.7031],\n",
      "        [-5.1250,  3.9844,  5.8750,  ..., -1.9766, -0.9297, -3.1250]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss2 = model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2 = model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print('Decoded Logits')\n",
    "print('------------------')\n",
    "print(tokenizer.decode(logits2.argmax(dim=-1)[0]))\n",
    "print('------------------\\n')\n",
    "\n",
    "print(f\"Normal Model + Padded Input\")\n",
    "print(f\"Loss (with padding): {loss2}\")\n",
    "print(f\"Logits (with padding): {logits2[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Logits\n",
      "------------------\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "------------------\n",
      "\n",
      "Flash Attn Model + Padded Input\n",
      "Loss (padding input & masking labels): 11.508078575134277\n",
      "Logits (padding input & masking labels): tensor([[ 0.0608, -0.1953,  0.3203,  ...,  1.3281,  1.8359,  0.6094],\n",
      "        [-8.4375, -9.8750, -0.3477,  ..., -3.5312, -8.0625, -2.8906],\n",
      "        [-3.7656, -2.8125,  4.0625,  ..., -1.3672, -2.7812, -0.3457],\n",
      "        ...,\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562],\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562],\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "flash_attn_loss = flash_attn_model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "flash_attn_logits = flash_attn_model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print('Decoded Logits')\n",
    "print('------------------')\n",
    "print(tokenizer.decode(flash_attn_logits.argmax(dim=-1)[0]))\n",
    "print('------------------\\n')\n",
    "\n",
    "print(f\"Flash Attn Model + Padded Input\")\n",
    "print(f\"Loss (padding input & masking labels): {flash_attn_loss}\")\n",
    "print(f\"Logits (padding input & masking labels): {flash_attn_logits[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 0, 3, 0],\n",
      "        [1, 0, 3, 3],\n",
      "        [0, 3, 2, 0]]) tensor([[3, 4, 2, 0],\n",
      "        [3, 4, 2, 4]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 5, (3, 4))\n",
    "b = torch.randint(0, 5, (2, 4))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a and b to dtype=torch.float32\n",
    "a = a.float()\n",
    "b = b.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = torch.tensor([[0, 4, 3, 1]])\n",
    "b1 = b1.float()\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.7500, -1.2500]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((a[:, None, :] - b1) * (a[:, None, :]), dim=2).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 0., 3., 0.]],\n",
      "\n",
      "        [[1., 0., 3., 3.]],\n",
      "\n",
      "        [[0., 3., 2., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2500,  2.5000, -0.7500],\n",
       "        [ 0.2500, -0.5000, -0.7500]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a[:, None, :])\n",
    "torch.mean((a[:, None, :] - b) * (a[:, None, :]), dim=2).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0217, 0.4867, 0.4916]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_example = [[4.46, 7.57, 7.58]]\n",
    "# tensor_example = tensor_example\n",
    "tensor_example = torch.tensor(tensor_example)\n",
    "\n",
    "# plot the softmax output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_softmax(tensor_example, knn_T=10):\n",
    "#     tensor_example = tensor_example / knn_T\n",
    "#     softmax = torch.nn.Softmax(dim=1)\n",
    "#     softmax_output = softmax(tensor_example)\n",
    "#     print(softmax_output)\n",
    "#     plt.plot(softmax_output.detach().numpy().flatten(), 'o')\n",
    "#     # shot the x axis as discrete values\n",
    "#     plt.xticks(np.arange(16))\n",
    "#     # plot the y from 0 to 1\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_softmax(tensor_example, 100)\n",
    "torch.softmax(tensor_example, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3967], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer(\" positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Choose sentiment from terrible or great.\n",
    "\n",
    "Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris.\n",
    "Sentiment: terrible\n",
    "Review:  suspenseful enough for older kids but not . \n",
    "Sentiment: great\n",
    "\n",
    "Review: the subtle strength of elling is that it never squandering touch with the reality of the grim situation . \n",
    "Sentiment:\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16, use_flash_attention_2=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_flash_attention_2=True, load_in_8bit=True)\n",
    "# gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "# gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to('cuda')\n",
    "\n",
    "# mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "# mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attacked_answer\n",
       " 1    23\n",
       " 0    14\n",
       "-1    13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../checkpoints//rte/meta-llama/Llama-2-7b-hf/swap_labels/icl_attack-seed-1-shot-8/swap_labels_log.csv\")\n",
    "\n",
    "def get_demo_and_question(text):\n",
    "    demons = text.split(\"<SPLIT>\")\n",
    "    demons = [demon.split(\":\")[1].strip('\\n ').strip('[]') for demon in demons]\n",
    "\n",
    "    question = (demons[0], demons[1], \"\")\n",
    "    icl_examples = []\n",
    "    demons = demons[2:]\n",
    "    for i in range(len(demons) // 3):\n",
    "        icl_examples.append((demons[i * 3], demons[i * 3 + 1], demons[i * 3 + 2]))\n",
    "    return question, icl_examples\n",
    "\n",
    "def compare_non_modifable(row):\n",
    "    original = row['original_text']\n",
    "    modified = row['perturbed_text']\n",
    "    ori_q, ori_icl_examples = get_demo_and_question(original)\n",
    "    mod_q, mod_icl_examples = get_demo_and_question(modified)\n",
    "\n",
    "    return (all([(e[0] == ae[0]) and (e[1] == ae[1]) for e, ae in zip(ori_icl_examples, mod_icl_examples)])) and (ori_q == mod_q)\n",
    "\n",
    "def compute_distributions(question, icl_examples):\n",
    "    template = \"{}\\n The question is: {}. True or False?\\nThe Answer is: {}\"\n",
    "    verbalizer = {0: \"true\", 1: \"false\"}\n",
    "    label_id = [tokenizer.encode(verbalizer[0])[1], tokenizer.encode(verbalizer[1])[1]]\n",
    "\n",
    "    demos = []\n",
    "    for demo in icl_examples:\n",
    "        demos.append(template.format(demo[0], demo[1], demo[2]))\n",
    "    q = template.format(question[0], question[1], \"\").strip()\n",
    "\n",
    "    prompt = \"\\n\\n\".join(demos) + \"\\n\\n\" + q\n",
    "\n",
    "    # print(prompt)\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    logits = model(**tokenized).logits\n",
    "    output = logits[:, -1, :].detach().cpu()\n",
    "\n",
    "    output_label = output[:, label_id].softmax(dim=-1)\n",
    "    return output_label.argmax(dim=-1).item()\n",
    "\n",
    "def compute_the_attacked_answer(row):\n",
    "    if row['result_type'] == 'Skipped':\n",
    "        return -1\n",
    "    \n",
    "    original = row['original_text']\n",
    "    modified = row['perturbed_text']\n",
    "    # ori_q, ori_icl_examples = get_demo_and_question(original)\n",
    "    mod_q, mod_icl_examples = get_demo_and_question(modified)\n",
    "\n",
    "    return compute_distributions(mod_q, mod_icl_examples)\n",
    "\n",
    "def compute_original_answer(row):\n",
    "    if row['result_type'] == 'Skipped':\n",
    "        return -1\n",
    "    \n",
    "    original = row['original_text']\n",
    "    modified = row['perturbed_text']\n",
    "    ori_q, ori_icl_examples = get_demo_and_question(original)\n",
    "    # mod_q, mod_icl_examples = get_demo_and_question(modified)\n",
    "\n",
    "    return compute_distributions(ori_q, ori_icl_examples)\n",
    "\n",
    "df['non_modifiable'] = df.apply(compare_non_modifable, axis=1)\n",
    "df['attacked_answer'] = df.apply(compute_the_attacked_answer, axis=1)\n",
    "df['original_answer'] = df.apply(compute_original_answer, axis=1)\n",
    "\n",
    "df['attacked_answer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(icl_examples, percentage):\n",
    "    np.random.seed(1)\n",
    "    idx = np.random.choice(len(icl_examples), int(len(icl_examples) * percentage), replace=False)\n",
    "    for i in idx:\n",
    "        icl_examples[i] = (icl_examples[i][0], icl_examples[i][1], 'false' if icl_examples[i][2] == 'true' else 'true')\n",
    "\n",
    "    return icl_examples\n",
    "\n",
    "def compute_random_flip_original_answer(row):\n",
    "    if row['result_type'] == 'Skipped':\n",
    "        return -1\n",
    "    \n",
    "    original = row['original_text']\n",
    "    ori_q, ori_icl_examples = get_demo_and_question(original)\n",
    "    ori_icl_examples = random_flip(ori_icl_examples, 0.5)\n",
    "    # mod_q, mod_icl_examples = get_demo_and_question(modified)\n",
    "\n",
    "    return compute_distributions(ori_q, ori_icl_examples)\n",
    "\n",
    "df['random_flip_original_answer'] = df.apply(compute_random_flip_original_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy\n",
      "correct\n",
      "True     35\n",
      "False    15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack Accuracy\n",
      "attack_correct\n",
      "False    39\n",
      "True     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Random Flip Accuracy\n",
      "random_flip_correct\n",
      "True     34\n",
      "False    16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['correct'] = df['original_answer'] == df['ground_truth_output']\n",
    "df['attack_correct'] = df['attacked_answer'] == df['ground_truth_output']\n",
    "df['random_flip_correct'] = df['random_flip_original_answer'] == df['ground_truth_output']\n",
    "\n",
    "print('Original Accuracy')\n",
    "print(df['correct'].value_counts())\n",
    "print('\\nAttack Accuracy')\n",
    "print(df['attack_correct'].value_counts())\n",
    "print('\\nRandom Flip Accuracy')\n",
    "print(df['random_flip_correct'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>original_score</th>\n",
       "      <th>perturbed_score</th>\n",
       "      <th>original_output</th>\n",
       "      <th>perturbed_output</th>\n",
       "      <th>ground_truth_output</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>result_type</th>\n",
       "      <th>non_modifiable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[[Premise]]]]: dana reeve, the widow of the ...</td>\n",
       "      <td>[[[[Premise]]]]: dana reeve, the widow of the ...</td>\n",
       "      <td>0.577495</td>\n",
       "      <td>0.577495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[[Premise]]]]: yet, we now are discovering t...</td>\n",
       "      <td>[[[[Premise]]]]: yet, we now are discovering t...</td>\n",
       "      <td>0.629775</td>\n",
       "      <td>0.629775</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[[Premise]]]]: cairo is now home to some 15 ...</td>\n",
       "      <td>[[[[Premise]]]]: cairo is now home to some 15 ...</td>\n",
       "      <td>0.348645</td>\n",
       "      <td>0.546738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[[Premise]]]]: the amish community in pennsy...</td>\n",
       "      <td>[[[[Premise]]]]: the amish community in pennsy...</td>\n",
       "      <td>0.433981</td>\n",
       "      <td>0.538983</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[[Premise]]]]: security forces were on high ...</td>\n",
       "      <td>[[[[Premise]]]]: security forces were on high ...</td>\n",
       "      <td>0.370225</td>\n",
       "      <td>0.577495</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[[[Premise]]]]: in 1979, the leaders signed t...</td>\n",
       "      <td>[[[[Premise]]]]: in 1979, the leaders signed t...</td>\n",
       "      <td>0.284576</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[[[Premise]]]]: singer and actress britney sp...</td>\n",
       "      <td>[[[[Premise]]]]: singer and actress britney sp...</td>\n",
       "      <td>0.187133</td>\n",
       "      <td>0.573678</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[[[Premise]]]]: following the successful bid ...</td>\n",
       "      <td>[[[[Premise]]]]: following the successful bid ...</td>\n",
       "      <td>0.320821</td>\n",
       "      <td>0.531209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[[Premise]]]]: steve jobs was attacked by sc...</td>\n",
       "      <td>[[[[Premise]]]]: steve jobs was attacked by sc...</td>\n",
       "      <td>0.320821</td>\n",
       "      <td>0.535098</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[[Premise]]]]: traditionally, the brahui of ...</td>\n",
       "      <td>[[[[Premise]]]]: traditionally, the brahui of ...</td>\n",
       "      <td>0.523420</td>\n",
       "      <td>0.523420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[[[Premise]]]]: the international humanitaria...</td>\n",
       "      <td>[[[[Premise]]]]: the international humanitaria...</td>\n",
       "      <td>0.225417</td>\n",
       "      <td>0.558327</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[[[Premise]]]]: in a bowl, whisk together the...</td>\n",
       "      <td>[[[[Premise]]]]: in a bowl, whisk together the...</td>\n",
       "      <td>0.476580</td>\n",
       "      <td>0.577495</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[[[Premise]]]]: in nigeria, by far the most p...</td>\n",
       "      <td>[[[[Premise]]]]: in nigeria, by far the most p...</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[[[Premise]]]]: a jury is slated to decide fo...</td>\n",
       "      <td>[[[[Premise]]]]: a jury is slated to decide fo...</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.538983</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[[[Premise]]]]: take consumer products giant ...</td>\n",
       "      <td>[[[[Premise]]]]: take consumer products giant ...</td>\n",
       "      <td>0.615088</td>\n",
       "      <td>0.615088</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[[[Premise]]]]: deceased u.s. soldiers and th...</td>\n",
       "      <td>[[[[Premise]]]]: deceased u.s. soldiers and th...</td>\n",
       "      <td>0.468791</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[[[Premise]]]]: mice given a substance found ...</td>\n",
       "      <td>[[[[Premise]]]]: mice given a substance found ...</td>\n",
       "      <td>0.511717</td>\n",
       "      <td>0.511717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[[[Premise]]]]: charles de gaulle died in 197...</td>\n",
       "      <td>[[[[Premise]]]]: charles de gaulle died in 197...</td>\n",
       "      <td>0.422505</td>\n",
       "      <td>0.527317</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[[[Premise]]]]: teenage sensation wayne roone...</td>\n",
       "      <td>[[[[Premise]]]]: teenage sensation wayne roone...</td>\n",
       "      <td>0.259826</td>\n",
       "      <td>0.542863</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[[[Premise]]]]: fujimori charged that on janu...</td>\n",
       "      <td>[[[[Premise]]]]: fujimori charged that on janu...</td>\n",
       "      <td>0.484380</td>\n",
       "      <td>0.542863</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[[[Premise]]]]: hepburn's platinum, diamond a...</td>\n",
       "      <td>[[[[Premise]]]]: hepburn's platinum, diamond a...</td>\n",
       "      <td>0.278257</td>\n",
       "      <td>0.581303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[[[Premise]]]]: huckaby voluntarily submitted...</td>\n",
       "      <td>[[[[Premise]]]]: huckaby voluntarily submitted...</td>\n",
       "      <td>0.191933</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[[[Premise]]]]: camden, n.j. (reuters) — thre...</td>\n",
       "      <td>[[[[Premise]]]]: camden, n.j. (reuters) — thre...</td>\n",
       "      <td>0.449393</td>\n",
       "      <td>0.511717</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[[[Premise]]]]: a number of the items that he...</td>\n",
       "      <td>[[[[Premise]]]]: a number of the items that he...</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.531209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[[[Premise]]]]: according to reports, a man p...</td>\n",
       "      <td>[[[[Premise]]]]: according to reports, a man p...</td>\n",
       "      <td>0.370225</td>\n",
       "      <td>0.511717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[[[Premise]]]]: the san diego padres ace, jak...</td>\n",
       "      <td>[[[[Premise]]]]: the san diego padres ace, jak...</td>\n",
       "      <td>0.281406</td>\n",
       "      <td>0.256832</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[[[Premise]]]]: despite cnooc's all-cash bid,...</td>\n",
       "      <td>[[[[Premise]]]]: despite cnooc's all-cash bid,...</td>\n",
       "      <td>0.519521</td>\n",
       "      <td>0.519521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[[[Premise]]]]: u.s. forces have been engaged...</td>\n",
       "      <td>[[[[Premise]]]]: u.s. forces have been engaged...</td>\n",
       "      <td>0.297470</td>\n",
       "      <td>0.550607</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[[[Premise]]]]: but huawei says that expansio...</td>\n",
       "      <td>[[[[Premise]]]]: but huawei says that expansio...</td>\n",
       "      <td>0.095349</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[[[Premise]]]]: according to becky gibbons of...</td>\n",
       "      <td>[[[[Premise]]]]: according to becky gibbons of...</td>\n",
       "      <td>0.633410</td>\n",
       "      <td>0.633410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[[[[Premise]]]]: a smaller proportion of yugos...</td>\n",
       "      <td>[[[[Premise]]]]: a smaller proportion of yugos...</td>\n",
       "      <td>0.352202</td>\n",
       "      <td>0.519521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[[[[Premise]]]]: eric harris and dylan klebold...</td>\n",
       "      <td>[[[[Premise]]]]: eric harris and dylan klebold...</td>\n",
       "      <td>0.373876</td>\n",
       "      <td>0.542863</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[[[[Premise]]]]: 47-year-old susan boyle from ...</td>\n",
       "      <td>[[[[Premise]]]]: 47-year-old susan boyle from ...</td>\n",
       "      <td>0.366590</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[[[[Premise]]]]: on october 1 2001, eu and oth...</td>\n",
       "      <td>[[[[Premise]]]]: on october 1 2001, eu and oth...</td>\n",
       "      <td>0.275130</td>\n",
       "      <td>0.527317</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[[[[Premise]]]]: the longest day ever lengthen...</td>\n",
       "      <td>[[[[Premise]]]]: the longest day ever lengthen...</td>\n",
       "      <td>0.558327</td>\n",
       "      <td>0.558327</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[[[[Premise]]]]: the qin (from which the name ...</td>\n",
       "      <td>[[[[Premise]]]]: the qin (from which the name ...</td>\n",
       "      <td>0.476580</td>\n",
       "      <td>0.546738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[[[[Premise]]]]: hands across the divide was f...</td>\n",
       "      <td>[[[[Premise]]]]: hands across the divide was f...</td>\n",
       "      <td>0.407333</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[[[[Premise]]]]: the ioc meeting will also rev...</td>\n",
       "      <td>[[[[Premise]]]]: the ioc meeting will also rev...</td>\n",
       "      <td>0.585101</td>\n",
       "      <td>0.585101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[[[[Premise]]]]: amazon shares fell nearly 4 p...</td>\n",
       "      <td>[[[[Premise]]]]: amazon shares fell nearly 4 p...</td>\n",
       "      <td>0.718594</td>\n",
       "      <td>0.718594</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[[[[Premise]]]]: hodler claimed there were als...</td>\n",
       "      <td>[[[[Premise]]]]: hodler claimed there were als...</td>\n",
       "      <td>0.359364</td>\n",
       "      <td>0.519521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[[[[Premise]]]]: a compound in breast milk has...</td>\n",
       "      <td>[[[[Premise]]]]: a compound in breast milk has...</td>\n",
       "      <td>0.407333</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[[[[Premise]]]]: the plan was released by mr d...</td>\n",
       "      <td>[[[[Premise]]]]: the plan was released by mr d...</td>\n",
       "      <td>0.345105</td>\n",
       "      <td>0.535098</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[[[[Premise]]]]: alice cooper, a founder of th...</td>\n",
       "      <td>[[[[Premise]]]]: alice cooper, a founder of th...</td>\n",
       "      <td>0.484380</td>\n",
       "      <td>0.562177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[[[[Premise]]]]: pibul songgram was the pro-ja...</td>\n",
       "      <td>[[[[Premise]]]]: pibul songgram was the pro-ja...</td>\n",
       "      <td>0.542863</td>\n",
       "      <td>0.542863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[[[[Premise]]]]: as spacecraft commander for a...</td>\n",
       "      <td>[[[[Premise]]]]: as spacecraft commander for a...</td>\n",
       "      <td>0.199308</td>\n",
       "      <td>0.573678</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[[[[Premise]]]]: weber worked for wabc for 12 ...</td>\n",
       "      <td>[[[[Premise]]]]: weber worked for wabc for 12 ...</td>\n",
       "      <td>0.245085</td>\n",
       "      <td>0.262842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Failed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[[[[Premise]]]]: russian cosmonaut valery poly...</td>\n",
       "      <td>[[[[Premise]]]]: russian cosmonaut valery poly...</td>\n",
       "      <td>0.531209</td>\n",
       "      <td>0.531209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[[[[Premise]]]]: the harvest of sea-weeds is n...</td>\n",
       "      <td>[[[[Premise]]]]: the harvest of sea-weeds is n...</td>\n",
       "      <td>0.250913</td>\n",
       "      <td>0.511717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[[[[Premise]]]]: he became a boxing referee in...</td>\n",
       "      <td>[[[[Premise]]]]: he became a boxing referee in...</td>\n",
       "      <td>0.242206</td>\n",
       "      <td>0.637031</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>Successful</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[[[[Premise]]]]: the team drawing up iraq's ne...</td>\n",
       "      <td>[[[[Premise]]]]: the team drawing up iraq's ne...</td>\n",
       "      <td>0.611382</td>\n",
       "      <td>0.611382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_text  \\\n",
       "0   [[[[Premise]]]]: dana reeve, the widow of the ...   \n",
       "1   [[[[Premise]]]]: yet, we now are discovering t...   \n",
       "2   [[[[Premise]]]]: cairo is now home to some 15 ...   \n",
       "3   [[[[Premise]]]]: the amish community in pennsy...   \n",
       "4   [[[[Premise]]]]: security forces were on high ...   \n",
       "5   [[[[Premise]]]]: in 1979, the leaders signed t...   \n",
       "6   [[[[Premise]]]]: singer and actress britney sp...   \n",
       "7   [[[[Premise]]]]: following the successful bid ...   \n",
       "8   [[[[Premise]]]]: steve jobs was attacked by sc...   \n",
       "9   [[[[Premise]]]]: traditionally, the brahui of ...   \n",
       "10  [[[[Premise]]]]: the international humanitaria...   \n",
       "11  [[[[Premise]]]]: in a bowl, whisk together the...   \n",
       "12  [[[[Premise]]]]: in nigeria, by far the most p...   \n",
       "13  [[[[Premise]]]]: a jury is slated to decide fo...   \n",
       "14  [[[[Premise]]]]: take consumer products giant ...   \n",
       "15  [[[[Premise]]]]: deceased u.s. soldiers and th...   \n",
       "16  [[[[Premise]]]]: mice given a substance found ...   \n",
       "17  [[[[Premise]]]]: charles de gaulle died in 197...   \n",
       "18  [[[[Premise]]]]: teenage sensation wayne roone...   \n",
       "19  [[[[Premise]]]]: fujimori charged that on janu...   \n",
       "20  [[[[Premise]]]]: hepburn's platinum, diamond a...   \n",
       "21  [[[[Premise]]]]: huckaby voluntarily submitted...   \n",
       "22  [[[[Premise]]]]: camden, n.j. (reuters) — thre...   \n",
       "23  [[[[Premise]]]]: a number of the items that he...   \n",
       "24  [[[[Premise]]]]: according to reports, a man p...   \n",
       "25  [[[[Premise]]]]: the san diego padres ace, jak...   \n",
       "26  [[[[Premise]]]]: despite cnooc's all-cash bid,...   \n",
       "27  [[[[Premise]]]]: u.s. forces have been engaged...   \n",
       "28  [[[[Premise]]]]: but huawei says that expansio...   \n",
       "29  [[[[Premise]]]]: according to becky gibbons of...   \n",
       "30  [[[[Premise]]]]: a smaller proportion of yugos...   \n",
       "31  [[[[Premise]]]]: eric harris and dylan klebold...   \n",
       "32  [[[[Premise]]]]: 47-year-old susan boyle from ...   \n",
       "33  [[[[Premise]]]]: on october 1 2001, eu and oth...   \n",
       "34  [[[[Premise]]]]: the longest day ever lengthen...   \n",
       "35  [[[[Premise]]]]: the qin (from which the name ...   \n",
       "36  [[[[Premise]]]]: hands across the divide was f...   \n",
       "37  [[[[Premise]]]]: the ioc meeting will also rev...   \n",
       "38  [[[[Premise]]]]: amazon shares fell nearly 4 p...   \n",
       "39  [[[[Premise]]]]: hodler claimed there were als...   \n",
       "40  [[[[Premise]]]]: a compound in breast milk has...   \n",
       "41  [[[[Premise]]]]: the plan was released by mr d...   \n",
       "42  [[[[Premise]]]]: alice cooper, a founder of th...   \n",
       "43  [[[[Premise]]]]: pibul songgram was the pro-ja...   \n",
       "44  [[[[Premise]]]]: as spacecraft commander for a...   \n",
       "45  [[[[Premise]]]]: weber worked for wabc for 12 ...   \n",
       "46  [[[[Premise]]]]: russian cosmonaut valery poly...   \n",
       "47  [[[[Premise]]]]: the harvest of sea-weeds is n...   \n",
       "48  [[[[Premise]]]]: he became a boxing referee in...   \n",
       "49  [[[[Premise]]]]: the team drawing up iraq's ne...   \n",
       "\n",
       "                                       perturbed_text  original_score  \\\n",
       "0   [[[[Premise]]]]: dana reeve, the widow of the ...        0.577495   \n",
       "1   [[[[Premise]]]]: yet, we now are discovering t...        0.629775   \n",
       "2   [[[[Premise]]]]: cairo is now home to some 15 ...        0.348645   \n",
       "3   [[[[Premise]]]]: the amish community in pennsy...        0.433981   \n",
       "4   [[[[Premise]]]]: security forces were on high ...        0.370225   \n",
       "5   [[[[Premise]]]]: in 1979, the leaders signed t...        0.284576   \n",
       "6   [[[[Premise]]]]: singer and actress britney sp...        0.187133   \n",
       "7   [[[[Premise]]]]: following the successful bid ...        0.320821   \n",
       "8   [[[[Premise]]]]: steve jobs was attacked by sc...        0.320821   \n",
       "9   [[[[Premise]]]]: traditionally, the brahui of ...        0.523420   \n",
       "10  [[[[Premise]]]]: the international humanitaria...        0.225417   \n",
       "11  [[[[Premise]]]]: in a bowl, whisk together the...        0.476580   \n",
       "12  [[[[Premise]]]]: in nigeria, by far the most p...        0.294215   \n",
       "13  [[[[Premise]]]]: a jury is slated to decide fo...        0.492188   \n",
       "14  [[[[Premise]]]]: take consumer products giant ...        0.615088   \n",
       "15  [[[[Premise]]]]: deceased u.s. soldiers and th...        0.468791   \n",
       "16  [[[[Premise]]]]: mice given a substance found ...        0.511717   \n",
       "17  [[[[Premise]]]]: charles de gaulle died in 197...        0.422505   \n",
       "18  [[[[Premise]]]]: teenage sensation wayne roone...        0.259826   \n",
       "19  [[[[Premise]]]]: fujimori charged that on janu...        0.484380   \n",
       "20  [[[[Premise]]]]: hepburn's platinum, diamond a...        0.278257   \n",
       "21  [[[[Premise]]]]: huckaby voluntarily submitted...        0.191933   \n",
       "22  [[[[Premise]]]]: camden, n.j. (reuters) — thre...        0.449393   \n",
       "23  [[[[Premise]]]]: a number of the items that he...        0.268941   \n",
       "24  [[[[Premise]]]]: according to reports, a man p...        0.370225   \n",
       "25  [[[[Premise]]]]: the san diego padres ace, jak...        0.281406   \n",
       "26  [[[[Premise]]]]: despite cnooc's all-cash bid,...        0.519521   \n",
       "27  [[[[Premise]]]]: u.s. forces have been engaged...        0.297470   \n",
       "28  [[[[Premise]]]]: but huawei says that expansio...        0.095349   \n",
       "29  [[[[Premise]]]]: according to becky gibbons of...        0.633410   \n",
       "30  [[[[Premise]]]]: a smaller proportion of yugos...        0.352202   \n",
       "31  [[[[Premise]]]]: eric harris and dylan klebold...        0.373876   \n",
       "32  [[[[Premise]]]]: 47-year-old susan boyle from ...        0.366590   \n",
       "33  [[[[Premise]]]]: on october 1 2001, eu and oth...        0.275130   \n",
       "34  [[[[Premise]]]]: the longest day ever lengthen...        0.558327   \n",
       "35  [[[[Premise]]]]: the qin (from which the name ...        0.476580   \n",
       "36  [[[[Premise]]]]: hands across the divide was f...        0.407333   \n",
       "37  [[[[Premise]]]]: the ioc meeting will also rev...        0.585101   \n",
       "38  [[[[Premise]]]]: amazon shares fell nearly 4 p...        0.718594   \n",
       "39  [[[[Premise]]]]: hodler claimed there were als...        0.359364   \n",
       "40  [[[[Premise]]]]: a compound in breast milk has...        0.407333   \n",
       "41  [[[[Premise]]]]: the plan was released by mr d...        0.345105   \n",
       "42  [[[[Premise]]]]: alice cooper, a founder of th...        0.484380   \n",
       "43  [[[[Premise]]]]: pibul songgram was the pro-ja...        0.542863   \n",
       "44  [[[[Premise]]]]: as spacecraft commander for a...        0.199308   \n",
       "45  [[[[Premise]]]]: weber worked for wabc for 12 ...        0.245085   \n",
       "46  [[[[Premise]]]]: russian cosmonaut valery poly...        0.531209   \n",
       "47  [[[[Premise]]]]: the harvest of sea-weeds is n...        0.250913   \n",
       "48  [[[[Premise]]]]: he became a boxing referee in...        0.242206   \n",
       "49  [[[[Premise]]]]: the team drawing up iraq's ne...        0.611382   \n",
       "\n",
       "    perturbed_score  original_output  perturbed_output  ground_truth_output  \\\n",
       "0          0.577495                0                 0                    1   \n",
       "1          0.629775                1                 1                    0   \n",
       "2          0.546738                1                 0                    1   \n",
       "3          0.538983                1                 0                    1   \n",
       "4          0.577495                0                 1                    0   \n",
       "5          0.515620                0                 1                    0   \n",
       "6          0.573678                0                 1                    0   \n",
       "7          0.531209                0                 1                    0   \n",
       "8          0.535098                0                 1                    0   \n",
       "9          0.523420                1                 1                    0   \n",
       "10         0.558327                0                 1                    0   \n",
       "11         0.577495                1                 0                    1   \n",
       "12         0.503906                1                 0                    1   \n",
       "13         0.538983                0                 1                    0   \n",
       "14         0.615088                1                 1                    0   \n",
       "15         0.500000                1                 0                    1   \n",
       "16         0.511717                0                 0                    1   \n",
       "17         0.527317                0                 1                    0   \n",
       "18         0.542863                0                 1                    0   \n",
       "19         0.542863                1                 0                    1   \n",
       "20         0.581303                0                 1                    0   \n",
       "21         0.515620                0                 1                    0   \n",
       "22         0.511717                0                 1                    0   \n",
       "23         0.531209                0                 1                    0   \n",
       "24         0.511717                1                 0                    1   \n",
       "25         0.256832                1                 1                    1   \n",
       "26         0.519521                0                 0                    1   \n",
       "27         0.550607                0                 1                    0   \n",
       "28         0.138462                1                 1                    1   \n",
       "29         0.633410                0                 0                    1   \n",
       "30         0.519521                1                 0                    1   \n",
       "31         0.542863                0                 1                    0   \n",
       "32         0.500000                1                 0                    1   \n",
       "33         0.527317                1                 0                    1   \n",
       "34         0.558327                1                 1                    0   \n",
       "35         0.546738                1                 0                    1   \n",
       "36         0.507812                1                 0                    1   \n",
       "37         0.585101                1                 1                    0   \n",
       "38         0.718594                0                 0                    1   \n",
       "39         0.519521                1                 0                    1   \n",
       "40         0.503906                0                 1                    0   \n",
       "41         0.535098                1                 0                    1   \n",
       "42         0.562177                0                 1                    0   \n",
       "43         0.542863                1                 1                    0   \n",
       "44         0.573678                0                 1                    0   \n",
       "45         0.262842                1                 1                    1   \n",
       "46         0.531209                1                 1                    0   \n",
       "47         0.511717                1                 0                    1   \n",
       "48         0.637031                1                 0                    1   \n",
       "49         0.611382                0                 0                    1   \n",
       "\n",
       "    num_queries result_type  non_modifiable  \n",
       "0             1     Skipped            True  \n",
       "1             1     Skipped            True  \n",
       "2            46  Successful            True  \n",
       "3            32  Successful            True  \n",
       "4            32  Successful            True  \n",
       "5            46  Successful            True  \n",
       "6            59  Successful            True  \n",
       "7            46  Successful            True  \n",
       "8            59  Successful            True  \n",
       "9             1     Skipped            True  \n",
       "10           82  Successful            True  \n",
       "11           32  Successful            True  \n",
       "12           59  Successful            True  \n",
       "13           17  Successful            True  \n",
       "14            1     Skipped            True  \n",
       "15           32  Successful            True  \n",
       "16            1     Skipped            True  \n",
       "17           17  Successful            True  \n",
       "18           71  Successful            True  \n",
       "19           17  Successful            True  \n",
       "20           59  Successful            True  \n",
       "21           59  Successful            True  \n",
       "22           17  Successful            True  \n",
       "23           46  Successful            True  \n",
       "24           32  Successful            True  \n",
       "25          137      Failed            True  \n",
       "26            1     Skipped            True  \n",
       "27           46  Successful            True  \n",
       "28          137      Failed            True  \n",
       "29            1     Skipped            True  \n",
       "30           71  Successful            True  \n",
       "31           71  Successful            True  \n",
       "32           71  Successful            True  \n",
       "33          116  Successful            True  \n",
       "34            1     Skipped            True  \n",
       "35           17  Successful            True  \n",
       "36           17  Successful            True  \n",
       "37            1     Skipped            True  \n",
       "38            1     Skipped            True  \n",
       "39           59  Successful            True  \n",
       "40           17  Successful            True  \n",
       "41           17  Successful            True  \n",
       "42           17  Successful            True  \n",
       "43            1     Skipped            True  \n",
       "44           71  Successful            True  \n",
       "45          137      Failed            True  \n",
       "46            1     Skipped            True  \n",
       "47          101  Successful            True  \n",
       "48          109  Successful            True  \n",
       "49            1     Skipped            True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demonstration = \"\"\"\n",
    "[[[[Premise]]]]: cairo is now home to some 15 million people - a burgeoning population that produces approximately 10,000 tonnes of rubbish per day, putting an enormous strain on public services. in the past 10 years, the government has tried hard to encourage private investment in the refuse sector, but some estimate 4,000 tonnes of waste is left behind every day, festering in the heat as it waits for someone to clear it up. it is often the people in the poorest neighbourhoods that are worst affected. but in some areas they are fighting back. in shubra, one of the northern districts of the city, the residents have taken to the streets armed with dustpans and brushes to clean up public areas which have been used as public dumps.<SPLIT>[[[[Hypothesis]]]]: 15 million tonnes of rubbish are produced daily in cairo.<SPLIT>[[[[Premise_0]]]]: george herbert walker bush (born june 12, 1924) is the former 41st president of the united states of america. almost immediately upon his return from the war in december 1944, george bush married barbara pierce.<SPLIT>[[[[Hypothesis_0]]]]: the name of george h.w. bush's wife is barbara.<SPLIT>[[[[Label_0]]]]: true<SPLIT>[[[[Premise_1]]]]: it rewrites the rules of global trade, established by the general agreement on tariffs and trade, or gatt, in 1947, and modified in multiple rounds of negotiations since then.<SPLIT>[[[[Hypothesis_1]]]]: gatt was formed in 1947.<SPLIT>[[[[Label_1]]]]: false<SPLIT>[[[[Premise_2]]]]: us military forces are evacuating u.s. citizens and citizens of 72 other countries from liberia at the request of the u.s.<SPLIT>[[[[Hypothesis_2]]]]: u.s. military evacuated u.s. citizens.<SPLIT>[[[[Label_2]]]]: true<SPLIT>[[[[Premise_3]]]]: floods are one of europe's most widespread disasters. major flooding has occurred nearly every year somewhere on our continent during the last few decades.<SPLIT>[[[[Hypothesis_3]]]]: flooding in europe causes major economic losses.<SPLIT>[[[[Label_3]]]]: [[false]]<SPLIT>[[[[Premise_4]]]]: rock stars aerosmith are to hold a free concert in hawaii to placate angry fans who brought a legal case against them. the walk this way hitmakers cancelled a sold-out show in maui two years ago, leaving hundreds of fans out of pocket. they filed a class action case, which claimed the band had pulled out in favour of a bigger gig in chicago and a private show for car dealers in oahu. lawyers for the would-be concert-goers said aerosmith had now agreed to put on a new show, and would pay all expenses.<SPLIT>[[[[Hypothesis_4]]]]: aerosmith are a rock band.<SPLIT>[[[[Label_4]]]]: true<SPLIT>[[[[Premise_5]]]]: one reason for increased osteoporosis in developed countries is the sodium-potassium imbalance.<SPLIT>[[[[Hypothesis_5]]]]: dietary intake of potassium prevents osteoporosis.<SPLIT>[[[[Label_5]]]]: [[false]]<SPLIT>[[[[Premise_6]]]]: jerusalem, april 2 (xinhua) -- israel's new foreign minister avigdor lieberman was questioned by police on thursday over several criminal allegations, local news service ynet reported.  national fraud unit investigators questioned the deputy premier, who is suspected of bribery, money laundering, fraud and breach of trust, for over seven hours, and another round is in the cards, said the report.  \"\"lieberman answered all of the questions he was asked, and will continue to do so in the future,\"\" his attorney yaron kostelitz was quoted as saying.<SPLIT>[[[[Hypothesis_6]]]]: avigdor lieberman is the foreign minister of israel.<SPLIT>[[[[Label_6]]]]: true<SPLIT>[[[[Premise_7]]]]: other friends were not surprised at his death. \"\"i wasn't surprised,\"\" said george stranahan, a former owner of the woody creek tavern, a favourite haunt of thompson. \"\"i never expected hunter to die in a hospital bed with tubes coming out of him.\"\" neighbours have said how his broken leg had prevented him from leaving his house as often as he had liked to. one neighbour and long-standing friend, mike cleverly, said thompson was clearly hobbled by the broken leg. \"\"medically speaking, he's had a rotten year.\"\"<SPLIT>[[[[Hypothesis_7]]]]: the woody creek tavern is owned by george stranahan.<SPLIT>[[[[Label_7]]]]: false<SPLIT>[[[[Premise_8]]]]: witching hour passed and potter fans poured into bookshops around the world on saturday, snatching up copies of the latest instalment in the series that promises to be the fastest-selling book in history.<SPLIT>[[[[Hypothesis_8]]]]: potter fans rushed to tills in order to purchase the book.<SPLIT>[[[[Label_8]]]]: true<SPLIT>[[[[Premise_9]]]]: alan mulally, boeing's head of the unit, said at the start of the strike that it may cause delivery delays that would give airbus sas an advantage in what is the strongest commercial aircraft market in five years.<SPLIT>[[[[Hypothesis_9]]]]: alan mulally is the owner of boeing.<SPLIT>[[[[Label_9]]]]: false<SPLIT>[[[[Premise_10]]]]: to promote the simpsons movie that will be released july 26, 2007, over a dozen 7-elevens in the united states have been transformed into kwik-e-marts, the grocery store from the popular tv series. most of the other 7-eleven stores will also sell products with brands reminding of the simpsons, such as buzz cola, krustyo's cereal, squishees, and bart simpson's favourite comic book radioactive man, but not duff beer, homer's favourite drink.<SPLIT>[[[[Hypothesis_10]]]]: the simpsons is a show broadcast in america.<SPLIT>[[[[Label_10]]]]: true<SPLIT>[[[[Premise_11]]]]: he is like some great writers, from charles dickens to william faulkner to gabriel garcia marquez: the insistence on the value of the local leads to the universal.<SPLIT>[[[[Hypothesis_11]]]]: gabriel garcia marquez is a nobel prize winner.<SPLIT>[[[[Label_11]]]]: false<SPLIT>[[[[Premise_12]]]]: colin l. powell and laura bush, wife of gov. george w. bush, are to speak on the opening night in philadelphia, while the democrats have tentative plans to have president clinton and hillary rodham clinton address the delegates on the first night in los angeles.<SPLIT>[[[[Hypothesis_12]]]]: the name of george w. bush's wife is laura.<SPLIT>[[[[Label_12]]]]: true<SPLIT>[[[[Premise_13]]]]: after insulting the un committee insinuating they were irrelavant, bush no sooner had approval from congress to use \"\"force/war\"\" if hussein did not permit unfettered access for inspections - and the war ships were moved into place on the gulf.<SPLIT>[[[[Hypothesis_13]]]]: hussein gives un inspectors unfettered access.<SPLIT>[[[[Label_13]]]]: [[false]]<SPLIT>[[[[Premise_14]]]]: while local cab drivers had compliments, not everyone appreciated the company. the chicago cubs decided to cancel their booking after hearing about the con, and j.j. hardy of the milwaukee brewers reported being kept up by fans on wednesday night. the brewers lost their game the following day. the convention closed with next year's date ―june 26-29 ―along with the announcement of a guest: disney animator floyd norman. the event's theme will be \"\"it's a jungle out there!\"\".<SPLIT>[[[[Hypothesis_14]]]]: j.j. hardy has a contract with the milwaukee brewers.<SPLIT>[[[[Label_14]]]]: true<SPLIT>[[[[Premise_15]]]]: for a western european country, the birth rate in finland is high.<SPLIT>[[[[Hypothesis_15]]]]: finland is the european country with the highest birth rate.<SPLIT>[[[[Label_15]]]]: false\"\"\"\n",
    "\n",
    "demons = demonstration.split(\"<SPLIT>\")\n",
    "demons = [demon.split(\":\")[1].strip('\\n ').strip('[]') for demon in demons]\n",
    "\n",
    "question = (demons[0], demons[1], \"\")\n",
    "icl_examples = []\n",
    "demons = demons[2:]\n",
    "for i in range(len(demons) // 3):\n",
    "    icl_examples.append((demons[i * 3], demons[i * 3 + 1], demons[i * 3 + 2]))\n",
    "\n",
    "template = \"{}\\n The question is: {}. True or False?\\nThe Answer is: {}\"\n",
    "verbalizer = {0: \"true\", 1: \"false\"}\n",
    "\n",
    "demos = []\n",
    "for demo in icl_examples:\n",
    "    demos.append(template.format(demo[0], demo[1], demo[2]))\n",
    "q = template.format(question[0], question[1], \"\").strip()\n",
    "\n",
    "prompt = \"\\n\\n\".join(demos) + \"\\n\\n\" + q\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "logits = model(**tokenized).logits\n",
    "output = logits[:, -1, :].detach().cpu()\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "label_id = [tokenizer.encode(verbalizer[0])[1], tokenizer.encode(verbalizer[1])[1]]\n",
    "\n",
    "print(tokenizer.decode(output.argmax(dim=-1)))\n",
    "output_label = output[:, label_id].softmax(dim=-1)\n",
    "output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "george herbert walker bush (born june 12, 1924) is the former 41st president of the united states of america. almost immediately upon his return from the war in december 1944, george bush married barbara pierce.\n",
      " The question is: the name of george h.w. bush's wife is barbara.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "it rewrites the rules of global trade, established by the general agreement on tariffs and trade, or gatt, in 1947, and modified in multiple rounds of negotiations since then.\n",
      " The question is: gatt was formed in 1947.. True or False?\n",
      "The Answer is: false\n",
      "\n",
      "us military forces are evacuating u.s. citizens and citizens of 72 other countries from liberia at the request of the u.s.\n",
      " The question is: u.s. military evacuated u.s. citizens.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "floods are one of europe's most widespread disasters. major flooding has occurred nearly every year somewhere on our continent during the last few decades.\n",
      " The question is: flooding in europe causes major economic losses.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "rock stars aerosmith are to hold a free concert in hawaii to placate angry fans who brought a legal case against them. the walk this way hitmakers cancelled a sold-out show in maui two years ago, leaving hundreds of fans out of pocket. they filed a class action case, which claimed the band had pulled out in favour of a bigger gig in chicago and a private show for car dealers in oahu. lawyers for the would-be concert-goers said aerosmith had now agreed to put on a new show, and would pay all expenses.\n",
      " The question is: aerosmith are a rock band.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "one reason for increased osteoporosis in developed countries is the sodium-potassium imbalance.\n",
      " The question is: dietary intake of potassium prevents osteoporosis.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "jerusalem, april 2 (xinhua) -- israel's new foreign minister avigdor lieberman was questioned by police on thursday over several criminal allegations, local news service ynet reported.  national fraud unit investigators questioned the deputy premier, who is suspected of bribery, money laundering, fraud and breach of trust, for over seven hours, and another round is in the cards, said the report.  \"\"lieberman answered all of the questions he was asked, and will continue to do so in the future,\"\" his attorney yaron kostelitz was quoted as saying.\n",
      " The question is: avigdor lieberman is the foreign minister of israel.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "other friends were not surprised at his death. \"\"i wasn't surprised,\"\" said george stranahan, a former owner of the woody creek tavern, a favourite haunt of thompson. \"\"i never expected hunter to die in a hospital bed with tubes coming out of him.\"\" neighbours have said how his broken leg had prevented him from leaving his house as often as he had liked to. one neighbour and long-standing friend, mike cleverly, said thompson was clearly hobbled by the broken leg. \"\"medically speaking, he's had a rotten year.\"\"\n",
      " The question is: the woody creek tavern is owned by george stranahan.. True or False?\n",
      "The Answer is: false\n",
      "\n",
      "witching hour passed and potter fans poured into bookshops around the world on saturday, snatching up copies of the latest instalment in the series that promises to be the fastest-selling book in history.\n",
      " The question is: potter fans rushed to tills in order to purchase the book.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "alan mulally, boeing's head of the unit, said at the start of the strike that it may cause delivery delays that would give airbus sas an advantage in what is the strongest commercial aircraft market in five years.\n",
      " The question is: alan mulally is the owner of boeing.. True or False?\n",
      "The Answer is: false\n",
      "\n",
      "to promote the simpsons movie that will be released july 26, 2007, over a dozen 7-elevens in the united states have been transformed into kwik-e-marts, the grocery store from the popular tv series. most of the other 7-eleven stores will also sell products with brands reminding of the simpsons, such as buzz cola, krustyo's cereal, squishees, and bart simpson's favourite comic book radioactive man, but not duff beer, homer's favourite drink.\n",
      " The question is: the simpsons is a show broadcast in america.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "he is like some great writers, from charles dickens to william faulkner to gabriel garcia marquez\n",
      " The question is: gabriel garcia marquez is a nobel prize winner.. True or False?\n",
      "The Answer is: false\n",
      "\n",
      "colin l. powell and laura bush, wife of gov. george w. bush, are to speak on the opening night in philadelphia, while the democrats have tentative plans to have president clinton and hillary rodham clinton address the delegates on the first night in los angeles.\n",
      " The question is: the name of george w. bush's wife is laura.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "after insulting the un committee insinuating they were irrelavant, bush no sooner had approval from congress to use \"\"force/war\"\" if hussein did not permit unfettered access for inspections - and the war ships were moved into place on the gulf.\n",
      " The question is: hussein gives un inspectors unfettered access.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "while local cab drivers had compliments, not everyone appreciated the company. the chicago cubs decided to cancel their booking after hearing about the con, and j.j. hardy of the milwaukee brewers reported being kept up by fans on wednesday night. the brewers lost their game the following day. the convention closed with next year's date ―june 26-29 ―along with the announcement of a guest\n",
      " The question is: j.j. hardy has a contract with the milwaukee brewers.. True or False?\n",
      "The Answer is: true\n",
      "\n",
      "for a western european country, the birth rate in finland is high.\n",
      " The question is: finland is the european country with the highest birth rate.. True or False?\n",
      "The Answer is: false\n",
      "\n",
      "cairo is now home to some 15 million people - a burgeoning population that produces approximately 10,000 tonnes of rubbish per day, putting an enormous strain on public services. in the past 10 years, the government has tried hard to encourage private investment in the refuse sector, but some estimate 4,000 tonnes of waste is left behind every day, festering in the heat as it waits for someone to clear it up. it is often the people in the poorest neighbourhoods that are worst affected. but in some areas they are fighting back. in shubra, one of the northern districts of the city, the residents have taken to the streets armed with dustpans and brushes to clean up public areas which have been used as public dumps.\n",
      " The question is: 15 million tonnes of rubbish are produced daily in cairo.. True or False?\n",
      "The Answer is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5506, 0.4494]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_sample = \"\"\"[[[[Premise]]]]: cairo is now home to some 15 million people - a burgeoning population that produces approximately 10,000 tonnes of rubbish per day, putting an enormous strain on public services. in the past 10 years, the government has tried hard to encourage private investment in the refuse sector, but some estimate 4,000 tonnes of waste is left behind every day, festering in the heat as it waits for someone to clear it up. it is often the people in the poorest neighbourhoods that are worst affected. but in some areas they are fighting back. in shubra, one of the northern districts of the city, the residents have taken to the streets armed with dustpans and brushes to clean up public areas which have been used as public dumps.<SPLIT>[[[[Hypothesis]]]]: 15 million tonnes of rubbish are produced daily in cairo.<SPLIT>[[[[Premise_0]]]]: george herbert walker bush (born june 12, 1924) is the former 41st president of the united states of america. almost immediately upon his return from the war in december 1944, george bush married barbara pierce.<SPLIT>[[[[Hypothesis_0]]]]: the name of george h.w. bush's wife is barbara.<SPLIT>[[[[Label_0]]]]: true<SPLIT>[[[[Premise_1]]]]: it rewrites the rules of global trade, established by the general agreement on tariffs and trade, or gatt, in 1947, and modified in multiple rounds of negotiations since then.<SPLIT>[[[[Hypothesis_1]]]]: gatt was formed in 1947.<SPLIT>[[[[Label_1]]]]: false<SPLIT>[[[[Premise_2]]]]: us military forces are evacuating u.s. citizens and citizens of 72 other countries from liberia at the request of the u.s.<SPLIT>[[[[Hypothesis_2]]]]: u.s. military evacuated u.s. citizens.<SPLIT>[[[[Label_2]]]]: true<SPLIT>[[[[Premise_3]]]]: floods are one of europe's most widespread disasters. major flooding has occurred nearly every year somewhere on our continent during the last few decades.<SPLIT>[[[[Hypothesis_3]]]]: flooding in europe causes major economic losses.<SPLIT>[[[[Label_3]]]]: [[true]]<SPLIT>[[[[Premise_4]]]]: rock stars aerosmith are to hold a free concert in hawaii to placate angry fans who brought a legal case against them. the walk this way hitmakers cancelled a sold-out show in maui two years ago, leaving hundreds of fans out of pocket. they filed a class action case, which claimed the band had pulled out in favour of a bigger gig in chicago and a private show for car dealers in oahu. lawyers for the would-be concert-goers said aerosmith had now agreed to put on a new show, and would pay all expenses.<SPLIT>[[[[Hypothesis_4]]]]: aerosmith are a rock band.<SPLIT>[[[[Label_4]]]]: true<SPLIT>[[[[Premise_5]]]]: one reason for increased osteoporosis in developed countries is the sodium-potassium imbalance.<SPLIT>[[[[Hypothesis_5]]]]: dietary intake of potassium prevents osteoporosis.<SPLIT>[[[[Label_5]]]]: [[true]]<SPLIT>[[[[Premise_6]]]]: jerusalem, april 2 (xinhua) -- israel's new foreign minister avigdor lieberman was questioned by police on thursday over several criminal allegations, local news service ynet reported.  national fraud unit investigators questioned the deputy premier, who is suspected of bribery, money laundering, fraud and breach of trust, for over seven hours, and another round is in the cards, said the report.  \"\"lieberman answered all of the questions he was asked, and will continue to do so in the future,\"\" his attorney yaron kostelitz was quoted as saying.<SPLIT>[[[[Hypothesis_6]]]]: avigdor lieberman is the foreign minister of israel.<SPLIT>[[[[Label_6]]]]: true<SPLIT>[[[[Premise_7]]]]: other friends were not surprised at his death. \"\"i wasn't surprised,\"\" said george stranahan, a former owner of the woody creek tavern, a favourite haunt of thompson. \"\"i never expected hunter to die in a hospital bed with tubes coming out of him.\"\" neighbours have said how his broken leg had prevented him from leaving his house as often as he had liked to. one neighbour and long-standing friend, mike cleverly, said thompson was clearly hobbled by the broken leg. \"\"medically speaking, he's had a rotten year.\"\"<SPLIT>[[[[Hypothesis_7]]]]: the woody creek tavern is owned by george stranahan.<SPLIT>[[[[Label_7]]]]: false<SPLIT>[[[[Premise_8]]]]: witching hour passed and potter fans poured into bookshops around the world on saturday, snatching up copies of the latest instalment in the series that promises to be the fastest-selling book in history.<SPLIT>[[[[Hypothesis_8]]]]: potter fans rushed to tills in order to purchase the book.<SPLIT>[[[[Label_8]]]]: true<SPLIT>[[[[Premise_9]]]]: alan mulally, boeing's head of the unit, said at the start of the strike that it may cause delivery delays that would give airbus sas an advantage in what is the strongest commercial aircraft market in five years.<SPLIT>[[[[Hypothesis_9]]]]: alan mulally is the owner of boeing.<SPLIT>[[[[Label_9]]]]: false<SPLIT>[[[[Premise_10]]]]: to promote the simpsons movie that will be released july 26, 2007, over a dozen 7-elevens in the united states have been transformed into kwik-e-marts, the grocery store from the popular tv series. most of the other 7-eleven stores will also sell products with brands reminding of the simpsons, such as buzz cola, krustyo's cereal, squishees, and bart simpson's favourite comic book radioactive man, but not duff beer, homer's favourite drink.<SPLIT>[[[[Hypothesis_10]]]]: the simpsons is a show broadcast in america.<SPLIT>[[[[Label_10]]]]: true<SPLIT>[[[[Premise_11]]]]: he is like some great writers, from charles dickens to william faulkner to gabriel garcia marquez: the insistence on the value of the local leads to the universal.<SPLIT>[[[[Hypothesis_11]]]]: gabriel garcia marquez is a nobel prize winner.<SPLIT>[[[[Label_11]]]]: false<SPLIT>[[[[Premise_12]]]]: colin l. powell and laura bush, wife of gov. george w. bush, are to speak on the opening night in philadelphia, while the democrats have tentative plans to have president clinton and hillary rodham clinton address the delegates on the first night in los angeles.<SPLIT>[[[[Hypothesis_12]]]]: the name of george w. bush's wife is laura.<SPLIT>[[[[Label_12]]]]: true<SPLIT>[[[[Premise_13]]]]: after insulting the un committee insinuating they were irrelavant, bush no sooner had approval from congress to use \"\"force/war\"\" if hussein did not permit unfettered access for inspections - and the war ships were moved into place on the gulf.<SPLIT>[[[[Hypothesis_13]]]]: hussein gives un inspectors unfettered access.<SPLIT>[[[[Label_13]]]]: [[true]]<SPLIT>[[[[Premise_14]]]]: while local cab drivers had compliments, not everyone appreciated the company. the chicago cubs decided to cancel their booking after hearing about the con, and j.j. hardy of the milwaukee brewers reported being kept up by fans on wednesday night. the brewers lost their game the following day. the convention closed with next year's date ―june 26-29 ―along with the announcement of a guest: disney animator floyd norman. the event's theme will be \"\"it's a jungle out there!\"\".<SPLIT>[[[[Hypothesis_14]]]]: j.j. hardy has a contract with the milwaukee brewers.<SPLIT>[[[[Label_14]]]]: true<SPLIT>[[[[Premise_15]]]]: for a western european country, the birth rate in finland is high.<SPLIT>[[[[Hypothesis_15]]]]: finland is the european country with the highest birth rate.<SPLIT>[[[[Label_15]]]]: false\"\"\"\n",
    "\n",
    "attack_sample = attack_sample.split(\"<SPLIT>\")\n",
    "demons = [demon.split(\":\")[1].strip('\\n ').strip('[]') for demon in attack_sample]\n",
    "# print(demons)\n",
    "question = (demons[0], demons[1], \"\")\n",
    "icl_examples = []\n",
    "demons = demons[2:]\n",
    "for i in range(len(demons) // 3):\n",
    "    icl_examples.append((demons[i * 3], demons[i * 3 + 1], demons[i * 3 + 2]))\n",
    "\n",
    "template = \"{}\\n The question is: {}. True or False?\\nThe Answer is: {}\"\n",
    "verbalizer = {0: \"true\", 1: \"false\"}\n",
    "\n",
    "demos = []\n",
    "for demo in icl_examples:\n",
    "    demos.append(template.format(demo[0], demo[1], demo[2]))\n",
    "q = template.format(question[0], question[1], \"\").strip()\n",
    "\n",
    "prompt = \"\\n\\n\".join(demos) + \"\\n\\n\" + q\n",
    "\n",
    "# print(prompt)\n",
    "tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "logits = model(**tokenized).logits\n",
    "output = logits[:, -1, :].detach().cpu()\n",
    "\n",
    "print(tokenizer.decode(output.argmax(dim=-1)))\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "output_label = output[:, label_id].softmax(dim=-1)\n",
    "output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 5852], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = [(\"george herbert walker bush (born june 12, 1924) is the former 41st president of the united states of america. almost immediately upon his return from the war in december 1944, george bush married barbara pierce.\", \"the name of george h.w. bush's wife is barbara.\", 0),\n",
    "                 (\"it rewrites the rules of global trade, established by the general agreement on tariffs and trade, or gatt, in 1947, and modified in multiple rounds of negotiations since then.\", \"gatt was formed in 1947.\", 1),\n",
    "                    (\"us military forces are evacuating u.s. citizens and citizens of 72 other countries from liberia at the request of the u.s.\", \"u.s. military evacuated u.s. citizens.\", 0),\n",
    "                    (\"floods are one of europe's most widespread disasters. major flooding has occurred nearly every year somewhere on our continent during the last few decades.\", \"flooding in europe causes major economic losses.\", 1),\n",
    "                    (\"rock stars aerosmith are to hold a free concert in hawaii to placate angry fans who brought a legal case against them. the walk this way hitmakers cancelled a sold-out show in maui two years ago, leaving hundreds of fans out of pocket. they filed a class action case, which claimed the band had pulled out in favour of a bigger gig in chicago and a private show for car dealers in oahu. lawyers for the would-be concert-goers said aerosmith had now agreed to put on a new show, and would pay all expenses.\", \"aerosmith are a rock band.\", 0),\n",
    "                    (\"one reason for increased osteoporosis in developed countries is the sodium-potassium imbalance.\", \"dietary intake of potassium prevents osteoporosis.\", 1)]\n",
    "\n",
    "template = [\"Premise: {}\\nHypothesis: {}\\nPrediction: {}\",\n",
    "            \"{}\\nThe question is: {}. True or False?\\nAnswer: {}\"]\n",
    "verbalizer = {0: \"true\", 1: \"false\"}\n",
    "# Premise: jerusalem, april 2 (xinhua) -- israel's new foreign minister avigdor lieberman was questioned by police on thursday over several criminal allegations, local news service ynet reported.  national fraud unit investigators questioned the deputy premier, who is suspected of bribery, money laundering, fraud and breach of trust, for over seven hours, and another round is in the cards, said the report.  \"lieberman answered all of the questions he was asked, and will continue to do so in the future,\" his attorney yaron kostelitz was quoted as saying.\n",
    "# Hypothesis: avigdor lieberman is the foreign minister of israel.\n",
    "# Prediction: false\n",
    "\n",
    "# Premise: other friends were not surprised at his death. \"i wasn't surprised,\" said george stranahan, a former owner of the woody creek tavern, a favourite haunt of thompson. \"i never expected hunter to die in a hospital bed with tubes coming out of him.\" neighbours have said how his broken leg had prevented him from leaving his house as often as he had liked to. one neighbour and long-standing friend, mike cleverly, said thompson was clearly hobbled by the broken leg. \"medically speaking, he's had a rotten year.\"\n",
    "# Hypothesis: the woody creek tavern is owned by george stranahan.\n",
    "# Prediction: true\n",
    "# \"\"\"\n",
    "\n",
    "questions = [(\"jerusalem, april 2 (xinhua) -- israel's new foreign minister avigdor lieberman was questioned by police on thursday over several criminal allegations, local news service ynet reported.  national fraud unit investigators questioned the deputy premier, who is suspected of bribery, money laundering, fraud and breach of trust, for over seven hours, and another round is in the cards, said the report.  \\\"lieberman answered all of the questions he was asked, and will continue to do so in the future,\\\" his attorney yaron kostelitz was quoted as saying.\", \"avigdor lieberman is the foreign minister of israel.\", 0), \n",
    "            (\"other friends were not surprised at his death. \\\"i wasn\\'t surprised,\\\" said george stranahan, a former owner of the woody creek tavern, a favourite haunt of thompson. \\\"i never expected hunter to die in a hospital bed with tubes coming out of him.\\\" neighbours have said how his broken leg had prevented him from leaving his house as often as he had liked to. one neighbour and long-standing friend, mike cleverly, said thompson was clearly hobbled by the broken leg. \\\"medically speaking, he\\'s had a rotten year.\", \"the woody creek tavern is owned by george stranahan.\", 1)]\n",
    "# question = \"Premise: he also referred to the \\\"illegal\\\" arrest on 31 may of mexican professor maria eugenia ochoa garcia, whom the salvadoran government accused of having connections with the salvadoran guerrillas.\\nHypothesis: professor ochoa garcia is a member of the salvadoran government.\\nPrediction:\"\n",
    "for question in questions:\n",
    "    for tem in template:\n",
    "        demos = []\n",
    "        for demo in demonstration:\n",
    "            demos.append(tem.format(demo[0], demo[1], verbalizer[demo[2]]))\n",
    "        q = tem.format(question[0], question[1], \"\").strip()\n",
    "\n",
    "        prompt = \"\\n\".join(demos) + \"\\n\" + q\n",
    "\n",
    "        tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "        logits = model(**tokenized).logits\n",
    "        output = logits[:, -1, :].detach().cpu()\n",
    "\n",
    "        print(tokenizer.decode(torch.argmax(output, dim=-1).squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of prompt_ids_list:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'positive': 97, 'negative': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "instructions = \"Classify the sentiment of negative and positive.\"\n",
    "icl_samples = [\"Review: is a step down for director gary fleder . \\nSentiment: negative\",\n",
    "               \"Review: the director , tom dey , had spliced together bits and pieces of midnight run and 48 hours ( and , for that matter , shrek ) . \\nSentiment: positive\",\n",
    "                \"Review: from two fatal ailments -- a dearth of vitality and a story that 's shapeless and uninflected . \\nSentiment: negative\",\n",
    "                \"Review: results that are sometimes bracing . \\nSentiment: positive\",\n",
    "                \"Review: plodding soap opera . \\nSentiment: negative\",\n",
    "                \"Review: all-star salute . \\nSentiment: positive\",\n",
    "                \"Review: fit all of pootie tang in between its punchlines . \\nSentiment: negative\",\n",
    "                \"Review: award-winning . \\nSentiment: positive\",\n",
    "                \"Review: deserve better . \\nSentiment: negative\",\n",
    "                \"Review: you actually buy into \\nSentiment: positive\",\n",
    "                \"Review: of cliches that shoplifts shamelessly from farewell-to-innocence movies like the wanderers and a bronx tale without cribbing any of their intelligence . \\nSentiment: negative\",\n",
    "                \"Review: real-life basis is , in fact , so interesting that no embellishment is \\nSentiment: positive\",\n",
    "                \"Review: to insulting the intelligence of anyone who has n't been living under a rock \\nSentiment: negative\",\n",
    "                \"Review: immensely ambitious \\nSentiment: positive\",\n",
    "                \"Review: into the modern rut of narrative banality \\nSentiment: negative\",\n",
    "                \"Review: user-friendly \\nSentiment: positive\"]\n",
    "sample = \"\"\"or doing last year 's taxes with your ex-wife . \\nSentiment:\"\"\"\n",
    "\n",
    "prompt_ids_list = []\n",
    "i = 0\n",
    "# permutation = np.random.permutation(len(icl_samples))\n",
    "# icl_samples = [icl_samples[i] for i in permutation]\n",
    "\n",
    "np.random.seed(1)\n",
    "for order in range(100):\n",
    "    permutation = np.random.permutation(len(icl_samples))\n",
    "    order = [icl_samples[i] for i in permutation]\n",
    "    prompt = '\\n\\n'.join(order) + '\\n\\n' + sample\n",
    "    # prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # prompt_ids = prompt_ids.to('cuda')\n",
    "\n",
    "    prompt_ids_list.append((prompt))\n",
    "    i += 1\n",
    "\n",
    "print('Length of prompt_ids_list: ', len(prompt_ids_list))\n",
    "\n",
    "batch_size = 8\n",
    "prompt_ids_list = [prompt_ids_list[i:i + batch_size] for i in range(0, len(prompt_ids_list), batch_size)]\n",
    "\n",
    "# convert prompt_ids_list to dataloader\n",
    "# prompt_ids_list = torch.utils.data.DataLoader(prompt_ids_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "outputs = []\n",
    "for batch in tqdm(prompt_ids_list):\n",
    "    batch = tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    output = model(batch['input_ids'], return_dict=True).logits\n",
    "    output = output[:, -1, :].detach().cpu()\n",
    "    # outputs += tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist())\n",
    "    outputs += (tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist()))\n",
    "    del batch\n",
    "    \n",
    "print(Counter(outputs))\n",
    "# prompt = '\\n\\n'.join(icl_samples) + '\\n\\n' + sample\n",
    "# prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# prompt_ids = prompt_ids.to('cuda')\n",
    "\n",
    "# output = model(prompt_ids, return_dict=True).logits\n",
    "# print(output.shape)\n",
    "# output = output[:, -1, :]\n",
    "# tokenizer.decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 16403], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([193, 167], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' negative', ' positive']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the sentiment of positive and negative.\n",
    "Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris . .\n",
    "Sentiment: negative\n",
    "Review: suspenseful enough for older kids but not .\n",
    "Sentiment: positive\n",
    "Review: another run-of-the-mill disney sequel intended for the home video market .\n",
    "Sentiment: negative\n",
    "Review: has never been smoother or more confident .\n",
    "Sentiment: positive\n",
    "Review: bad-movie .\n",
    "Sentiment: negative\n",
    "Review: sweetly .\n",
    "Sentiment: positive\n",
    "Review: like an extended dialogue exercise in retard 101 .\n",
    "Sentiment: negative\n",
    "Review: bouquet gives a performance that is masterly . .\n",
    "Sentiment: positive\"\"\"\n",
    "\n",
    "input1 = \"Review: one of creepiest, scariest movies to come along in a long, long time, easily rivaling blair witch or the others. \\nSentiment:\"\n",
    "input2 = \"Review: good movie . \\nSentiment:\"\n",
    "\n",
    "prompt_ids = gpt_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "inputs1 = prompt + input1\n",
    "inputs2 = prompt + input2\n",
    "\n",
    "inputs = [inputs1, inputs2]\n",
    "# inputs = [inputs1, inputs1]\n",
    "\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "input_ids = gpt_tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to('cuda')\n",
    "\n",
    "attention_mask = input_ids.ne(gpt_tokenizer.pad_token_id).float().to('cuda')\n",
    "output = gpt_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "\n",
    "last_non_pad_indices = torch.ne(input_ids, gpt_tokenizer.pad_token_id).sum(-1) - 1\n",
    "print(last_non_pad_indices)\n",
    "output = output[range(output.shape[0]), last_non_pad_indices, :]\n",
    "\n",
    "gpt_tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90, 32000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Classify the sentiment of positive and negative.\\n Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris.Sentiment: negative\\nReview:  suspenseful enough for older kids but not . \\nSentiment: positive\\n\"\n",
    "\n",
    "input = \"Review: the cd is not suitable for children . \\nSentiment:\"\n",
    "# try with the mistral tokenizer and model\n",
    "\n",
    "prompt_ids = mistral_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids = mistral_tokenizer.encode(input, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = torch.cat([prompt_ids, input_ids[:, 1:]], dim=-1).to('cuda')\n",
    "\n",
    "# add 10 padding tokens to the end of the input\n",
    "input_ids = torch.cat([input_ids, torch.ones((1, 10)).long().to('cuda')], dim=-1)\n",
    "\n",
    "attention_mask = input_ids != 1\n",
    "\n",
    "output = mistral_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "print(output.shape)\n",
    "output = output[:, len(input_ids[0]) - 11, :]\n",
    "mistral_tokenizer.decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1598], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_tokenizer(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [353, 5547], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer(\" terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
