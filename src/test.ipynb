{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.74s/it]\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.68s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, truncation_side='left', padding_side='right')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0},use_flash_attention_2=True)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0})\n",
    "flash_attn_model = LlamaForCausalLM.from_pretrained(model_id, device_map={\"\":0},use_flash_attention_2=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, truncation_side='left', padding_side='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "flash_attn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')}\n",
      "{'input_ids': tensor([[    1,   306, 29915,   345,  2355,   263, 12355,   873, 14928,   310,\n",
      "          1302,   535,  8842,   437,   437,   437,   437,  3634,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437,   437,   437,   437,   437,   437,   437,   437,   437,   437,\n",
      "           437]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/tokenization_utils_base.py:2630: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_prompt = \"I've got a lovely bunch of coconuts do do do dooo\"\n",
    "random = \" \".join([\"do\" for _ in range(50)])\n",
    "batched_input_prompt = [\"I've got a lovely bunch of coconuts do do do dooo\", \" \".join([\"do\" for _ in range(50)])]\n",
    "input_prompt_tokenized = tokenizer(input_prompt, return_tensors=\"pt\").to('cuda')\n",
    "input_prompt_padded_tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=\"max_length\", max_length=50).to('cuda')\n",
    "batched_input_prompt_tokenized = tokenizer(batched_input_prompt, return_tensors=\"pt\", padding=True, max_length=50).to('cuda')\n",
    "random_tokenized = tokenizer(random, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "print(input_prompt_tokenized)\n",
    "print(input_prompt_padded_tokenized)\n",
    "print(batched_input_prompt_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Model + Non-Padded Input\n",
      "Loss (no padding): 2.8300979137420654\n",
      "Logits (no padding): tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.3750, -9.8125, -0.3691,  ..., -3.4844, -8.0000, -2.8594],\n",
      "         [-3.7812, -2.7656,  4.0000,  ..., -1.4297, -2.8125, -0.3926],\n",
      "         ...,\n",
      "         [-3.4688, -3.0625,  8.3125,  ...,  0.2559, -2.6875, -2.8125],\n",
      "         [-3.0000, -3.0938,  8.7500,  ...,  0.0806, -2.5156, -2.7656],\n",
      "         [-2.6562, -1.6953,  7.7812,  ..., -0.6484, -3.1562, -2.5000]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids_masked = torch.zeros(input_prompt_padded_tokenized.input_ids.shape, dtype=torch.int64).to('cuda')\n",
    "torch.where(input_prompt_padded_tokenized.input_ids == tokenizer.pad_token_id,\n",
    "            torch.tensor(-100, dtype=torch.int64),\n",
    "            input_prompt_padded_tokenized.input_ids,\n",
    "            out=input_ids_masked)\n",
    "\n",
    "loss1 = model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits1 = model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(f\"Normal Model + Non-Padded Input\")\n",
    "print(f\"Loss (no padding): {loss1}\")\n",
    "print(f\"Logits (no padding): {logits1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attn Model + Non-Padded Input\n",
      "Loss (no padding) with flash attn: 2.8371522426605225\n",
      "Logits (no padding) with flash attn: tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.4375, -9.8750, -0.3555,  ..., -3.5000, -8.0625, -2.9062],\n",
      "         [-3.7812, -2.7812,  4.0625,  ..., -1.3828, -2.7969, -0.3438],\n",
      "         ...,\n",
      "         [-3.4844, -3.0781,  8.3750,  ...,  0.2871, -2.6719, -2.8125],\n",
      "         [-3.0625, -3.1250,  8.6875,  ...,  0.0928, -2.5781, -2.7812],\n",
      "         [-2.6562, -1.6719,  7.7812,  ..., -0.6914, -3.1406, -2.5156]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n",
      "Loss (no padding) with flash attn: 8.834760665893555\n"
     ]
    }
   ],
   "source": [
    "loss1_falsh_attn = flash_attn_model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits1_falsh_attn = flash_attn_model(\n",
    "    input_prompt_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_tokenized.attention_mask,\n",
    "    labels=input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "loss_random = model(\n",
    "    random_tokenized.input_ids,\n",
    "    attention_mask=random_tokenized.attention_mask,\n",
    "    labels=random_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "print(f\"Flash Attn Model + Non-Padded Input\")\n",
    "print(f\"Loss (no padding) with flash attn: {loss1_falsh_attn}\")\n",
    "print(f\"Logits (no padding) with flash attn: {logits1_falsh_attn}\")\n",
    "print(f\"Loss (no padding) with flash attn: {loss_random + loss1_falsh_attn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 32000])\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'''\n",
      "Normal Model + Padded Input\n",
      "Loss (padding): 8.399846076965332\n",
      "Logits (padding): tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.3750, -9.8125, -0.3691,  ..., -3.4844, -8.0000, -2.8594],\n",
      "         [-3.7812, -2.7656,  4.0000,  ..., -1.4297, -2.8125, -0.3926],\n",
      "         ...,\n",
      "         [-4.9062,  8.0000,  5.2500,  ..., -2.2812, -2.2812, -2.7344],\n",
      "         [-5.4062,  3.4219,  5.3125,  ..., -2.6094, -1.1250, -3.4062],\n",
      "         [-5.3125,  2.5312,  5.9375,  ..., -2.2656, -0.9961, -3.6719]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n",
      "Decoded Logits (padding): []\n"
     ]
    }
   ],
   "source": [
    "loss2 = model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2 = model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(logits2.shape)\n",
    "# decode the logits\n",
    "print(tokenizer.decode(logits2.argmax(dim=-1)[0]))\n",
    "\n",
    "print(f\"Normal Model + Padded Input\")\n",
    "print(f\"Loss (padding): {loss2}\")\n",
    "print(f\"Logits (padding): {logits2}\")\n",
    "print(f\"Decoded Logits (padding): {decoded_logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "Flash Attn Model + Padded Input\n",
      "Loss (padding) with flash attn: 16.86910057067871\n",
      "Logits (padding) with flash attn: tensor([[[ 0.1357, -0.1206,  0.3125,  ...,  1.3594,  1.8984,  0.6641],\n",
      "         [-8.4375, -9.8750, -0.3555,  ..., -3.5000, -8.0625, -2.9062],\n",
      "         [-3.7812, -2.7812,  4.0625,  ..., -1.3828, -2.7969, -0.3438],\n",
      "         ...,\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250],\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250],\n",
      "         [ 8.5000, 27.0000,  2.2188,  ...,  4.2188,  1.9766,  3.6250]]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss2_falsh_attn = flash_attn_model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2_falsh_attn = flash_attn_model(\n",
    "    input_prompt_padded_tokenized.input_ids,\n",
    "    attention_mask=input_prompt_padded_tokenized.attention_mask,\n",
    "    labels=input_prompt_padded_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print(tokenizer.decode(logits2_falsh_attn.argmax(dim=-1)[0]))\n",
    "\n",
    "print(f\"Flash Attn Model + Padded Input\")\n",
    "print(f\"Loss (padding) with flash attn: {loss2_falsh_attn}\")\n",
    "print(f\"Logits (padding) with flash attn: {logits2_falsh_attn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Logits\n",
      "------------------\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>''''\n",
      "------------------\n",
      "\n",
      "Normal Model + Padded Input\n",
      "Loss (with padding): 7.247009754180908\n",
      "Logits (with padding): tensor([[ 0.0608, -0.1953,  0.3203,  ...,  1.3281,  1.8359,  0.6094],\n",
      "        [-8.3750, -9.8125, -0.3574,  ..., -3.5000, -8.0625, -2.8750],\n",
      "        [-3.8281, -2.8281,  4.0000,  ..., -1.4375, -2.8438, -0.4102],\n",
      "        ...,\n",
      "        [-5.3438,  3.5625,  5.4062,  ..., -2.5781, -1.0469, -3.3594],\n",
      "        [-5.3750,  2.4375,  5.8438,  ..., -2.2969, -1.0547, -3.7031],\n",
      "        [-5.1250,  3.9844,  5.8750,  ..., -1.9766, -0.9297, -3.1250]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss2 = model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "logits2 = model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print('Decoded Logits')\n",
    "print('------------------')\n",
    "print(tokenizer.decode(logits2.argmax(dim=-1)[0]))\n",
    "print('------------------\\n')\n",
    "\n",
    "print(f\"Normal Model + Padded Input\")\n",
    "print(f\"Loss (with padding): {loss2}\")\n",
    "print(f\"Logits (with padding): {logits2[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Logits\n",
      "------------------\n",
      "Unterscheidung'm been a bitely little of coconuts\n",
      "o do do doo<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "------------------\n",
      "\n",
      "Flash Attn Model + Padded Input\n",
      "Loss (padding input & masking labels): 11.508078575134277\n",
      "Logits (padding input & masking labels): tensor([[ 0.0608, -0.1953,  0.3203,  ...,  1.3281,  1.8359,  0.6094],\n",
      "        [-8.4375, -9.8750, -0.3477,  ..., -3.5312, -8.0625, -2.8906],\n",
      "        [-3.7656, -2.8125,  4.0625,  ..., -1.3672, -2.7812, -0.3457],\n",
      "        ...,\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562],\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562],\n",
      "        [ 8.5000, 27.0000,  2.2344,  ...,  4.2500,  2.0000,  3.6562]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "flash_attn_loss = flash_attn_model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").loss\n",
    "\n",
    "flash_attn_logits = flash_attn_model(\n",
    "    batched_input_prompt_tokenized.input_ids,\n",
    "    attention_mask=batched_input_prompt_tokenized.attention_mask,\n",
    "    labels=batched_input_prompt_tokenized.input_ids\n",
    ").logits\n",
    "\n",
    "print('Decoded Logits')\n",
    "print('------------------')\n",
    "print(tokenizer.decode(flash_attn_logits.argmax(dim=-1)[0]))\n",
    "print('------------------\\n')\n",
    "\n",
    "print(f\"Flash Attn Model + Padded Input\")\n",
    "print(f\"Loss (padding input & masking labels): {flash_attn_loss}\")\n",
    "print(f\"Logits (padding input & masking labels): {flash_attn_logits[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 1, 3],\n",
      "        [0, 0, 0, 1],\n",
      "        [4, 1, 3, 2]]) tensor([[0, 4, 3, 1],\n",
      "        [2, 3, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 5, (3, 4))\n",
    "b = torch.randint(0, 5, (2, 4))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a and b to dtype=torch.float32\n",
    "a = a.float()\n",
    "b = b.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = torch.tensor([[0, 4, 3, 1]])\n",
    "b1 = b1.float()\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5000, 0.0000, 3.7500]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((a[:, None, :] - b1) * (a[:, None, :]), dim=2).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 1., 1., 3.]],\n",
      "\n",
      "        [[0., 0., 0., 1.]],\n",
      "\n",
      "        [[4., 1., 3., 2.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5000,  0.0000,  3.7500],\n",
       "        [ 0.7500, -0.2500,  2.2500]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a[:, None, :])\n",
    "torch.mean((a[:, None, :] - b) * (a[:, None, :]), dim=2).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0217, 0.4867, 0.4916]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_example = [[4.46, 7.57, 7.58]]\n",
    "# tensor_example = tensor_example\n",
    "tensor_example = torch.tensor(tensor_example)\n",
    "\n",
    "# plot the softmax output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_softmax(tensor_example, knn_T=10):\n",
    "#     tensor_example = tensor_example / knn_T\n",
    "#     softmax = torch.nn.Softmax(dim=1)\n",
    "#     softmax_output = softmax(tensor_example)\n",
    "#     print(softmax_output)\n",
    "#     plt.plot(softmax_output.detach().numpy().flatten(), 'o')\n",
    "#     # shot the x axis as discrete values\n",
    "#     plt.xticks(np.arange(16))\n",
    "#     # plot the y from 0 to 1\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_softmax(tensor_example, 100)\n",
    "torch.softmax(tensor_example, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3967], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer(\" positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/ceph_rbd/mvp/src/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mChoose sentiment from terrible or great.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mSentiment:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16, use_flash_attention_2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to('cuda')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpod/mnt/ceph_rbd/mvp/src/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to('cuda')\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_utils.py:3105\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map)\n\u001b[1;32m   3104\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3105\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3107\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   3108\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    967\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    970\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:795\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    796\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    798\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:795\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    796\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    798\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:605\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    601\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m (\n\u001b[1;32m    603\u001b[0m     LlamaAttention(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39m_flash_attn_2_enabled\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 605\u001b[0m     \u001b[39melse\u001b[39;00m LlamaFlashAttention2(config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m    606\u001b[0m )\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    608\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:281\u001b[0m, in \u001b[0;36mLlamaAttention.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size:\n\u001b[1;32m    277\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    278\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhidden_size must be divisible by num_heads (got `hidden_size`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and `num_heads`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim, bias\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mattention_bias)\n\u001b[1;32m    282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mattention_bias)\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mattention_bias)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Choose sentiment from terrible or great.\n",
    "\n",
    "Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris.\n",
    "Sentiment: terrible\n",
    "Review:  suspenseful enough for older kids but not . \n",
    "Sentiment: great\n",
    "\n",
    "Review: the subtle strength of elling is that it never squandering touch with the reality of the grim situation . \n",
    "Sentiment:\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16, use_flash_attention_2=True)\n",
    "\n",
    "# gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "# gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to('cuda')\n",
    "\n",
    "# mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "# mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of prompt_ids_list:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'positive': 97, 'negative': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "instructions = \"Classify the sentiment of negative and positive.\"\n",
    "icl_samples = [\"Review: is a step down for director gary fleder . \\nSentiment: negative\",\n",
    "               \"Review: the director , tom dey , had spliced together bits and pieces of midnight run and 48 hours ( and , for that matter , shrek ) . \\nSentiment: positive\",\n",
    "                \"Review: from two fatal ailments -- a dearth of vitality and a story that 's shapeless and uninflected . \\nSentiment: negative\",\n",
    "                \"Review: results that are sometimes bracing . \\nSentiment: positive\",\n",
    "                \"Review: plodding soap opera . \\nSentiment: negative\",\n",
    "                \"Review: all-star salute . \\nSentiment: positive\",\n",
    "                \"Review: fit all of pootie tang in between its punchlines . \\nSentiment: negative\",\n",
    "                \"Review: award-winning . \\nSentiment: positive\",\n",
    "                \"Review: deserve better . \\nSentiment: negative\",\n",
    "                \"Review: you actually buy into \\nSentiment: positive\",\n",
    "                \"Review: of cliches that shoplifts shamelessly from farewell-to-innocence movies like the wanderers and a bronx tale without cribbing any of their intelligence . \\nSentiment: negative\",\n",
    "                \"Review: real-life basis is , in fact , so interesting that no embellishment is \\nSentiment: positive\",\n",
    "                \"Review: to insulting the intelligence of anyone who has n't been living under a rock \\nSentiment: negative\",\n",
    "                \"Review: immensely ambitious \\nSentiment: positive\",\n",
    "                \"Review: into the modern rut of narrative banality \\nSentiment: negative\",\n",
    "                \"Review: user-friendly \\nSentiment: positive\"]\n",
    "sample = \"\"\"or doing last year 's taxes with your ex-wife . \\nSentiment:\"\"\"\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt_ids_list = []\n",
    "i = 0\n",
    "# permutation = np.random.permutation(len(icl_samples))\n",
    "# icl_samples = [icl_samples[i] for i in permutation]\n",
    "\n",
    "np.random.seed(1)\n",
    "for order in range(100):\n",
    "    permutation = np.random.permutation(len(icl_samples))\n",
    "    order = [icl_samples[i] for i in permutation]\n",
    "    prompt = '\\n\\n'.join(order) + '\\n\\n' + sample\n",
    "    # prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # prompt_ids = prompt_ids.to('cuda')\n",
    "\n",
    "    prompt_ids_list.append((prompt))\n",
    "    i += 1\n",
    "\n",
    "print('Length of prompt_ids_list: ', len(prompt_ids_list))\n",
    "\n",
    "batch_size = 8\n",
    "prompt_ids_list = [prompt_ids_list[i:i + batch_size] for i in range(0, len(prompt_ids_list), batch_size)]\n",
    "\n",
    "# convert prompt_ids_list to dataloader\n",
    "# prompt_ids_list = torch.utils.data.DataLoader(prompt_ids_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "outputs = []\n",
    "for batch in tqdm(prompt_ids_list):\n",
    "    batch = tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    output = model(batch['input_ids'], return_dict=True).logits\n",
    "    output = output[:, -1, :].detach().cpu()\n",
    "    # outputs += tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist())\n",
    "    outputs += (tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist()))\n",
    "    del batch\n",
    "    \n",
    "print(Counter(outputs))\n",
    "# prompt = '\\n\\n'.join(icl_samples) + '\\n\\n' + sample\n",
    "# prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# prompt_ids = prompt_ids.to('cuda')\n",
    "\n",
    "# output = model(prompt_ids, return_dict=True).logits\n",
    "# print(output.shape)\n",
    "# output = output[:, -1, :]\n",
    "# tokenizer.decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 16403], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([193, 167], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' negative', ' positive']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the sentiment of positive and negative.\n",
    "Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris . .\n",
    "Sentiment: negative\n",
    "Review: suspenseful enough for older kids but not .\n",
    "Sentiment: positive\n",
    "Review: another run-of-the-mill disney sequel intended for the home video market .\n",
    "Sentiment: negative\n",
    "Review: has never been smoother or more confident .\n",
    "Sentiment: positive\n",
    "Review: bad-movie .\n",
    "Sentiment: negative\n",
    "Review: sweetly .\n",
    "Sentiment: positive\n",
    "Review: like an extended dialogue exercise in retard 101 .\n",
    "Sentiment: negative\n",
    "Review: bouquet gives a performance that is masterly . .\n",
    "Sentiment: positive\"\"\"\n",
    "\n",
    "input1 = \"Review: one of creepiest, scariest movies to come along in a long, long time, easily rivaling blair witch or the others. \\nSentiment:\"\n",
    "input2 = \"Review: good movie . \\nSentiment:\"\n",
    "\n",
    "prompt_ids = gpt_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "inputs1 = prompt + input1\n",
    "inputs2 = prompt + input2\n",
    "\n",
    "inputs = [inputs1, inputs2]\n",
    "# inputs = [inputs1, inputs1]\n",
    "\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "input_ids = gpt_tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to('cuda')\n",
    "\n",
    "attention_mask = input_ids.ne(gpt_tokenizer.pad_token_id).float().to('cuda')\n",
    "output = gpt_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "\n",
    "last_non_pad_indices = torch.ne(input_ids, gpt_tokenizer.pad_token_id).sum(-1) - 1\n",
    "print(last_non_pad_indices)\n",
    "output = output[range(output.shape[0]), last_non_pad_indices, :]\n",
    "\n",
    "gpt_tokenizer.batch_decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90, 32000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Classify the sentiment of positive and negative.\\n Review: i would recommend big bad love only to winger fans who have missed her since 1995 's forget paris.Sentiment: negative\\nReview:  suspenseful enough for older kids but not . \\nSentiment: positive\\n\"\n",
    "\n",
    "input = \"Review: the cd is not suitable for children . \\nSentiment:\"\n",
    "# try with the mistral tokenizer and model\n",
    "\n",
    "prompt_ids = mistral_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids = mistral_tokenizer.encode(input, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = torch.cat([prompt_ids, input_ids[:, 1:]], dim=-1).to('cuda')\n",
    "\n",
    "# add 10 padding tokens to the end of the input\n",
    "input_ids = torch.cat([input_ids, torch.ones((1, 10)).long().to('cuda')], dim=-1)\n",
    "\n",
    "attention_mask = input_ids != 1\n",
    "\n",
    "output = mistral_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "print(output.shape)\n",
    "output = output[:, len(input_ids[0]) - 11, :]\n",
    "mistral_tokenizer.decode(torch.argmax(output, dim=-1).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1598], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_tokenizer(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [353, 5547], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer(\" terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
