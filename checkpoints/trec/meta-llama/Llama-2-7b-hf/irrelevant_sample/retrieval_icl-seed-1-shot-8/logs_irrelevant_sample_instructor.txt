1
Namespace(adv_augment=0, alpha=None, attack_name='irrelevant_sample', batch_size=4, beta=1.0, checkpoint_interval=1000, config_file=None, data_dir='./data', dataset='trec', dataset_path=None, ensemble_num=1, epsilon=1.0, examples_per_label=1, fix_dist=False, is_quantized=False, knn_T=None, knn_k=None, knn_model='bert-base-uncased', local_rank=0, local_world_size=1, lr=1e-05, mask_augment=False, mask_prob=0.15, mask_ratio=0.3, max_length=1024, max_percent_words=0.33, mode='attack', model='meta-llama/Llama-2-7b-hf', model_dir='./checkpoints/trec/meta-llama/Llama-2-7b-hf/irrelevant_sample/retrieval_icl-seed-1-shot-8_instructor', model_id='0', model_type='retrieval_icl', norm='l2', num_epochs=20, num_examples=1000, num_iter=1, num_labels=6, num_template=-1, path='None', patience=10, pool_label_words='max', pool_templates='mean', precision='bfloat16', prompt_num=2, query_budget=-1, replace_ratio=0.1, retrieve_method='instructor', sampled_num=1, save_icl_examples_path=None, seed=1, shot=8, split='test', template_file='configs/templates_mnli.yaml', tindex=0, train_epoch=30, train_size=0.95, val_size=0.05, verbalizer_file='configs/verbalizer_mnli.yaml', weight_decay=0.01)
meta-llama/Llama-2-7b-hf
Model Directory: ./checkpoints/trec/meta-llama/Llama-2-7b-hf/irrelevant_sample/retrieval_icl-seed-1-shot-8_instructor
1 Physical GPUs, 1 Logical GPUs
Namespace(adv_augment=0, alpha=None, attack_name='irrelevant_sample', batch_size=4, beta=1.0, cache_dir='./checkpoints/trec/meta-llama/Llama-2-7b-hf/irrelevant_sample/retrieval_icl-seed-1-shot-8_instructor/cache', checkpoint_interval=1000, config_file=None, data_dir='./data', dataset='trec', dataset_path=None, ensemble_num=1, epsilon=1.0, examples_per_label=1, fix_dist=False, is_quantized=False, knn_T=None, knn_k=None, knn_model='bert-base-uncased', local_rank=0, local_world_size=1, lr=1e-05, mask_augment=False, mask_prob=0.15, mask_ratio=0.3, max_length=1024, max_percent_words=0.33, mode='attack', model='meta-llama/Llama-2-7b-hf', model_dir='./checkpoints/trec/meta-llama/Llama-2-7b-hf/irrelevant_sample/retrieval_icl-seed-1-shot-8_instructor', model_id='0', model_type='retrieval_icl', norm='l2', num_epochs=20, num_examples=1000, num_iter=1, num_labels=6, num_template=-1, path='None', patience=10, pool_label_words='max', pool_templates='mean', precision='bfloat16', prompt_num=2, query_budget=-1, ralm_save_path='./data/ralm/trec_instructor_ood.pkl', replace_ratio=0.1, retrieve_method='instructor', sampled_num=1, save_icl_examples_path=None, seed=1, shot=8, split='test', template_file='configs/templates_mnli.yaml', tindex=0, train_epoch=30, train_size=0.95, val_size=0.05, verbalizer_file='configs/verbalizer_mnli.yaml', weight_decay=0.01)
Finished loading ood dataset
{'sentence': 'what is the student population at the university of massachusetts in amherst ?', 'label': 5, 'idx': 0}
Finished replacing ood dataset
{'sentence': 'what is the student population at the university of massachusetts in amherst ?', 'label': 5, 'idx': 0}
load INSTRUCTOR_Transformer
max_seq_length  512
Length of anchor subsample 24
Length of icl examples 3
Finished loading model
Finished encoding anchor data
anchor_data_embeddings shape: torch.Size([5179, 768])
query shape: 500
query_embedding shape: torch.Size([500, 768])
cos_scores shape: torch.Size([500, 5179])
Saving retrieved examples to ./data/ralm/trec_instructor_ood.pkl
