1+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8
textattack: Updating TextAttack package dependencies.
textattack: Downloading NLTK required packages.
2024-03-07 00:19:49.594159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:19:50.408291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package omw to /root/nltk_data...
[nltk_data] Downloading package universal_tagset to /root/nltk_data...
[nltk_data]   Unzipping taggers/universal_tagset.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.
  0%|          | 0.00/481M [00:00<?, ?B/s]  0%|          | 52.2k/481M [00:00<26:05, 308kB/s]  0%|          | 126k/481M [00:00<21:07, 380kB/s]   0%|          | 198k/481M [00:00<20:12, 397kB/s]  0%|          | 261k/481M [00:00<20:46, 386kB/s]  0%|          | 333k/481M [00:00<20:10, 398kB/s]  0%|          | 396k/481M [00:01<20:38, 388kB/s]  0%|          | 509k/481M [00:01<16:50, 476kB/s]  0%|          | 602k/481M [00:01<16:07, 497kB/s]  0%|          | 684k/481M [00:01<16:19, 491kB/s]  0%|          | 855k/481M [00:01<12:22, 647kB/s]  0%|          | 1.61M/481M [00:01<04:27, 1.80MB/s]  1%|          | 3.59M/481M [00:01<01:27, 5.44MB/s]  1%|▏         | 7.21M/481M [00:02<00:40, 11.7MB/s]  2%|▏         | 10.6M/481M [00:02<00:29, 16.2MB/s]  3%|▎         | 14.7M/481M [00:02<00:21, 22.1MB/s]  4%|▍         | 18.4M/481M [00:02<00:18, 25.5MB/s]  4%|▍         | 21.6M/481M [00:02<00:16, 27.3MB/s]  5%|▌         | 25.1M/481M [00:02<00:15, 29.2MB/s]  6%|▌         | 28.4M/481M [00:02<00:15, 29.7MB/s]  7%|▋         | 32.2M/481M [00:02<00:13, 32.2MB/s]  7%|▋         | 35.8M/481M [00:02<00:13, 32.5MB/s]  8%|▊         | 39.1M/481M [00:03<00:13, 32.3MB/s]  9%|▉         | 42.8M/481M [00:03<00:13, 33.6MB/s] 10%|▉         | 46.2M/481M [00:03<00:13, 33.0MB/s] 10%|█         | 50.3M/481M [00:03<00:12, 35.4MB/s] 11%|█         | 53.9M/481M [00:03<00:12, 34.1MB/s] 12%|█▏        | 57.3M/481M [00:03<00:12, 34.0MB/s] 13%|█▎        | 61.1M/481M [00:03<00:11, 35.1MB/s] 13%|█▎        | 64.6M/481M [00:03<00:12, 34.2MB/s] 14%|█▍        | 68.2M/481M [00:03<00:12, 33.6MB/s] 15%|█▍        | 71.9M/481M [00:04<00:12, 33.7MB/s] 16%|█▌        | 75.9M/481M [00:04<00:11, 35.5MB/s] 17%|█▋        | 79.5M/481M [00:04<00:11, 34.5MB/s] 17%|█▋        | 83.1M/481M [00:04<00:11, 33.9MB/s] 18%|█▊        | 86.9M/481M [00:04<00:11, 34.3MB/s] 19%|█▉        | 90.6M/481M [00:04<00:11, 33.4MB/s] 20%|█▉        | 94.7M/481M [00:04<00:11, 33.5MB/s] 20%|██        | 98.4M/481M [00:04<00:11, 34.2MB/s] 21%|██▏       | 103M/481M [00:04<00:10, 34.9MB/s]  22%|██▏       | 107M/481M [00:05<00:10, 34.3MB/s] 23%|██▎       | 110M/481M [00:05<00:10, 33.9MB/s] 24%|██▍       | 115M/481M [00:05<00:10, 35.1MB/s] 25%|██▍       | 118M/481M [00:05<00:10, 34.1MB/s] 25%|██▌       | 122M/481M [00:05<00:10, 34.7MB/s] 26%|██▌       | 126M/481M [00:05<00:09, 35.5MB/s] 27%|██▋       | 130M/481M [00:05<00:10, 32.7MB/s] 28%|██▊       | 134M/481M [00:05<00:09, 35.4MB/s] 29%|██▊       | 138M/481M [00:05<00:09, 35.5MB/s] 29%|██▉       | 141M/481M [00:06<00:09, 34.7MB/s] 30%|███       | 145M/481M [00:06<00:10, 33.3MB/s] 31%|███       | 149M/481M [00:06<00:09, 35.0MB/s] 32%|███▏      | 152M/481M [00:06<00:09, 34.6MB/s] 32%|███▏      | 156M/481M [00:06<00:08, 36.3MB/s] 33%|███▎      | 160M/481M [00:06<00:08, 36.0MB/s] 34%|███▍      | 164M/481M [00:06<00:09, 34.3MB/s] 35%|███▍      | 167M/481M [00:06<00:09, 33.9MB/s] 35%|███▌      | 171M/481M [00:06<00:08, 34.5MB/s] 36%|███▋      | 175M/481M [00:06<00:08, 34.8MB/s] 37%|███▋      | 179M/481M [00:07<00:08, 35.8MB/s] 38%|███▊      | 182M/481M [00:07<00:08, 33.4MB/s] 39%|███▊      | 186M/481M [00:07<00:08, 34.3MB/s] 39%|███▉      | 189M/481M [00:07<00:08, 33.8MB/s] 40%|████      | 193M/481M [00:07<00:08, 35.7MB/s] 41%|████      | 197M/481M [00:07<00:07, 35.7MB/s] 42%|████▏     | 201M/481M [00:07<00:08, 34.3MB/s] 42%|████▏     | 204M/481M [00:07<00:08, 33.2MB/s] 43%|████▎     | 208M/481M [00:07<00:07, 34.4MB/s] 44%|████▍     | 212M/481M [00:08<00:07, 34.1MB/s] 45%|████▍     | 216M/481M [00:08<00:07, 36.0MB/s] 46%|████▌     | 220M/481M [00:08<00:07, 34.5MB/s] 46%|████▋     | 223M/481M [00:08<00:07, 35.3MB/s] 47%|████▋     | 227M/481M [00:08<00:07, 33.5MB/s] 48%|████▊     | 231M/481M [00:08<00:07, 35.1MB/s] 49%|████▊     | 235M/481M [00:08<00:07, 34.8MB/s] 49%|████▉     | 238M/481M [00:08<00:07, 33.5MB/s] 50%|█████     | 241M/481M [00:08<00:08, 29.8MB/s] 51%|█████     | 244M/481M [00:09<00:08, 28.2MB/s] 52%|█████▏    | 248M/481M [00:09<00:08, 26.2MB/s] 52%|█████▏    | 252M/481M [00:09<00:07, 29.6MB/s] 53%|█████▎    | 255M/481M [00:09<00:08, 26.7MB/s] 54%|█████▍    | 259M/481M [00:09<00:07, 29.4MB/s] 55%|█████▍    | 263M/481M [00:09<00:08, 26.7MB/s] 55%|█████▌    | 267M/481M [00:09<00:07, 29.9MB/s] 56%|█████▌    | 270M/481M [00:10<00:07, 27.1MB/s] 57%|█████▋    | 274M/481M [00:10<00:06, 30.1MB/s] 58%|█████▊    | 278M/481M [00:10<00:07, 27.5MB/s] 58%|█████▊    | 282M/481M [00:10<00:06, 30.3MB/s] 59%|█████▉    | 285M/481M [00:10<00:07, 27.9MB/s] 60%|██████    | 289M/481M [00:10<00:06, 30.8MB/s] 61%|██████    | 293M/481M [00:10<00:06, 28.2MB/s] 62%|██████▏   | 297M/481M [00:10<00:05, 31.1MB/s] 62%|██████▏   | 301M/481M [00:11<00:06, 28.6MB/s] 63%|██████▎   | 305M/481M [00:11<00:05, 31.4MB/s] 64%|██████▍   | 309M/481M [00:11<00:05, 28.8MB/s] 65%|██████▍   | 313M/481M [00:11<00:05, 31.7MB/s] 66%|██████▌   | 316M/481M [00:11<00:05, 28.5MB/s] 67%|██████▋   | 320M/481M [00:11<00:05, 31.3MB/s] 67%|██████▋   | 324M/481M [00:11<00:05, 28.5MB/s] 68%|██████▊   | 328M/481M [00:11<00:04, 30.9MB/s] 69%|██████▉   | 332M/481M [00:12<00:05, 28.6MB/s] 70%|██████▉   | 336M/481M [00:12<00:04, 31.4MB/s] 70%|███████   | 339M/481M [00:12<00:04, 32.7MB/s] 71%|███████   | 343M/481M [00:12<00:04, 29.5MB/s] 72%|███████▏  | 347M/481M [00:12<00:04, 32.6MB/s] 73%|███████▎  | 350M/481M [00:12<00:03, 33.4MB/s] 74%|███████▎  | 354M/481M [00:12<00:04, 30.2MB/s] 74%|███████▍  | 358M/481M [00:12<00:03, 33.2MB/s] 75%|███████▌  | 362M/481M [00:12<00:03, 34.3MB/s] 76%|███████▌  | 366M/481M [00:13<00:03, 31.1MB/s] 77%|███████▋  | 369M/481M [00:13<00:03, 32.1MB/s] 77%|███████▋  | 373M/481M [00:13<00:03, 33.2MB/s] 78%|███████▊  | 377M/481M [00:13<00:03, 30.7MB/s] 79%|███████▉  | 381M/481M [00:13<00:02, 33.8MB/s] 80%|███████▉  | 385M/481M [00:13<00:02, 35.3MB/s] 81%|████████  | 388M/481M [00:13<00:02, 31.2MB/s] 82%|████████▏ | 392M/481M [00:13<00:02, 33.8MB/s] 82%|████████▏ | 396M/481M [00:14<00:02, 33.5MB/s] 83%|████████▎ | 400M/481M [00:14<00:02, 33.5MB/s] 84%|████████▍ | 403M/481M [00:14<00:02, 32.8MB/s] 85%|████████▍ | 407M/481M [00:14<00:02, 34.8MB/s] 85%|████████▌ | 411M/481M [00:14<00:02, 34.1MB/s] 86%|████████▌ | 414M/481M [00:14<00:02, 32.4MB/s] 87%|████████▋ | 418M/481M [00:14<00:01, 32.4MB/s] 88%|████████▊ | 421M/481M [00:14<00:01, 34.2MB/s] 88%|████████▊ | 425M/481M [00:14<00:01, 31.2MB/s] 89%|████████▉ | 429M/481M [00:15<00:01, 33.6MB/s] 90%|█████████ | 433M/481M [00:15<00:01, 35.7MB/s] 91%|█████████ | 437M/481M [00:15<00:01, 32.0MB/s] 92%|█████████▏| 441M/481M [00:15<00:01, 34.9MB/s] 92%|█████████▏| 445M/481M [00:15<00:01, 35.2MB/s] 93%|█████████▎| 449M/481M [00:15<00:01, 31.7MB/s] 94%|█████████▍| 453M/481M [00:15<00:00, 33.9MB/s] 95%|█████████▍| 457M/481M [00:15<00:00, 36.6MB/s] 96%|█████████▌| 461M/481M [00:15<00:00, 32.5MB/s] 97%|█████████▋| 465M/481M [00:16<00:00, 34.5MB/s] 97%|█████████▋| 469M/481M [00:16<00:00, 36.7MB/s] 98%|█████████▊| 473M/481M [00:16<00:00, 33.3MB/s] 99%|█████████▉| 477M/481M [00:16<00:00, 34.2MB/s]100%|█████████▉| 481M/481M [00:16<00:00, 35.9MB/s]100%|██████████| 481M/481M [00:16<00:00, 29.1MB/s]
textattack: Unzipping file /root/.cache/textattack/tmpfip9kcux.zip to /root/.cache/textattack/word_embeddings/paragramcf.
textattack: Successfully saved word_embeddings/paragramcf to cache.
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 21.6MB/s]                   2024-03-07 00:20:19.408896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:19.417909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:19.420350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:19.435876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:19.438269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:19.440643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:20.298126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:20.299761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:20.301231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:20:20.302699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0

Downloading readme:   0%|          | 0.00/255 [00:00<?, ?B/s]Downloading readme: 100%|██████████| 255/255 [00:00<00:00, 2.46MB/s]
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/493k [00:00<?, ?B/s][A
Downloading data: 100%|██████████| 493k/493k [00:00<00:00, 1.45MB/s][ADownloading data: 100%|██████████| 493k/493k [00:00<00:00, 1.45MB/s]
Downloading data files:  50%|█████     | 1/2 [00:00<00:00,  2.91it/s]
Downloading data:   0%|          | 0.00/52.1k [00:00<?, ?B/s][A
Downloading data: 100%|██████████| 52.1k/52.1k [00:00<00:00, 508kB/s][ADownloading data: 100%|██████████| 52.1k/52.1k [00:00<00:00, 507kB/s]
Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  4.45it/s]
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 2353.71it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3394 examples [00:00, 472452.55 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 376 examples [00:00, 220075.12 examples/s]
Map:   0%|          | 0/3394 [00:00<?, ? examples/s]Map:  56%|█████▌    | 1895/3394 [00:00<00:00, 18827.70 examples/s]Map: 100%|██████████| 3394/3394 [00:00<00:00, 18976.47 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 14273.44 examples/s]
Map:   0%|          | 0/3224 [00:00<?, ? examples/s]Map:  34%|███▎      | 1082/3224 [00:00<00:00, 10751.67 examples/s]Map:  70%|██████▉   | 2243/3224 [00:00<00:00, 11250.02 examples/s]Map: 100%|██████████| 3224/3224 [00:00<00:00, 11179.26 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 17489.64 examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]Map: 100%|██████████| 170/170 [00:00<00:00, 10083.03 examples/s]
Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 233kB/s]
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.4MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.94MB/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.93MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 370kB/s]
Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 609/609 [00:00<00:00, 173kB/s]
Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 13.1MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   0%|          | 21.0M/9.98G [00:00<00:50, 198MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 52.4M/9.98G [00:00<00:40, 244MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:38, 257MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 115M/9.98G [00:00<00:37, 262MB/s] [A
Downloading (…)of-00002.safetensors:   1%|▏         | 147M/9.98G [00:00<00:37, 265MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 178M/9.98G [00:00<00:36, 267MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:36, 269MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 241M/9.98G [00:00<00:36, 267MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 273M/9.98G [00:01<00:36, 269MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 304M/9.98G [00:01<00:35, 270MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 336M/9.98G [00:01<00:35, 271MB/s][A
Downloading (…)of-00002.safetensors:   4%|▎         | 367M/9.98G [00:01<00:35, 270MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 398M/9.98G [00:01<00:35, 269MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:35, 270MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 461M/9.98G [00:01<00:35, 270MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 493M/9.98G [00:01<00:35, 271MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 524M/9.98G [00:01<00:35, 264MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 556M/9.98G [00:02<00:38, 246MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 587M/9.98G [00:02<00:40, 234MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 619M/9.98G [00:02<00:41, 227MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 650M/9.98G [00:02<00:41, 225MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:02<00:41, 223MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 713M/9.98G [00:02<00:41, 221MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 744M/9.98G [00:03<00:42, 217MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:03<00:40, 227MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 807M/9.98G [00:03<00:38, 236MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 839M/9.98G [00:03<00:37, 240MB/s][A
Downloading (…)of-00002.safetensors:   9%|▊         | 870M/9.98G [00:03<00:37, 243MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 902M/9.98G [00:03<00:36, 246MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 933M/9.98G [00:03<00:36, 248MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 965M/9.98G [00:03<00:36, 248MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 996M/9.98G [00:03<00:35, 252MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 1.03G/9.98G [00:04<00:35, 255MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.06G/9.98G [00:04<00:34, 260MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.09G/9.98G [00:04<00:33, 263MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.12G/9.98G [00:04<00:35, 250MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:04<00:36, 239MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:04<00:38, 229MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.22G/9.98G [00:04<00:39, 225MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.25G/9.98G [00:05<00:39, 221MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.28G/9.98G [00:05<00:39, 219MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.31G/9.98G [00:05<00:39, 217MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.34G/9.98G [00:05<00:39, 217MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.37G/9.98G [00:05<00:40, 214MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:05<00:39, 214MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.44G/9.98G [00:05<00:39, 214MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:06<00:39, 214MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.50G/9.98G [00:06<00:39, 216MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:06<00:39, 217MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.56G/9.98G [00:06<00:39, 215MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.59G/9.98G [00:06<00:38, 215MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▋        | 1.63G/9.98G [00:06<00:38, 214MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.66G/9.98G [00:06<00:39, 213MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.69G/9.98G [00:07<00:38, 214MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:07<00:38, 215MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.75G/9.98G [00:07<00:38, 214MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:07<00:38, 214MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.81G/9.98G [00:07<00:38, 214MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:07<00:38, 213MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.88G/9.98G [00:08<00:52, 153MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:08<00:46, 175MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:08<00:41, 193MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.97G/9.98G [00:08<00:38, 207MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.00G/9.98G [00:08<00:36, 218MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:08<00:35, 226MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.07G/9.98G [00:08<00:33, 233MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:09<00:33, 238MB/s][A
Downloading (…)of-00002.safetensors:  21%|██▏       | 2.13G/9.98G [00:09<00:32, 243MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:09<00:31, 245MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.19G/9.98G [00:09<00:31, 247MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.22G/9.98G [00:09<00:31, 248MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.25G/9.98G [00:09<00:30, 249MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.29G/9.98G [00:09<00:30, 251MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.32G/9.98G [00:09<00:30, 250MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:10<00:30, 250MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.38G/9.98G [00:10<00:30, 252MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:10<00:30, 249MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.44G/9.98G [00:10<00:30, 249MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 2.47G/9.98G [00:10<00:29, 251MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.51G/9.98G [00:10<00:29, 251MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.54G/9.98G [00:10<00:29, 251MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.57G/9.98G [00:10<00:29, 252MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.60G/9.98G [00:11<00:29, 252MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 2.63G/9.98G [00:11<00:29, 250MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.66G/9.98G [00:11<00:29, 250MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.69G/9.98G [00:11<00:29, 250MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:11<00:28, 250MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.76G/9.98G [00:11<00:28, 251MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.79G/9.98G [00:11<00:28, 250MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.82G/9.98G [00:11<00:28, 250MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:12<00:28, 250MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.88G/9.98G [00:12<00:28, 251MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:12<00:28, 251MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.95G/9.98G [00:12<00:28, 249MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:12<00:28, 249MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 3.01G/9.98G [00:12<00:30, 231MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 3.04G/9.98G [00:12<00:31, 219MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:13<00:30, 228MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:13<00:29, 234MB/s][A
Downloading (…)of-00002.safetensors:  31%|███▏      | 3.14G/9.98G [00:13<00:28, 240MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.17G/9.98G [00:13<00:28, 239MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.20G/9.98G [00:13<00:27, 244MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:13<00:27, 249MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.26G/9.98G [00:13<00:26, 251MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.29G/9.98G [00:13<00:26, 253MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.32G/9.98G [00:14<00:25, 260MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:14<00:24, 267MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:14<00:24, 273MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.42G/9.98G [00:14<00:24, 268MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.45G/9.98G [00:14<00:24, 263MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:14<00:24, 271MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 3.51G/9.98G [00:14<00:23, 276MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:14<00:22, 280MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.58G/9.98G [00:14<00:23, 276MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.61G/9.98G [00:15<00:23, 273MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▋      | 3.64G/9.98G [00:15<00:23, 275MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.67G/9.98G [00:15<00:22, 278MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.70G/9.98G [00:15<00:22, 279MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:15<00:22, 277MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.76G/9.98G [00:15<00:22, 278MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.80G/9.98G [00:15<00:22, 280MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.83G/9.98G [00:15<00:22, 272MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▊      | 3.86G/9.98G [00:15<00:22, 273MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.89G/9.98G [00:16<00:23, 264MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:16<00:22, 263MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.95G/9.98G [00:16<00:22, 264MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:16<00:22, 267MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 4.02G/9.98G [00:16<00:22, 265MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:16<00:22, 258MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.08G/9.98G [00:16<00:22, 265MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:16<00:21, 267MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.14G/9.98G [00:17<00:21, 273MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.17G/9.98G [00:17<00:21, 273MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.20G/9.98G [00:17<00:21, 273MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:17<00:20, 275MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.27G/9.98G [00:17<00:20, 274MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:17<00:20, 278MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.33G/9.98G [00:17<00:20, 275MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:17<00:20, 269MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.39G/9.98G [00:17<00:21, 263MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:18<00:21, 259MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:18<00:21, 256MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.49G/9.98G [00:18<00:21, 253MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 4.52G/9.98G [00:18<00:21, 253MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:18<00:21, 250MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.58G/9.98G [00:18<00:22, 244MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:18<00:21, 245MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.65G/9.98G [00:18<00:21, 246MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:19<00:21, 247MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.71G/9.98G [00:19<00:21, 243MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:19<00:21, 245MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.77G/9.98G [00:19<00:21, 244MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.80G/9.98G [00:19<00:21, 246MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.83G/9.98G [00:19<00:20, 247MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:19<00:20, 246MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.90G/9.98G [00:20<00:34, 146MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:20<00:30, 166MB/s][A
Downloading (…)of-00002.safetensors:  50%|████▉     | 4.96G/9.98G [00:20<00:27, 183MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:20<00:25, 198MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:20<00:23, 210MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.05G/9.98G [00:20<00:23, 213MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:21<00:21, 224MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████▏    | 5.12G/9.98G [00:21<00:20, 232MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:21<00:20, 237MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.18G/9.98G [00:21<00:20, 229MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:21<00:20, 235MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:21<00:19, 239MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.27G/9.98G [00:21<00:19, 243MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.31G/9.98G [00:21<00:19, 244MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:22<00:19, 242MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.37G/9.98G [00:22<00:19, 239MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.40G/9.98G [00:22<00:18, 242MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.43G/9.98G [00:22<00:18, 246MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.46G/9.98G [00:22<00:18, 248MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:22<00:17, 250MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.53G/9.98G [00:23<00:26, 166MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.56G/9.98G [00:23<00:23, 186MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:23<00:21, 206MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▋    | 5.62G/9.98G [00:23<00:19, 224MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.65G/9.98G [00:23<00:18, 238MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:23<00:17, 248MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.71G/9.98G [00:23<00:16, 253MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:23<00:17, 249MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.78G/9.98G [00:24<00:17, 241MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.81G/9.98G [00:24<00:17, 244MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▊    | 5.84G/9.98G [00:24<00:16, 245MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.87G/9.98G [00:24<00:16, 247MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.90G/9.98G [00:24<00:16, 245MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.93G/9.98G [00:24<00:16, 247MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.97G/9.98G [00:24<00:16, 246MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 6.00G/9.98G [00:24<00:16, 240MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:25<00:16, 243MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.06G/9.98G [00:25<00:15, 246MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.09G/9.98G [00:25<00:15, 247MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 6.12G/9.98G [00:25<00:15, 246MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:25<00:15, 245MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.19G/9.98G [00:25<00:15, 245MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:25<00:15, 245MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.25G/9.98G [00:25<00:15, 246MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.28G/9.98G [00:26<00:15, 246MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:26<00:14, 247MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:26<00:14, 248MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:26<00:14, 248MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.41G/9.98G [00:26<00:14, 247MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.44G/9.98G [00:26<00:14, 245MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.47G/9.98G [00:26<00:14, 246MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.50G/9.98G [00:26<00:14, 247MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.53G/9.98G [00:27<00:13, 248MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.56G/9.98G [00:27<00:13, 248MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.60G/9.98G [00:27<00:13, 248MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:27<00:13, 247MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.66G/9.98G [00:27<00:13, 245MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.69G/9.98G [00:27<00:13, 245MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.72G/9.98G [00:27<00:13, 235MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.75G/9.98G [00:28<00:13, 238MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.78G/9.98G [00:28<00:13, 239MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.82G/9.98G [00:28<00:13, 239MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▊   | 6.85G/9.98G [00:28<00:13, 240MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.88G/9.98G [00:28<00:13, 235MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:28<00:12, 238MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.94G/9.98G [00:28<00:12, 239MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:28<00:12, 241MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:29<00:12, 242MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.04G/9.98G [00:29<00:12, 242MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.07G/9.98G [00:29<00:12, 236MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.10G/9.98G [00:29<00:12, 236MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.13G/9.98G [00:29<00:12, 237MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:29<00:11, 239MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.19G/9.98G [00:29<00:11, 240MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:30<00:11, 240MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.26G/9.98G [00:30<00:11, 234MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.29G/9.98G [00:30<00:11, 234MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.32G/9.98G [00:30<00:11, 233MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:30<00:11, 235MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:30<00:10, 238MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.41G/9.98G [00:30<00:10, 240MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:30<00:10, 239MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:31<00:10, 231MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.51G/9.98G [00:31<00:10, 230MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:31<00:10, 232MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.57G/9.98G [00:31<00:10, 236MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:31<00:10, 230MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.63G/9.98G [00:31<00:09, 234MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.67G/9.98G [00:31<00:09, 236MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.70G/9.98G [00:32<00:09, 238MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.73G/9.98G [00:32<00:09, 233MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:32<00:09, 234MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.79G/9.98G [00:32<00:09, 236MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:32<00:09, 237MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▊  | 7.85G/9.98G [00:32<00:08, 236MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.89G/9.98G [00:32<00:08, 239MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:32<00:08, 241MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.95G/9.98G [00:33<00:12, 163MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.98G/9.98G [00:33<00:11, 181MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:33<00:09, 197MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.04G/9.98G [00:33<00:09, 209MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:33<00:08, 220MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.11G/9.98G [00:33<00:08, 226MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:34<00:07, 231MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.17G/9.98G [00:34<00:07, 235MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.20G/9.98G [00:34<00:07, 240MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.23G/9.98G [00:34<00:07, 246MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.26G/9.98G [00:34<00:06, 251MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.29G/9.98G [00:34<00:06, 254MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.33G/9.98G [00:34<00:06, 252MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.36G/9.98G [00:34<00:06, 250MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.39G/9.98G [00:35<00:06, 249MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:35<00:06, 248MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.45G/9.98G [00:35<00:06, 248MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:35<00:06, 247MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.51G/9.98G [00:35<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:35<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.58G/9.98G [00:35<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [00:35<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.64G/9.98G [00:36<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:36<00:05, 246MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.70G/9.98G [00:36<00:05, 247MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:36<00:05, 245MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.77G/9.98G [00:36<00:04, 248MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:36<00:04, 250MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.83G/9.98G [00:36<00:04, 254MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:36<00:04, 255MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.89G/9.98G [00:37<00:04, 255MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:37<00:04, 250MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.95G/9.98G [00:37<00:04, 241MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:37<00:04, 242MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 9.02G/9.98G [00:37<00:03, 243MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:37<00:03, 244MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.08G/9.98G [00:37<00:03, 244MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████▏| 9.11G/9.98G [00:37<00:03, 246MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.14G/9.98G [00:38<00:03, 244MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.18G/9.98G [00:38<00:03, 236MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.21G/9.98G [00:38<00:03, 240MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [00:38<00:03, 242MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.27G/9.98G [00:38<00:02, 244MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.30G/9.98G [00:38<00:02, 246MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▎| 9.33G/9.98G [00:38<00:02, 248MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:39<00:02, 246MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.40G/9.98G [00:39<00:02, 241MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:39<00:02, 244MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.46G/9.98G [00:39<00:02, 248MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:39<00:01, 250MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.52G/9.98G [00:39<00:01, 251MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:39<00:01, 251MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.58G/9.98G [00:39<00:01, 251MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [00:40<00:01, 241MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.65G/9.98G [00:40<00:01, 231MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:40<00:01, 224MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.71G/9.98G [00:40<00:01, 220MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:40<00:01, 217MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.77G/9.98G [00:40<00:00, 216MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.80G/9.98G [00:40<00:00, 214MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:41<00:00, 214MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [00:41<00:00, 212MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.90G/9.98G [00:41<00:00, 211MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [00:41<00:00, 211MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.96G/9.98G [00:41<00:00, 211MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:41<00:00, 239MB/s]
Downloading shards:  50%|█████     | 1/2 [00:41<00:41, 41.91s/it]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:16, 207MB/s][A
Downloading (…)of-00002.safetensors:   1%|▏         | 52.4M/3.50G [00:00<00:15, 229MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:14, 234MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:14, 236MB/s] [A
Downloading (…)of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:14, 238MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 178M/3.50G [00:00<00:13, 239MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:14, 235MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 241M/3.50G [00:01<00:13, 236MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 273M/3.50G [00:01<00:22, 146MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 294M/3.50G [00:01<00:24, 131MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 315M/3.50G [00:02<00:37, 85.2MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 336M/3.50G [00:02<00:31, 101MB/s] [A
Downloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:02<00:25, 125MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 388M/3.50G [00:02<00:22, 139MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:02<00:19, 159MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:02<00:17, 173MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 472M/3.50G [00:02<00:16, 181MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 493M/3.50G [00:02<00:16, 185MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 514M/3.50G [00:03<00:15, 190MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 535M/3.50G [00:03<00:15, 195MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 556M/3.50G [00:03<00:23, 128MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:03<00:22, 132MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:03<00:19, 147MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 619M/3.50G [00:03<00:18, 159MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 640M/3.50G [00:03<00:17, 168MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 661M/3.50G [00:04<00:16, 173MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 682M/3.50G [00:04<00:15, 182MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 713M/3.50G [00:04<00:14, 192MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 734M/3.50G [00:04<00:14, 196MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:04<00:13, 199MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:04<00:13, 202MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 807M/3.50G [00:04<00:13, 194MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 839M/3.50G [00:04<00:13, 203MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 870M/3.50G [00:05<00:12, 212MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 902M/3.50G [00:05<00:11, 219MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:05<00:11, 225MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 965M/3.50G [00:05<00:11, 231MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 996M/3.50G [00:05<00:10, 234MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 1.03G/3.50G [00:05<00:10, 236MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 1.06G/3.50G [00:05<00:10, 238MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:06<00:10, 226MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 1.12G/3.50G [00:06<00:10, 224MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 1.15G/3.50G [00:06<00:10, 226MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:06<00:10, 231MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 1.22G/3.50G [00:06<00:11, 201MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 1.25G/3.50G [00:06<00:10, 211MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.28G/3.50G [00:06<00:10, 217MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:07<00:09, 224MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 1.34G/3.50G [00:07<00:09, 229MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:07<00:09, 232MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 1.41G/3.50G [00:07<00:08, 235MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:07<00:15, 131MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:08<00:14, 138MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:08<00:12, 164MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:08<00:10, 187MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:08<00:09, 206MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:08<00:08, 222MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:08<00:08, 235MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 1.65G/3.50G [00:08<00:07, 245MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:08<00:07, 253MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 1.71G/3.50G [00:08<00:06, 258MB/s][A
Downloading (…)of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:09<00:09, 192MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:09<00:12, 138MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.79G/3.50G [00:09<00:11, 149MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.82G/3.50G [00:09<00:09, 172MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 1.86G/3.50G [00:09<00:08, 195MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:10<00:08, 185MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:10<00:08, 184MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:10<00:07, 201MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:10<00:07, 214MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:10<00:07, 213MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.03G/3.50G [00:11<00:14, 103MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:11<00:11, 127MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:11<00:09, 147MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 2.13G/3.50G [00:11<00:08, 165MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 2.16G/3.50G [00:11<00:07, 186MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:11<00:06, 205MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:12<00:05, 216MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:12<00:05, 230MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:12<00:05, 241MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:12<00:06, 173MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.35G/3.50G [00:12<00:07, 147MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:13<00:06, 170MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:13<00:05, 192MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:13<00:04, 211MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 2.47G/3.50G [00:13<00:04, 225MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.51G/3.50G [00:13<00:05, 181MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:13<00:04, 199MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:13<00:04, 215MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:13<00:03, 228MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 2.63G/3.50G [00:14<00:03, 240MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:14<00:03, 248MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 2.69G/3.50G [00:14<00:03, 255MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:14<00:03, 255MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:14<00:02, 264MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:14<00:02, 262MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 2.82G/3.50G [00:14<00:02, 270MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:14<00:02, 276MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 2.88G/3.50G [00:15<00:04, 142MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 2.92G/3.50G [00:15<00:03, 166MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 2.95G/3.50G [00:15<00:03, 176MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:15<00:02, 174MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 3.01G/3.50G [00:15<00:02, 198MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:16<00:02, 213MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 3.07G/3.50G [00:16<00:01, 227MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:16<00:02, 187MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 3.14G/3.50G [00:16<00:01, 196MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:16<00:01, 214MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████▏| 3.20G/3.50G [00:16<00:01, 228MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:16<00:01, 233MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.26G/3.50G [00:17<00:00, 241MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:17<00:00, 250MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:17<00:00, 257MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:17<00:00, 260MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 3.39G/3.50G [00:17<00:00, 261MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 3.42G/3.50G [00:17<00:00, 168MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:18<00:00, 149MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 3.47G/3.50G [00:18<00:00, 134MB/s][A
Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:18<00:00, 160MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:18<00:00, 190MB/s]
Downloading shards: 100%|██████████| 2/2 [01:00<00:00, 28.26s/it]Downloading shards: 100%|██████████| 2/2 [01:00<00:00, 30.31s/it]
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 18.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.67s/it]
Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]Downloading generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 33.7kB/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 10901.00 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.026 MB of 0.035 MB uploaded (0.005 MB deduped)wandb: / 0.035 MB of 0.035 MB uploaded (0.005 MB deduped)Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:24:08.960856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:24:09.678086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:24:16.095798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.104105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.106572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.119836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.122250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.124622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.307195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.308878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.310352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:24:16.311833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 18.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.61s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8
2024-03-07 00:26:55.068130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:26:55.801032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:27:02.492355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.501023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.503489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.521125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.523557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.525930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.702809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.704450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.705923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:27:02.707398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.35s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 10106.63 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:29:44.893062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:29:45.720058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:29:52.472579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.481980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.484398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.499536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.501926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.504302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.689039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.690649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.692118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:29:52.693578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 18.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.74s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8
2024-03-07 00:32:34.900359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:32:35.649375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:32:42.370058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.378794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.381217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.395449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.397840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.400204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.587353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.588931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.590383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:32:42.591856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 19.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.90s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 10166.76 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:35:22.972975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:35:23.754048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:35:30.432734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:30.518317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:30.520779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:30.535000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:30.537431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:30.539806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:31.689990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:31.691586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:31.693043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:35:31.694507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.06s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.026 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2
2024-03-07 00:38:16.970771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:38:17.729839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:38:24.356139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.364748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.367201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.381865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.384284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.386653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.568733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.570357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.571824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:38:24.573303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 18.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.81s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 14469.22 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:41:04.590260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:41:05.392570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:41:12.258300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.266874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.269275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.284120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.286489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.288833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.774721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.776310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.777760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:41:12.779226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 18.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.54s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2
2024-03-07 00:43:53.977490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:43:54.778231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:44:01.868792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:01.877895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:01.880334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:01.894732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:01.897152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:01.899537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:03.354064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:03.355717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:03.357196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:44:03.358675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.27s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 13818.69 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:46:46.218974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:46:47.060009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:46:53.936935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:53.945381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:53.947777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:53.962079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:53.964462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:53.966819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:54.267695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:54.269279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:54.270733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:46:54.272201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.26s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2
2024-03-07 00:49:35.204047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:49:36.035997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:49:43.321475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:43.330865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:43.333323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:43.348691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:43.351111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:43.353490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:45.161338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:45.162941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:45.164433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:49:45.165920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 19.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.99s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 14047.39 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.021 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:52:33.381696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:52:34.187814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:52:40.976268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:40.984554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:40.986955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.001176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.003561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.005919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.186967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.188565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.190016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:52:41.191475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.04s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4
2024-03-07 00:55:21.305499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:55:22.115462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:55:29.064948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.074308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.076757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.091763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.094157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.096524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.279149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.280744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.282191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:55:29.283645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 19.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.96s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 12420.81 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 00:58:11.714067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 00:58:12.550734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 00:58:19.265746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:19.274192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:19.276599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:19.291718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:19.294112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:19.296466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:22.889669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:22.891301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:22.893090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 00:58:22.897871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.18s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4
2024-03-07 01:01:04.026713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:01:04.841710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:01:12.155227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.164200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.166611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.180775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.183146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.185474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.761823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.763413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.764859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:01:12.766317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.20s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 10098.28 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 01:03:57.617463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:03:58.437689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:04:05.256726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.265295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.267743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.283192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.285602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.287976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.478822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.480498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.481968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:04:05.483448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.10s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4
2024-03-07 01:06:49.959234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:06:50.731746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:06:57.611900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.620621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.623046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.637202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.639586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.641934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.824695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.826317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.827764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:06:57.829216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.29s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 12088.63 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 01:09:39.616401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:09:40.395204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:09:47.409731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:47.420362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:47.422775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:47.437486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:47.439888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:47.442244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:48.636249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:48.637842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:48.639366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:09:48.640853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 19.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.96s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.029 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16
2024-03-07 01:12:29.802407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:12:30.631260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:12:37.958866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:37.968160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:37.970626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:37.985503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:37.987923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:37.990306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:38.177363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:38.179016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:38.180515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:12:38.182001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.00s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 7160.93 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 01:15:18.664447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:15:19.424757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:15:25.995792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.004690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.007138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.022443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.024855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.027324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.417375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.419010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.420501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:15:26.421969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.36s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16
2024-03-07 01:18:09.177984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:18:09.984847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:18:17.099729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.109727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.112182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.129086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.131512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.133886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.346852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.348499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.349980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:18:17.351465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.19s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 7298.02 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.026 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 01:20:58.673020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:20:59.411329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:21:06.416898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:06.425469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:06.427918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:06.476582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:06.478989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:06.481377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:07.568436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:07.570025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:07.571539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:21:07.573016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.17s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16
2024-03-07 01:23:51.862414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:23:52.654711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:23:59.458423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.518979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.521445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.537007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.539391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.541740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.803646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.805347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.806842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:23:59.808375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.35s/it]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 7175.79 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.026 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-07 01:26:42.025522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-07 01:26:42.824614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-07 01:26:49.582230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.612610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.615114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.629899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.632308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.634668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.913980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.916370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.918528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-07 01:26:49.920386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:255: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 19.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.47s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/cr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/376 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
