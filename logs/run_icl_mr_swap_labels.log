1+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8
textattack: Updating TextAttack package dependencies.
textattack: Downloading NLTK required packages.
2024-03-08 18:04:59.507938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:05:00.387525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package omw to /root/nltk_data...
[nltk_data] Downloading package universal_tagset to /root/nltk_data...
[nltk_data]   Unzipping taggers/universal_tagset.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.
  0%|          | 0.00/481M [00:00<?, ?B/s]  0%|          | 52.2k/481M [00:00<26:53, 298kB/s]  0%|          | 157k/481M [00:00<16:35, 484kB/s]   0%|          | 609k/481M [00:00<05:32, 1.45MB/s]  0%|          | 1.69M/481M [00:00<02:00, 3.99MB/s]  1%|          | 4.26M/481M [00:00<00:47, 10.1MB/s]  1%|▏         | 7.12M/481M [00:00<00:35, 13.3MB/s]  2%|▏         | 10.5M/481M [00:00<00:25, 18.8MB/s]  3%|▎         | 14.1M/481M [00:01<00:19, 23.4MB/s]  4%|▎         | 17.1M/481M [00:01<00:21, 21.6MB/s]  4%|▍         | 20.6M/481M [00:01<00:18, 24.9MB/s]  5%|▌         | 24.1M/481M [00:01<00:16, 27.8MB/s]  6%|▌         | 27.1M/481M [00:01<00:18, 24.2MB/s]  6%|▋         | 30.7M/481M [00:01<00:16, 27.1MB/s]  7%|▋         | 34.0M/481M [00:01<00:18, 24.8MB/s]  8%|▊         | 37.7M/481M [00:01<00:16, 27.6MB/s]  9%|▊         | 41.5M/481M [00:02<00:16, 25.9MB/s]  9%|▉         | 45.3M/481M [00:02<00:15, 28.7MB/s] 10%|█         | 49.1M/481M [00:02<00:13, 31.1MB/s] 11%|█         | 52.3M/481M [00:02<00:15, 27.1MB/s] 12%|█▏        | 55.7M/481M [00:02<00:14, 28.8MB/s] 12%|█▏        | 59.3M/481M [00:02<00:16, 26.1MB/s] 13%|█▎        | 63.0M/481M [00:02<00:14, 29.0MB/s] 14%|█▍        | 66.9M/481M [00:02<00:13, 31.3MB/s] 15%|█▍        | 70.2M/481M [00:03<00:15, 27.3MB/s] 15%|█▌        | 73.9M/481M [00:03<00:13, 29.9MB/s] 16%|█▌        | 77.9M/481M [00:03<00:14, 27.8MB/s] 17%|█▋        | 81.9M/481M [00:03<00:12, 30.9MB/s] 18%|█▊        | 85.8M/481M [00:03<00:12, 32.9MB/s] 19%|█▊        | 89.2M/481M [00:03<00:13, 28.9MB/s] 19%|█▉        | 93.2M/481M [00:03<00:12, 31.7MB/s] 20%|██        | 97.2M/481M [00:03<00:11, 33.7MB/s] 21%|██        | 101M/481M [00:04<00:12, 29.5MB/s]  22%|██▏       | 105M/481M [00:04<00:11, 32.2MB/s] 23%|██▎       | 109M/481M [00:04<00:10, 34.5MB/s] 23%|██▎       | 112M/481M [00:04<00:12, 30.1MB/s] 24%|██▍       | 117M/481M [00:04<00:11, 32.9MB/s] 25%|██▌       | 120M/481M [00:04<00:10, 34.5MB/s] 26%|██▌       | 124M/481M [00:04<00:11, 30.2MB/s] 27%|██▋       | 128M/481M [00:04<00:10, 32.2MB/s] 27%|██▋       | 132M/481M [00:05<00:10, 32.0MB/s] 28%|██▊       | 135M/481M [00:05<00:10, 32.3MB/s] 29%|██▉       | 139M/481M [00:05<00:10, 31.4MB/s] 29%|██▉       | 142M/481M [00:05<00:10, 32.4MB/s] 30%|███       | 145M/481M [00:05<00:10, 33.0MB/s] 31%|███       | 149M/481M [00:05<00:10, 31.2MB/s] 32%|███▏      | 153M/481M [00:05<00:10, 30.9MB/s] 32%|███▏      | 156M/481M [00:05<00:09, 32.9MB/s] 33%|███▎      | 160M/481M [00:05<00:09, 34.5MB/s] 34%|███▍      | 164M/481M [00:06<00:09, 32.3MB/s] 35%|███▍      | 167M/481M [00:06<00:09, 31.5MB/s] 36%|███▌      | 171M/481M [00:06<00:09, 33.0MB/s] 36%|███▋      | 175M/481M [00:06<00:09, 34.0MB/s] 37%|███▋      | 178M/481M [00:06<00:09, 31.8MB/s] 38%|███▊      | 182M/481M [00:06<00:09, 31.8MB/s] 38%|███▊      | 185M/481M [00:06<00:09, 32.7MB/s] 39%|███▉      | 189M/481M [00:06<00:08, 33.1MB/s] 40%|███▉      | 192M/481M [00:06<00:09, 31.0MB/s] 41%|████      | 196M/481M [00:07<00:09, 31.6MB/s] 42%|████▏     | 200M/481M [00:07<00:08, 33.1MB/s] 42%|████▏     | 204M/481M [00:07<00:08, 34.7MB/s] 43%|████▎     | 207M/481M [00:07<00:08, 32.4MB/s] 44%|████▎     | 211M/481M [00:07<00:08, 30.9MB/s] 44%|████▍     | 214M/481M [00:07<00:08, 32.0MB/s] 45%|████▌     | 218M/481M [00:07<00:08, 31.2MB/s] 46%|████▌     | 222M/481M [00:07<00:07, 32.9MB/s] 47%|████▋     | 225M/481M [00:07<00:08, 31.7MB/s] 47%|████▋     | 228M/481M [00:08<00:07, 32.3MB/s] 48%|████▊     | 232M/481M [00:08<00:08, 31.0MB/s] 49%|████▉     | 236M/481M [00:08<00:07, 31.1MB/s] 50%|████▉     | 240M/481M [00:08<00:07, 32.4MB/s] 51%|█████     | 244M/481M [00:08<00:07, 31.2MB/s] 51%|█████▏    | 247M/481M [00:08<00:07, 31.3MB/s] 52%|█████▏    | 251M/481M [00:08<00:07, 32.4MB/s] 53%|█████▎    | 255M/481M [00:08<00:07, 31.2MB/s] 54%|█████▎    | 259M/481M [00:08<00:06, 33.2MB/s] 54%|█████▍    | 262M/481M [00:09<00:06, 31.9MB/s] 55%|█████▌    | 265M/481M [00:09<00:06, 32.1MB/s] 56%|█████▌    | 269M/481M [00:09<00:06, 31.4MB/s] 57%|█████▋    | 273M/481M [00:09<00:06, 33.4MB/s] 57%|█████▋    | 276M/481M [00:09<00:06, 32.2MB/s] 58%|█████▊    | 280M/481M [00:09<00:06, 32.1MB/s] 59%|█████▉    | 284M/481M [00:09<00:06, 31.9MB/s] 60%|█████▉    | 287M/481M [00:09<00:06, 32.3MB/s] 60%|██████    | 290M/481M [00:09<00:06, 31.0MB/s] 61%|██████    | 294M/481M [00:10<00:05, 31.9MB/s] 62%|██████▏   | 298M/481M [00:10<00:05, 33.8MB/s] 63%|██████▎   | 301M/481M [00:10<00:05, 31.7MB/s] 63%|██████▎   | 304M/481M [00:10<00:05, 30.8MB/s] 64%|██████▍   | 308M/481M [00:10<00:05, 31.9MB/s] 65%|██████▍   | 312M/481M [00:10<00:04, 34.0MB/s] 65%|██████▌   | 315M/481M [00:10<00:05, 31.7MB/s] 66%|██████▌   | 319M/481M [00:10<00:05, 31.5MB/s] 67%|██████▋   | 322M/481M [00:10<00:04, 32.4MB/s] 68%|██████▊   | 326M/481M [00:11<00:04, 34.4MB/s] 69%|██████▊   | 330M/481M [00:11<00:04, 32.3MB/s] 69%|██████▉   | 333M/481M [00:11<00:04, 33.4MB/s] 70%|██████▉   | 337M/481M [00:11<00:04, 32.6MB/s] 71%|███████   | 340M/481M [00:11<00:04, 32.8MB/s] 71%|███████▏  | 344M/481M [00:11<00:04, 32.4MB/s] 72%|███████▏  | 347M/481M [00:11<00:04, 31.8MB/s] 73%|███████▎  | 351M/481M [00:11<00:03, 33.8MB/s] 74%|███████▎  | 354M/481M [00:11<00:03, 32.7MB/s] 74%|███████▍  | 358M/481M [00:12<00:03, 33.8MB/s] 75%|███████▌  | 362M/481M [00:12<00:03, 32.5MB/s] 76%|███████▌  | 365M/481M [00:12<00:03, 32.7MB/s] 77%|███████▋  | 369M/481M [00:12<00:03, 34.1MB/s] 77%|███████▋  | 372M/481M [00:12<00:03, 32.0MB/s] 78%|███████▊  | 376M/481M [00:12<00:03, 31.5MB/s] 79%|███████▉  | 380M/481M [00:12<00:03, 32.7MB/s] 80%|███████▉  | 383M/481M [00:12<00:02, 33.9MB/s] 80%|████████  | 387M/481M [00:12<00:02, 32.7MB/s] 81%|████████  | 390M/481M [00:13<00:02, 33.0MB/s] 82%|████████▏ | 393M/481M [00:13<00:02, 31.5MB/s] 82%|████████▏ | 397M/481M [00:13<00:02, 32.1MB/s] 83%|████████▎ | 401M/481M [00:13<00:02, 33.6MB/s] 84%|████████▍ | 404M/481M [00:13<00:02, 32.4MB/s] 85%|████████▍ | 407M/481M [00:13<00:02, 32.8MB/s] 85%|████████▌ | 411M/481M [00:13<00:02, 33.9MB/s] 86%|████████▌ | 414M/481M [00:13<00:02, 33.1MB/s] 87%|████████▋ | 418M/481M [00:13<00:01, 33.0MB/s] 87%|████████▋ | 421M/481M [00:13<00:01, 32.7MB/s] 88%|████████▊ | 424M/481M [00:14<00:01, 31.8MB/s] 89%|████████▉ | 428M/481M [00:14<00:01, 32.6MB/s] 90%|████████▉ | 431M/481M [00:14<00:01, 32.0MB/s] 90%|█████████ | 435M/481M [00:14<00:01, 33.1MB/s] 91%|█████████ | 438M/481M [00:14<00:01, 33.0MB/s] 92%|█████████▏| 442M/481M [00:14<00:01, 32.1MB/s] 92%|█████████▏| 445M/481M [00:14<00:01, 33.1MB/s] 93%|█████████▎| 449M/481M [00:14<00:01, 32.0MB/s] 94%|█████████▍| 452M/481M [00:14<00:00, 33.2MB/s] 95%|█████████▍| 456M/481M [00:15<00:00, 33.3MB/s] 95%|█████████▌| 459M/481M [00:15<00:00, 32.8MB/s] 96%|█████████▌| 463M/481M [00:15<00:00, 33.2MB/s] 97%|█████████▋| 466M/481M [00:15<00:00, 33.7MB/s] 98%|█████████▊| 470M/481M [00:15<00:00, 32.3MB/s] 98%|█████████▊| 473M/481M [00:15<00:00, 33.0MB/s] 99%|█████████▉| 476M/481M [00:15<00:00, 32.7MB/s]100%|█████████▉| 480M/481M [00:15<00:00, 32.3MB/s]100%|██████████| 481M/481M [00:15<00:00, 30.5MB/s]
textattack: Unzipping file /root/.cache/textattack/tmpl2wsx3ue.zip to /root/.cache/textattack/word_embeddings/paragramcf.
textattack: Successfully saved word_embeddings/paragramcf to cache.
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 18.4MB/s]                   2024-03-08 18:05:29.005551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.014359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.016760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.032503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.035047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.037414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.215150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.216795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.218267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.219730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0

Downloading builder script:   0%|          | 0.00/5.03k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 5.03k/5.03k [00:00<00:00, 33.0MB/s]
Downloading readme:   0%|          | 0.00/7.25k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 7.25k/7.25k [00:00<00:00, 42.9MB/s]
Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]Downloading data: 100%|██████████| 488k/488k [00:00<00:00, 8.10MB/s]
Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]Generating train split:  56%|█████▋    | 4801/8530 [00:00<00:00, 47281.26 examples/s]Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 50076.37 examples/s]
Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 36706.66 examples/s]
Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 36057.78 examples/s]
Map:   0%|          | 0/8530 [00:00<?, ? examples/s]Map:  28%|██▊       | 2400/8530 [00:00<00:00, 23844.71 examples/s]Map:  57%|█████▋    | 4822/8530 [00:00<00:00, 24055.37 examples/s]Map:  85%|████████▍ | 7243/8530 [00:00<00:00, 24121.41 examples/s]Map: 100%|██████████| 8530/8530 [00:00<00:00, 23857.78 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|██████████| 1066/1066 [00:00<00:00, 17965.07 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|██████████| 1066/1066 [00:00<00:00, 22994.78 examples/s]
Map:   0%|          | 0/8103 [00:00<?, ? examples/s]Map:  16%|█▋        | 1321/8103 [00:00<00:00, 13133.42 examples/s]Map:  33%|███▎      | 2689/8103 [00:00<00:00, 13322.76 examples/s]Map:  58%|█████▊    | 4678/8103 [00:00<00:00, 13258.04 examples/s]Map:  82%|████████▏ | 6677/8103 [00:00<00:00, 13223.61 examples/s]Map: 100%|██████████| 8103/8103 [00:00<00:00, 13119.47 examples/s]Map: 100%|██████████| 8103/8103 [00:00<00:00, 13148.33 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|██████████| 1066/1066 [00:00<00:00, 23316.88 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 24086.09 examples/s]
Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 148kB/s]
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.2MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.00MB/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.99MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 294kB/s]
Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 609/609 [00:00<00:00, 92.6kB/s]
Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 20.3MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   0%|          | 21.0M/9.98G [00:00<00:52, 189MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 52.4M/9.98G [00:00<00:44, 223MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:41, 236MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 115M/9.98G [00:00<00:40, 243MB/s] [A
Downloading (…)of-00002.safetensors:   1%|▏         | 147M/9.98G [00:00<00:39, 249MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 178M/9.98G [00:00<00:40, 239MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:42, 229MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 241M/9.98G [00:01<00:41, 234MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 273M/9.98G [00:01<00:40, 240MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 304M/9.98G [00:01<00:39, 247MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 336M/9.98G [00:01<00:38, 248MB/s][A
Downloading (…)of-00002.safetensors:   4%|▎         | 367M/9.98G [00:01<00:38, 247MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 398M/9.98G [00:01<00:40, 239MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:40, 235MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 461M/9.98G [00:01<00:40, 234MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 493M/9.98G [00:02<00:40, 231MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 524M/9.98G [00:02<00:41, 229MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 556M/9.98G [00:02<00:42, 222MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 587M/9.98G [00:02<00:45, 206MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 619M/9.98G [00:02<00:43, 214MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 650M/9.98G [00:02<00:45, 203MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 671M/9.98G [00:02<00:46, 202MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 692M/9.98G [00:03<00:46, 202MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 713M/9.98G [00:03<00:50, 183MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 734M/9.98G [00:03<00:53, 173MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 755M/9.98G [00:03<00:52, 177MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:03<00:50, 183MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 807M/9.98G [00:03<00:47, 194MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 839M/9.98G [00:03<00:45, 201MB/s][A
Downloading (…)of-00002.safetensors:   9%|▊         | 860M/9.98G [00:03<00:45, 202MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 891M/9.98G [00:04<00:42, 211MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 923M/9.98G [00:04<00:42, 215MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 954M/9.98G [00:04<00:42, 214MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 986M/9.98G [00:04<00:41, 218MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 1.02G/9.98G [00:04<00:43, 208MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:04<00:45, 197MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.06G/9.98G [00:04<00:45, 196MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.09G/9.98G [00:05<00:42, 207MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.12G/9.98G [00:05<00:42, 210MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:05<00:40, 219MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:05<00:40, 220MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.22G/9.98G [00:05<00:39, 221MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.25G/9.98G [00:05<00:41, 210MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.28G/9.98G [00:05<00:44, 197MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.30G/9.98G [00:06<00:44, 195MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.32G/9.98G [00:06<00:44, 196MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▎        | 1.35G/9.98G [00:06<00:41, 206MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.38G/9.98G [00:06<00:40, 210MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.42G/9.98G [00:06<00:40, 212MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 1.45G/9.98G [00:06<00:39, 217MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 1.48G/9.98G [00:06<00:38, 219MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:07<00:39, 212MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.54G/9.98G [00:07<00:40, 209MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:07<00:39, 211MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.60G/9.98G [00:07<00:39, 213MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:07<00:39, 210MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.67G/9.98G [00:07<00:39, 212MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:08<00:45, 183MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:08<00:45, 183MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:08<00:44, 186MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.76G/9.98G [00:08<00:43, 188MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:08<00:43, 188MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.81G/9.98G [00:08<00:41, 196MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.84G/9.98G [00:08<00:42, 192MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▊        | 1.86G/9.98G [00:08<00:43, 188MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.88G/9.98G [00:08<00:44, 183MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.90G/9.98G [00:09<00:43, 186MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.92G/9.98G [00:09<00:42, 190MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:09<00:43, 187MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.96G/9.98G [00:09<00:41, 191MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:09<00:40, 198MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.01G/9.98G [00:09<00:41, 191MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:09<00:40, 195MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.07G/9.98G [00:09<00:38, 207MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.09G/9.98G [00:10<00:39, 202MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.11G/9.98G [00:10<00:39, 198MB/s][A
Downloading (…)of-00002.safetensors:  21%|██▏       | 2.14G/9.98G [00:10<00:38, 204MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.17G/9.98G [00:10<00:36, 213MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:10<00:35, 217MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.23G/9.98G [00:10<00:36, 214MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:10<00:35, 219MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.30G/9.98G [00:10<00:35, 216MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.33G/9.98G [00:11<00:35, 215MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:11<00:35, 217MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:11<00:34, 219MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.42G/9.98G [00:11<00:34, 221MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 2.45G/9.98G [00:11<00:36, 204MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 2.49G/9.98G [00:11<00:35, 210MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:12<00:38, 196MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.54G/9.98G [00:12<00:38, 191MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.56G/9.98G [00:12<00:38, 191MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.59G/9.98G [00:12<00:39, 185MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:12<00:37, 194MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 2.64G/9.98G [00:12<00:57, 128MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.67G/9.98G [00:13<00:49, 148MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.71G/9.98G [00:13<00:43, 166MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.74G/9.98G [00:13<00:39, 182MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:13<00:36, 196MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.80G/9.98G [00:13<00:34, 206MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.83G/9.98G [00:13<00:33, 212MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▊       | 2.86G/9.98G [00:13<00:35, 202MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.88G/9.98G [00:14<00:36, 197MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.90G/9.98G [00:14<00:36, 192MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.93G/9.98G [00:14<00:36, 191MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.95G/9.98G [00:14<00:37, 190MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.97G/9.98G [00:14<00:39, 180MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.99G/9.98G [00:14<00:44, 156MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 3.01G/9.98G [00:14<00:42, 166MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 3.03G/9.98G [00:14<00:40, 171MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.05G/9.98G [00:15<00:39, 176MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:15<00:38, 181MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.09G/9.98G [00:15<00:37, 184MB/s][A
Downloading (…)of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:15<00:35, 195MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.16G/9.98G [00:15<00:34, 197MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.19G/9.98G [00:15<00:32, 211MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.22G/9.98G [00:15<00:31, 214MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.25G/9.98G [00:16<00:31, 214MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:16<00:31, 211MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.31G/9.98G [00:16<00:31, 211MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▎      | 3.34G/9.98G [00:16<00:30, 214MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.38G/9.98G [00:16<00:32, 200MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.40G/9.98G [00:16<00:34, 191MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.42G/9.98G [00:16<00:33, 194MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.45G/9.98G [00:17<00:31, 206MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:17<00:31, 207MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 3.50G/9.98G [00:17<00:31, 203MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:17<00:33, 193MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:17<00:34, 188MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.57G/9.98G [00:17<00:34, 184MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.59G/9.98G [00:17<00:35, 182MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.61G/9.98G [00:17<00:34, 183MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▋      | 3.63G/9.98G [00:17<00:34, 183MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.66G/9.98G [00:18<00:32, 195MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.69G/9.98G [00:18<00:30, 206MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.72G/9.98G [00:18<00:29, 216MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.75G/9.98G [00:18<00:28, 221MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.79G/9.98G [00:18<00:30, 204MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.81G/9.98G [00:18<00:31, 198MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.83G/9.98G [00:18<00:31, 197MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▊      | 3.86G/9.98G [00:19<00:30, 202MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:19<00:31, 195MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:19<00:32, 185MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:19<00:33, 182MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:19<00:33, 182MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.96G/9.98G [00:19<00:32, 185MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:19<00:31, 188MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 4.02G/9.98G [00:19<00:29, 199MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:20<00:28, 206MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.07G/9.98G [00:20<00:28, 205MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:20<00:30, 193MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:20<00:31, 187MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.14G/9.98G [00:20<00:30, 194MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.17G/9.98G [00:20<00:28, 201MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:20<00:28, 201MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:20<00:31, 185MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:21<00:30, 185MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.27G/9.98G [00:21<00:31, 184MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.29G/9.98G [00:21<00:30, 185MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.31G/9.98G [00:21<00:30, 187MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.33G/9.98G [00:21<00:30, 188MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▎     | 4.35G/9.98G [00:21<00:30, 187MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.38G/9.98G [00:21<00:28, 198MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:21<00:28, 198MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.44G/9.98G [00:22<00:27, 205MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:22<00:26, 206MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.48G/9.98G [00:22<00:28, 194MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 4.50G/9.98G [00:22<00:27, 198MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 4.52G/9.98G [00:22<00:27, 199MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.54G/9.98G [00:22<00:27, 198MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.56G/9.98G [00:22<00:27, 195MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.58G/9.98G [00:22<00:29, 184MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.60G/9.98G [00:22<00:29, 180MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▋     | 4.62G/9.98G [00:23<00:28, 187MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.66G/9.98G [00:23<00:26, 202MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.69G/9.98G [00:23<00:25, 210MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.72G/9.98G [00:23<00:24, 217MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.75G/9.98G [00:23<00:23, 220MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:23<00:23, 219MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.81G/9.98G [00:23<00:23, 221MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▊     | 4.84G/9.98G [00:24<00:24, 207MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:24<00:36, 140MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.89G/9.98G [00:24<00:33, 150MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:24<00:31, 161MB/s][A
Downloading (…)of-00002.safetensors:  50%|████▉     | 4.94G/9.98G [00:24<00:28, 178MB/s][A
Downloading (…)of-00002.safetensors:  50%|████▉     | 4.96G/9.98G [00:24<00:27, 185MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:24<00:25, 199MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:25<00:24, 206MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.05G/9.98G [00:25<00:23, 210MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:25<00:22, 215MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████▏    | 5.12G/9.98G [00:25<00:22, 218MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:25<00:22, 218MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.18G/9.98G [00:25<00:21, 224MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:25<00:20, 228MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:26<00:22, 214MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.27G/9.98G [00:26<00:23, 201MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:26<00:23, 196MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.32G/9.98G [00:26<00:24, 190MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:26<00:24, 192MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▎    | 5.36G/9.98G [00:26<00:23, 194MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.38G/9.98G [00:26<00:23, 192MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.40G/9.98G [00:26<00:24, 190MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.42G/9.98G [00:27<00:24, 189MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.45G/9.98G [00:27<00:22, 197MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.47G/9.98G [00:27<00:24, 181MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:27<00:24, 183MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.53G/9.98G [00:27<00:23, 189MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.55G/9.98G [00:27<00:24, 183MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.57G/9.98G [00:27<00:23, 187MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:28<00:24, 178MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.61G/9.98G [00:28<00:23, 185MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:28<00:22, 197MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.66G/9.98G [00:28<00:24, 179MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:28<00:27, 157MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.70G/9.98G [00:28<00:25, 165MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:28<00:26, 163MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:28<00:24, 171MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.77G/9.98G [00:29<00:23, 180MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.80G/9.98G [00:29<00:21, 194MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.83G/9.98G [00:29<00:20, 204MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:29<00:19, 210MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.89G/9.98G [00:29<00:19, 215MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:29<00:18, 220MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.96G/9.98G [00:29<00:18, 220MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 5.99G/9.98G [00:30<00:18, 218MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 6.02G/9.98G [00:30<00:19, 208MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.05G/9.98G [00:30<00:18, 211MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.08G/9.98G [00:30<00:18, 209MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:30<00:19, 196MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 6.12G/9.98G [00:30<00:20, 192MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:30<00:19, 198MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.18G/9.98G [00:30<00:19, 199MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.21G/9.98G [00:31<00:18, 207MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.24G/9.98G [00:31<00:17, 215MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:31<00:17, 210MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.30G/9.98G [00:31<00:17, 205MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.33G/9.98G [00:31<00:17, 206MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▎   | 6.35G/9.98G [00:31<00:17, 203MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:31<00:18, 193MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.40G/9.98G [00:32<00:18, 195MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:32<00:18, 189MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:32<00:17, 197MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.47G/9.98G [00:32<00:17, 195MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.50G/9.98G [00:32<00:17, 201MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.52G/9.98G [00:32<00:17, 200MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:32<00:17, 197MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.56G/9.98G [00:32<00:19, 178MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:33<00:18, 182MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:33<00:18, 186MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.64G/9.98G [00:33<00:16, 198MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.67G/9.98G [00:33<00:15, 209MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.70G/9.98G [00:33<00:15, 216MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.73G/9.98G [00:33<00:14, 221MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.76G/9.98G [00:33<00:14, 221MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:34<00:14, 220MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.83G/9.98G [00:34<00:14, 222MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:34<00:14, 220MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:34<00:13, 222MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.92G/9.98G [00:34<00:14, 212MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:34<00:16, 183MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:34<00:16, 186MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 6.99G/9.98G [00:35<00:15, 189MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 7.03G/9.98G [00:35<00:14, 199MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.05G/9.98G [00:35<00:15, 195MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.07G/9.98G [00:35<00:15, 188MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:35<00:15, 183MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.11G/9.98G [00:35<00:16, 179MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.13G/9.98G [00:35<00:16, 175MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.15G/9.98G [00:37<01:18, 35.9MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.17G/9.98G [00:37<00:59, 47.3MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.19G/9.98G [00:37<00:53, 52.1MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:38<00:36, 75.0MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.26G/9.98G [00:38<00:27, 99.1MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.28G/9.98G [00:38<00:24, 111MB/s] [A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.31G/9.98G [00:38<00:19, 135MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▎  | 7.34G/9.98G [00:38<00:18, 144MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.36G/9.98G [00:38<00:19, 132MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:38<00:18, 144MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.40G/9.98G [00:39<00:16, 154MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:39<00:15, 163MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:39<00:14, 173MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:39<00:13, 186MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.50G/9.98G [00:39<00:13, 189MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:39<00:12, 191MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:39<00:12, 194MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.56G/9.98G [00:39<00:12, 194MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.59G/9.98G [00:39<00:11, 202MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▋  | 7.62G/9.98G [00:40<00:11, 204MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.65G/9.98G [00:40<00:11, 202MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.68G/9.98G [00:40<00:11, 199MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.70G/9.98G [00:40<00:11, 192MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.72G/9.98G [00:40<00:11, 195MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:40<00:11, 198MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:40<00:11, 189MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:40<00:11, 191MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.81G/9.98G [00:41<00:10, 205MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▊  | 7.83G/9.98G [00:41<00:10, 204MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.86G/9.98G [00:41<00:10, 210MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.90G/9.98G [00:41<00:09, 217MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.93G/9.98G [00:41<00:09, 212MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:41<00:09, 203MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.98G/9.98G [00:41<00:09, 202MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:42<00:09, 215MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.04G/9.98G [00:42<00:08, 216MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:42<00:08, 221MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.11G/9.98G [00:42<00:08, 215MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:42<00:08, 209MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:42<00:08, 203MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [00:42<00:08, 210MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:43<00:08, 217MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:43<00:07, 222MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:43<00:07, 219MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:43<00:07, 223MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▎ | 8.35G/9.98G [00:43<00:07, 227MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.38G/9.98G [00:43<00:07, 220MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.41G/9.98G [00:43<00:07, 217MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:44<00:06, 225MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:44<00:06, 228MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [00:44<00:06, 232MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.54G/9.98G [00:44<00:06, 233MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:44<00:05, 242MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.60G/9.98G [00:44<00:05, 248MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:44<00:05, 229MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.66G/9.98G [00:45<00:06, 204MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [00:45<00:06, 195MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:45<00:06, 197MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.75G/9.98G [00:45<00:06, 203MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:45<00:05, 206MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.81G/9.98G [00:45<00:05, 208MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:45<00:05, 213MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.87G/9.98G [00:45<00:05, 218MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:46<00:04, 224MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.93G/9.98G [00:46<00:04, 229MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:46<00:04, 225MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 9.00G/9.98G [00:46<00:04, 234MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:46<00:04, 226MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.06G/9.98G [00:46<00:03, 235MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:46<00:03, 243MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████▏| 9.12G/9.98G [00:47<00:03, 249MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:47<00:03, 249MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.19G/9.98G [00:47<00:03, 250MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [00:47<00:03, 250MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:47<00:02, 249MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [00:47<00:02, 239MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.31G/9.98G [00:47<00:02, 235MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:47<00:02, 221MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.37G/9.98G [00:48<00:02, 217MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [00:48<00:02, 206MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:48<00:02, 200MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:48<00:02, 195MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.48G/9.98G [00:48<00:02, 207MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:48<00:02, 216MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.54G/9.98G [00:49<00:02, 173MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:49<00:02, 184MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▋| 9.60G/9.98G [00:49<00:01, 192MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:49<00:01, 199MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.67G/9.98G [00:49<00:01, 206MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.70G/9.98G [00:49<00:01, 208MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.73G/9.98G [00:49<00:01, 201MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [00:50<00:01, 211MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.79G/9.98G [00:50<00:00, 215MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.83G/9.98G [00:50<00:00, 215MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.86G/9.98G [00:50<00:00, 210MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:50<00:00, 207MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.91G/9.98G [00:50<00:00, 206MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.94G/9.98G [00:50<00:00, 211MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:51<00:00, 208MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:51<00:00, 195MB/s]
Downloading shards:  50%|█████     | 1/2 [00:51<00:51, 51.25s/it]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:17, 195MB/s][A
Downloading (…)of-00002.safetensors:   1%|▏         | 52.4M/3.50G [00:00<00:16, 203MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 73.4M/3.50G [00:00<00:16, 203MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 94.4M/3.50G [00:00<00:17, 192MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:17, 195MB/s] [A
Downloading (…)of-00002.safetensors:   4%|▍         | 136M/3.50G [00:00<00:16, 199MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 157M/3.50G [00:00<00:16, 202MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 178M/3.50G [00:00<00:18, 181MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 199M/3.50G [00:01<00:17, 187MB/s][A
Downloading (…)of-00002.safetensors:   6%|▋         | 220M/3.50G [00:01<00:25, 127MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 252M/3.50G [00:01<00:21, 151MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 273M/3.50G [00:01<00:20, 159MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 294M/3.50G [00:01<00:19, 168MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 325M/3.50G [00:01<00:17, 182MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 346M/3.50G [00:01<00:17, 185MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:02<00:16, 186MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 388M/3.50G [00:02<00:17, 174MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 409M/3.50G [00:02<00:17, 178MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 430M/3.50G [00:02<00:26, 117MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:02<00:31, 95.8MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:03<00:25, 118MB/s] [A
Downloading (…)of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:03<00:23, 130MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:03<00:20, 144MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 556M/3.50G [00:03<00:17, 164MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:03<00:16, 172MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:03<00:16, 177MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:03<00:15, 189MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▊        | 650M/3.50G [00:04<00:24, 116MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:04<00:31, 90.4MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:04<00:27, 101MB/s] [A
Downloading (…)of-00002.safetensors:  20%|██        | 713M/3.50G [00:04<00:24, 116MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 734M/3.50G [00:04<00:21, 127MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 765M/3.50G [00:05<00:18, 149MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:05<00:16, 168MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:05<00:15, 177MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 849M/3.50G [00:05<00:13, 190MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 870M/3.50G [00:06<00:29, 88.5MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:06<00:25, 103MB/s] [A
Downloading (…)of-00002.safetensors:  26%|██▌       | 912M/3.50G [00:06<00:22, 117MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:06<00:19, 133MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 954M/3.50G [00:06<00:17, 147MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 975M/3.50G [00:06<00:16, 157MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 1.01G/3.50G [00:06<00:14, 176MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:06<00:12, 194MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 1.07G/3.50G [00:07<00:12, 196MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:07<00:17, 139MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 1.11G/3.50G [00:07<00:28, 83.7MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:08<00:24, 96.4MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 1.15G/3.50G [00:08<00:21, 111MB/s] [A
Downloading (…)of-00002.safetensors:  34%|███▎      | 1.17G/3.50G [00:08<00:18, 123MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:08<00:15, 147MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 1.24G/3.50G [00:08<00:14, 160MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 1.26G/3.50G [00:08<00:13, 167MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:08<00:11, 185MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:08<00:11, 190MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:09<00:25, 83.4MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:09<00:21, 97.9MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:09<00:17, 121MB/s] [A
Downloading (…)of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:10<00:14, 141MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:10<00:16, 128MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:10<00:14, 138MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:10<00:12, 162MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.51G/3.50G [00:10<00:11, 166MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▎     | 1.53G/3.50G [00:10<00:11, 172MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:10<00:12, 160MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 1.57G/3.50G [00:11<00:16, 120MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 1.59G/3.50G [00:11<00:14, 135MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:11<00:12, 147MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 1.64G/3.50G [00:11<00:12, 154MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 1.66G/3.50G [00:11<00:11, 159MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:11<00:11, 163MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▊     | 1.70G/3.50G [00:11<00:10, 166MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 1.72G/3.50G [00:11<00:10, 174MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 1.75G/3.50G [00:12<00:09, 189MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:12<00:18, 93.8MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.79G/3.50G [00:12<00:16, 105MB/s] [A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:12<00:14, 120MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.84G/3.50G [00:13<00:12, 133MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 1.86G/3.50G [00:13<00:11, 146MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:13<00:09, 167MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:13<00:09, 175MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 1.93G/3.50G [00:13<00:09, 168MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 1.95G/3.50G [00:13<00:09, 168MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 1.98G/3.50G [00:13<00:08, 182MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:14<00:15, 96.3MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.02G/3.50G [00:14<00:15, 95.8MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.04G/3.50G [00:14<00:13, 107MB/s] [A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:14<00:11, 121MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 2.09G/3.50G [00:14<00:10, 136MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:15<00:08, 158MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 2.15G/3.50G [00:15<00:07, 176MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 2.17G/3.50G [00:15<00:07, 183MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:15<00:06, 193MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.23G/3.50G [00:15<00:09, 135MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:16<00:10, 117MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:16<00:09, 131MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.30G/3.50G [00:16<00:08, 141MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:16<00:07, 153MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.34G/3.50G [00:16<00:07, 163MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.36G/3.50G [00:16<00:06, 172MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:16<00:06, 176MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:16<00:05, 187MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:16<00:05, 196MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 2.46G/3.50G [00:17<00:06, 154MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 2.49G/3.50G [00:17<00:06, 165MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:17<00:05, 183MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:17<00:05, 187MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:17<00:04, 198MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:17<00:04, 211MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 2.63G/3.50G [00:17<00:03, 219MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:18<00:03, 221MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 2.69G/3.50G [00:18<00:05, 157MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.72G/3.50G [00:18<00:06, 129MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:18<00:05, 149MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:19<00:05, 139MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:19<00:04, 148MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:19<00:04, 159MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:19<00:04, 160MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:19<00:03, 169MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 2.87G/3.50G [00:19<00:03, 170MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:19<00:03, 175MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▎ | 2.93G/3.50G [00:19<00:02, 192MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:19<00:02, 203MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:20<00:04, 116MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:20<00:03, 130MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.03G/3.50G [00:20<00:03, 150MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.05G/3.50G [00:20<00:02, 154MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:20<00:02, 170MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 3.11G/3.50G [00:21<00:02, 185MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 3.14G/3.50G [00:21<00:02, 131MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:21<00:02, 157MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████▏| 3.20G/3.50G [00:21<00:02, 108MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 3.22G/3.50G [00:22<00:02, 101MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.24G/3.50G [00:22<00:02, 112MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:22<00:01, 133MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:22<00:01, 142MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:22<00:01, 152MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:22<00:01, 162MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 3.37G/3.50G [00:22<00:00, 177MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:23<00:00, 192MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 3.42G/3.50G [00:23<00:00, 114MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:23<00:00, 124MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 3.46G/3.50G [00:23<00:00, 136MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:23<00:00, 146MB/s][A
Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:24<00:00, 151MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:24<00:00, 146MB/s]
Downloading shards: 100%|██████████| 2/2 [01:15<00:00, 35.30s/it]Downloading shards: 100%|██████████| 2/2 [01:15<00:00, 37.70s/it]
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.91s/it]
Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]Downloading generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 29.5kB/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 10802.41 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:09:33.693209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:09:34.572171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:09:41.834723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.844051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.846462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.861972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.864349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.866688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.055454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.057076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.058527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.059984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.18s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8
2024-03-08 18:12:27.522372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:12:28.369382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:12:35.903887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.916553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.919004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.935065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.937443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.939774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.128934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.130564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.132023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.133494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.99s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 10698.34 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.025 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:15:25.203796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:15:26.016046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:15:34.025427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.039257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.041751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.061770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.064166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.066508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.248928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.250542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.251996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.253459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.01s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8
2024-03-08 18:18:20.286905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:18:21.118834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:18:28.586251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.598230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.600710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.624021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.626558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.628907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.816215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.817852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.819311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.820785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.82s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 10844.64 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:21:13.381304: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:21:14.190597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:21:21.931563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.941428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.948440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.965843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.968444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.970820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.164201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.165847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.167316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.168796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.26s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2
2024-03-08 18:24:07.082678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:24:07.877641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:24:15.360456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.369539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.371956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.387514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.389948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.392514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.578853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.580467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.581921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.583393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.04s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 15126.55 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:27:03.120205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:27:03.932522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:27:11.164406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.173919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.176343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.191788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.194174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.196526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.385462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.387093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.388544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.390007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.62s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2
2024-03-08 18:29:55.091451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:29:55.920480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:30:03.327650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.337275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.339683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.355652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.358042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.360400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.549718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.551366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.553200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.554670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.01s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 14993.04 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:32:48.069796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:32:48.869540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:32:56.099852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.109501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.111947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.127949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.130497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.132862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.319051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.320710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.322172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.323635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.21s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2
2024-03-08 18:35:43.088009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:35:43.913536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:35:51.450063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.464355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.466856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.487709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.490123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.492473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.681443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.683086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.684546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.686015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.01s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 14681.21 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:38:39.985596: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:38:40.759356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:38:48.016638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.025957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.028389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.043440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.046000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.048342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.236053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.237686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.239262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.241139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.07s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4
2024-03-08 18:41:32.676495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:41:33.489706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:41:41.018722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.029830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.032291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.052623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.055162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.057485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.248623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.250297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.251756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.253221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.99s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 14105.00 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:44:26.462007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:44:27.252240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:44:34.356445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.368729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.371146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.386778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.389173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.391522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.615960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.618964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.621590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.624316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.87s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4
2024-03-08 18:47:20.636759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:47:21.503797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:47:29.209536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.220968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.223383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.240533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.242925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.245333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.431928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.433649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.435153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.436639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.39s/it]
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 187349ac-3665-483d-9d61-8bbf7a93aea5)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 187349ac-3665-483d-9d61-8bbf7a93aea5)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 13452.42 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:50:28.081366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:50:28.902993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:50:36.178935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.188405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.190788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.205896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.208263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.210587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.401292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.402864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.404307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.405753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 14936211-b65e-4c55-88fc-6577e1048d7a)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 14936211-b65e-4c55-88fc-6577e1048d7a)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 3beaef89-83d5-4985-90cd-df98bca02303)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 3beaef89-83d5-4985-90cd-df98bca02303)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7c300cfe-070e-4ea2-8bc2-ee2262371074)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7c300cfe-070e-4ea2-8bc2-ee2262371074)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.07s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4
2024-03-08 18:56:15.933530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:56:16.746850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:56:24.297172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.310059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.312703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.335487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.337886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.340221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.530089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.531713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.533167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.534622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.04s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 13725.80 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.026 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:59:18.054932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:59:18.835814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:59:25.980133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:25.989891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:25.992312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.007062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.009461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.011803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.214106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.215727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.217190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.218665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.85s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16
2024-03-08 19:03:21.000217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:03:21.840357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:03:29.388218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.399147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.401590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.418773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.421163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.423505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.611009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.612630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.614098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.615566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.28s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 8143.08 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 7902.11 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:06:15.994141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:06:16.833107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:06:24.290652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.300318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.302732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.318795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.322683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.325029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.529872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.531489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.532932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.534394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.06s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16
2024-03-08 19:09:10.444086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:09:11.311779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:09:18.739722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.748769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.751177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.767469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.769866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.772438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.958397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.960021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.961480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.962934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.02s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 8001.77 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 7742.98 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:12:03.356316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:12:04.188865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:12:11.457265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.467401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.469895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.485399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.487828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.490366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.699980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.701628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.703101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.704572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.00s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16
2024-03-08 19:14:57.719793: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:14:58.567181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:15:06.163923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.173518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.175939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.190909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.193293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.195642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.392679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.394329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.395883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.397372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.24s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 7641.33 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 7393.83 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:17:52.960652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:17:53.752263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:18:00.940031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.952038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.954487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.970163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.972573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.974952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.181782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.183422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.184888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.186371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.45s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
