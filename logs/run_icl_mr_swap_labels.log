1+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8
textattack: Updating TextAttack package dependencies.
textattack: Downloading NLTK required packages.
2024-03-08 18:04:59.507938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:05:00.387525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package omw to /root/nltk_data...
[nltk_data] Downloading package universal_tagset to /root/nltk_data...
[nltk_data]   Unzipping taggers/universal_tagset.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.
  0%|          | 0.00/481M [00:00<?, ?B/s]  0%|          | 52.2k/481M [00:00<26:53, 298kB/s]  0%|          | 157k/481M [00:00<16:35, 484kB/s]   0%|          | 609k/481M [00:00<05:32, 1.45MB/s]  0%|          | 1.69M/481M [00:00<02:00, 3.99MB/s]  1%|          | 4.26M/481M [00:00<00:47, 10.1MB/s]  1%|â–         | 7.12M/481M [00:00<00:35, 13.3MB/s]  2%|â–         | 10.5M/481M [00:00<00:25, 18.8MB/s]  3%|â–Ž         | 14.1M/481M [00:01<00:19, 23.4MB/s]  4%|â–Ž         | 17.1M/481M [00:01<00:21, 21.6MB/s]  4%|â–         | 20.6M/481M [00:01<00:18, 24.9MB/s]  5%|â–Œ         | 24.1M/481M [00:01<00:16, 27.8MB/s]  6%|â–Œ         | 27.1M/481M [00:01<00:18, 24.2MB/s]  6%|â–‹         | 30.7M/481M [00:01<00:16, 27.1MB/s]  7%|â–‹         | 34.0M/481M [00:01<00:18, 24.8MB/s]  8%|â–Š         | 37.7M/481M [00:01<00:16, 27.6MB/s]  9%|â–Š         | 41.5M/481M [00:02<00:16, 25.9MB/s]  9%|â–‰         | 45.3M/481M [00:02<00:15, 28.7MB/s] 10%|â–ˆ         | 49.1M/481M [00:02<00:13, 31.1MB/s] 11%|â–ˆ         | 52.3M/481M [00:02<00:15, 27.1MB/s] 12%|â–ˆâ–        | 55.7M/481M [00:02<00:14, 28.8MB/s] 12%|â–ˆâ–        | 59.3M/481M [00:02<00:16, 26.1MB/s] 13%|â–ˆâ–Ž        | 63.0M/481M [00:02<00:14, 29.0MB/s] 14%|â–ˆâ–        | 66.9M/481M [00:02<00:13, 31.3MB/s] 15%|â–ˆâ–        | 70.2M/481M [00:03<00:15, 27.3MB/s] 15%|â–ˆâ–Œ        | 73.9M/481M [00:03<00:13, 29.9MB/s] 16%|â–ˆâ–Œ        | 77.9M/481M [00:03<00:14, 27.8MB/s] 17%|â–ˆâ–‹        | 81.9M/481M [00:03<00:12, 30.9MB/s] 18%|â–ˆâ–Š        | 85.8M/481M [00:03<00:12, 32.9MB/s] 19%|â–ˆâ–Š        | 89.2M/481M [00:03<00:13, 28.9MB/s] 19%|â–ˆâ–‰        | 93.2M/481M [00:03<00:12, 31.7MB/s] 20%|â–ˆâ–ˆ        | 97.2M/481M [00:03<00:11, 33.7MB/s] 21%|â–ˆâ–ˆ        | 101M/481M [00:04<00:12, 29.5MB/s]  22%|â–ˆâ–ˆâ–       | 105M/481M [00:04<00:11, 32.2MB/s] 23%|â–ˆâ–ˆâ–Ž       | 109M/481M [00:04<00:10, 34.5MB/s] 23%|â–ˆâ–ˆâ–Ž       | 112M/481M [00:04<00:12, 30.1MB/s] 24%|â–ˆâ–ˆâ–       | 117M/481M [00:04<00:11, 32.9MB/s] 25%|â–ˆâ–ˆâ–Œ       | 120M/481M [00:04<00:10, 34.5MB/s] 26%|â–ˆâ–ˆâ–Œ       | 124M/481M [00:04<00:11, 30.2MB/s] 27%|â–ˆâ–ˆâ–‹       | 128M/481M [00:04<00:10, 32.2MB/s] 27%|â–ˆâ–ˆâ–‹       | 132M/481M [00:05<00:10, 32.0MB/s] 28%|â–ˆâ–ˆâ–Š       | 135M/481M [00:05<00:10, 32.3MB/s] 29%|â–ˆâ–ˆâ–‰       | 139M/481M [00:05<00:10, 31.4MB/s] 29%|â–ˆâ–ˆâ–‰       | 142M/481M [00:05<00:10, 32.4MB/s] 30%|â–ˆâ–ˆâ–ˆ       | 145M/481M [00:05<00:10, 33.0MB/s] 31%|â–ˆâ–ˆâ–ˆ       | 149M/481M [00:05<00:10, 31.2MB/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 153M/481M [00:05<00:10, 30.9MB/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 156M/481M [00:05<00:09, 32.9MB/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 160M/481M [00:05<00:09, 34.5MB/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 164M/481M [00:06<00:09, 32.3MB/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 167M/481M [00:06<00:09, 31.5MB/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 171M/481M [00:06<00:09, 33.0MB/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 175M/481M [00:06<00:09, 34.0MB/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 178M/481M [00:06<00:09, 31.8MB/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 182M/481M [00:06<00:09, 31.8MB/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 185M/481M [00:06<00:09, 32.7MB/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 189M/481M [00:06<00:08, 33.1MB/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 192M/481M [00:06<00:09, 31.0MB/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 196M/481M [00:07<00:09, 31.6MB/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200M/481M [00:07<00:08, 33.1MB/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204M/481M [00:07<00:08, 34.7MB/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 207M/481M [00:07<00:08, 32.4MB/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 211M/481M [00:07<00:08, 30.9MB/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 214M/481M [00:07<00:08, 32.0MB/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 218M/481M [00:07<00:08, 31.2MB/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 222M/481M [00:07<00:07, 32.9MB/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 225M/481M [00:07<00:08, 31.7MB/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 228M/481M [00:08<00:07, 32.3MB/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 232M/481M [00:08<00:08, 31.0MB/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 236M/481M [00:08<00:07, 31.1MB/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 240M/481M [00:08<00:07, 32.4MB/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244M/481M [00:08<00:07, 31.2MB/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247M/481M [00:08<00:07, 31.3MB/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251M/481M [00:08<00:07, 32.4MB/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 255M/481M [00:08<00:07, 31.2MB/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 259M/481M [00:08<00:06, 33.2MB/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262M/481M [00:09<00:06, 31.9MB/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 265M/481M [00:09<00:06, 32.1MB/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 269M/481M [00:09<00:06, 31.4MB/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 273M/481M [00:09<00:06, 33.4MB/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 276M/481M [00:09<00:06, 32.2MB/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 280M/481M [00:09<00:06, 32.1MB/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 284M/481M [00:09<00:06, 31.9MB/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 287M/481M [00:09<00:06, 32.3MB/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 290M/481M [00:09<00:06, 31.0MB/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 294M/481M [00:10<00:05, 31.9MB/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298M/481M [00:10<00:05, 33.8MB/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 301M/481M [00:10<00:05, 31.7MB/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 304M/481M [00:10<00:05, 30.8MB/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308M/481M [00:10<00:05, 31.9MB/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312M/481M [00:10<00:04, 34.0MB/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 315M/481M [00:10<00:05, 31.7MB/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 319M/481M [00:10<00:05, 31.5MB/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 322M/481M [00:10<00:04, 32.4MB/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 326M/481M [00:11<00:04, 34.4MB/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 330M/481M [00:11<00:04, 32.3MB/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 333M/481M [00:11<00:04, 33.4MB/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 337M/481M [00:11<00:04, 32.6MB/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 340M/481M [00:11<00:04, 32.8MB/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344M/481M [00:11<00:04, 32.4MB/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 347M/481M [00:11<00:04, 31.8MB/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 351M/481M [00:11<00:03, 33.8MB/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 354M/481M [00:11<00:03, 32.7MB/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358M/481M [00:12<00:03, 33.8MB/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 362M/481M [00:12<00:03, 32.5MB/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 365M/481M [00:12<00:03, 32.7MB/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 369M/481M [00:12<00:03, 34.1MB/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 372M/481M [00:12<00:03, 32.0MB/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 376M/481M [00:12<00:03, 31.5MB/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 380M/481M [00:12<00:03, 32.7MB/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 383M/481M [00:12<00:02, 33.9MB/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 387M/481M [00:12<00:02, 32.7MB/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 390M/481M [00:13<00:02, 33.0MB/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 393M/481M [00:13<00:02, 31.5MB/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 397M/481M [00:13<00:02, 32.1MB/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 401M/481M [00:13<00:02, 33.6MB/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 404M/481M [00:13<00:02, 32.4MB/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407M/481M [00:13<00:02, 32.8MB/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 411M/481M [00:13<00:02, 33.9MB/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 414M/481M [00:13<00:02, 33.1MB/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 418M/481M [00:13<00:01, 33.0MB/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 421M/481M [00:13<00:01, 32.7MB/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 424M/481M [00:14<00:01, 31.8MB/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 428M/481M [00:14<00:01, 32.6MB/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 431M/481M [00:14<00:01, 32.0MB/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 435M/481M [00:14<00:01, 33.1MB/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 438M/481M [00:14<00:01, 33.0MB/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 442M/481M [00:14<00:01, 32.1MB/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 445M/481M [00:14<00:01, 33.1MB/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 449M/481M [00:14<00:01, 32.0MB/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452M/481M [00:14<00:00, 33.2MB/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 456M/481M [00:15<00:00, 33.3MB/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 459M/481M [00:15<00:00, 32.8MB/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 463M/481M [00:15<00:00, 33.2MB/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 466M/481M [00:15<00:00, 33.7MB/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 470M/481M [00:15<00:00, 32.3MB/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 473M/481M [00:15<00:00, 33.0MB/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 476M/481M [00:15<00:00, 32.7MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 480M/481M [00:15<00:00, 32.3MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481M/481M [00:15<00:00, 30.5MB/s]
textattack: Unzipping file /root/.cache/textattack/tmpl2wsx3ue.zip to /root/.cache/textattack/word_embeddings/paragramcf.
textattack: Successfully saved word_embeddings/paragramcf to cache.
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 18.4MB/s]                   2024-03-08 18:05:29.005551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.014359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.016760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.032503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.035047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.037414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.215150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.216795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.218267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:05:29.219730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0

Downloading builder script:   0%|          | 0.00/5.03k [00:00<?, ?B/s]Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.03k/5.03k [00:00<00:00, 33.0MB/s]
Downloading readme:   0%|          | 0.00/7.25k [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.25k/7.25k [00:00<00:00, 42.9MB/s]
Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 488k/488k [00:00<00:00, 8.10MB/s]
Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]Generating train split:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4801/8530 [00:00<00:00, 47281.26 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8530/8530 [00:00<00:00, 50076.37 examples/s]
Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 36706.66 examples/s]
Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 36057.78 examples/s]
Map:   0%|          | 0/8530 [00:00<?, ? examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 2400/8530 [00:00<00:00, 23844.71 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4822/8530 [00:00<00:00, 24055.37 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7243/8530 [00:00<00:00, 24121.41 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8530/8530 [00:00<00:00, 23857.78 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 17965.07 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 22994.78 examples/s]
Map:   0%|          | 0/8103 [00:00<?, ? examples/s]Map:  16%|â–ˆâ–‹        | 1321/8103 [00:00<00:00, 13133.42 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2689/8103 [00:00<00:00, 13322.76 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4678/8103 [00:00<00:00, 13258.04 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6677/8103 [00:00<00:00, 13223.61 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8103/8103 [00:00<00:00, 13119.47 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8103/8103 [00:00<00:00, 13148.33 examples/s]
Map:   0%|          | 0/1066 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:00<00:00, 23316.88 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 24086.09 examples/s]
Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 776/776 [00:00<00:00, 148kB/s]
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500k/500k [00:00<00:00, 12.2MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:00<00:00, 5.00MB/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:00<00:00, 4.99MB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 294kB/s]
Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 609/609 [00:00<00:00, 92.6kB/s]
Downloading (â€¦)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading (â€¦)fetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.8k/26.8k [00:00<00:00, 20.3MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
Downloading (â€¦)of-00002.safetensors:   0%|          | 21.0M/9.98G [00:00<00:52, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 52.4M/9.98G [00:00<00:44, 223MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:41, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 115M/9.98G [00:00<00:40, 243MB/s] [A
Downloading (â€¦)of-00002.safetensors:   1%|â–         | 147M/9.98G [00:00<00:39, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 178M/9.98G [00:00<00:40, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 210M/9.98G [00:00<00:42, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 241M/9.98G [00:01<00:41, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 273M/9.98G [00:01<00:40, 240MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 304M/9.98G [00:01<00:39, 247MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 336M/9.98G [00:01<00:38, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–Ž         | 367M/9.98G [00:01<00:38, 247MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 398M/9.98G [00:01<00:40, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 430M/9.98G [00:01<00:40, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–         | 461M/9.98G [00:01<00:40, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–         | 493M/9.98G [00:02<00:40, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 524M/9.98G [00:02<00:41, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 556M/9.98G [00:02<00:42, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 587M/9.98G [00:02<00:45, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 619M/9.98G [00:02<00:43, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 650M/9.98G [00:02<00:45, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 671M/9.98G [00:02<00:46, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 692M/9.98G [00:03<00:46, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 713M/9.98G [00:03<00:50, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 734M/9.98G [00:03<00:53, 173MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 755M/9.98G [00:03<00:52, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 776M/9.98G [00:03<00:50, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 807M/9.98G [00:03<00:47, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 839M/9.98G [00:03<00:45, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–Š         | 860M/9.98G [00:03<00:45, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 891M/9.98G [00:04<00:42, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 923M/9.98G [00:04<00:42, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 954M/9.98G [00:04<00:42, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 986M/9.98G [00:04<00:41, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–ˆ         | 1.02G/9.98G [00:04<00:43, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–ˆ         | 1.04G/9.98G [00:04<00:45, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.06G/9.98G [00:04<00:45, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.09G/9.98G [00:05<00:42, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.12G/9.98G [00:05<00:42, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.15G/9.98G [00:05<00:40, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.18G/9.98G [00:05<00:40, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.22G/9.98G [00:05<00:39, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.25G/9.98G [00:05<00:41, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.28G/9.98G [00:05<00:44, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.30G/9.98G [00:06<00:44, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.32G/9.98G [00:06<00:44, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–Ž        | 1.35G/9.98G [00:06<00:41, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 1.38G/9.98G [00:06<00:40, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 1.42G/9.98G [00:06<00:40, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 1.45G/9.98G [00:06<00:39, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 1.48G/9.98G [00:06<00:38, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–Œ        | 1.51G/9.98G [00:07<00:39, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–Œ        | 1.54G/9.98G [00:07<00:40, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.57G/9.98G [00:07<00:39, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.60G/9.98G [00:07<00:39, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–‹        | 1.64G/9.98G [00:07<00:39, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.67G/9.98G [00:07<00:39, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.70G/9.98G [00:08<00:45, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.72G/9.98G [00:08<00:45, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.74G/9.98G [00:08<00:44, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.76G/9.98G [00:08<00:43, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.78G/9.98G [00:08<00:43, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.81G/9.98G [00:08<00:41, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.84G/9.98G [00:08<00:42, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–Š        | 1.86G/9.98G [00:08<00:43, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.88G/9.98G [00:08<00:44, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.90G/9.98G [00:09<00:43, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.92G/9.98G [00:09<00:42, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.94G/9.98G [00:09<00:43, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 1.96G/9.98G [00:09<00:41, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 1.99G/9.98G [00:09<00:40, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 2.01G/9.98G [00:09<00:41, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 2.03G/9.98G [00:09<00:40, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.07G/9.98G [00:09<00:38, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.09G/9.98G [00:10<00:39, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.11G/9.98G [00:10<00:39, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆâ–       | 2.14G/9.98G [00:10<00:38, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.17G/9.98G [00:10<00:36, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.20G/9.98G [00:10<00:35, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.23G/9.98G [00:10<00:36, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.26G/9.98G [00:10<00:35, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.30G/9.98G [00:10<00:35, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.33G/9.98G [00:11<00:35, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–Ž       | 2.36G/9.98G [00:11<00:35, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.39G/9.98G [00:11<00:34, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.42G/9.98G [00:11<00:34, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 2.45G/9.98G [00:11<00:36, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 2.49G/9.98G [00:11<00:35, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 2.52G/9.98G [00:12<00:38, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 2.54G/9.98G [00:12<00:38, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.56G/9.98G [00:12<00:38, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.59G/9.98G [00:12<00:39, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 2.62G/9.98G [00:12<00:37, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 2.64G/9.98G [00:12<00:57, 128MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.67G/9.98G [00:13<00:49, 148MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.71G/9.98G [00:13<00:43, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.74G/9.98G [00:13<00:39, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.77G/9.98G [00:13<00:36, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.80G/9.98G [00:13<00:34, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.83G/9.98G [00:13<00:33, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–Š       | 2.86G/9.98G [00:13<00:35, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.88G/9.98G [00:14<00:36, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.90G/9.98G [00:14<00:36, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.93G/9.98G [00:14<00:36, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 2.95G/9.98G [00:14<00:37, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 2.97G/9.98G [00:14<00:39, 180MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 2.99G/9.98G [00:14<00:44, 156MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 3.01G/9.98G [00:14<00:42, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 3.03G/9.98G [00:14<00:40, 171MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.05G/9.98G [00:15<00:39, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.07G/9.98G [00:15<00:38, 181MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.09G/9.98G [00:15<00:37, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 3.12G/9.98G [00:15<00:35, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.16G/9.98G [00:15<00:34, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.19G/9.98G [00:15<00:32, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.22G/9.98G [00:15<00:31, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.25G/9.98G [00:16<00:31, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.28G/9.98G [00:16<00:31, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.31G/9.98G [00:16<00:31, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.34G/9.98G [00:16<00:30, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.38G/9.98G [00:16<00:32, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.40G/9.98G [00:16<00:34, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.42G/9.98G [00:16<00:33, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 3.45G/9.98G [00:17<00:31, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 3.48G/9.98G [00:17<00:31, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.50G/9.98G [00:17<00:31, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.52G/9.98G [00:17<00:33, 193MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.54G/9.98G [00:17<00:34, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.57G/9.98G [00:17<00:34, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.59G/9.98G [00:17<00:35, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.61G/9.98G [00:17<00:34, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 3.63G/9.98G [00:17<00:34, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.66G/9.98G [00:18<00:32, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.69G/9.98G [00:18<00:30, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.72G/9.98G [00:18<00:29, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.75G/9.98G [00:18<00:28, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.79G/9.98G [00:18<00:30, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.81G/9.98G [00:18<00:31, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.83G/9.98G [00:18<00:31, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 3.86G/9.98G [00:19<00:30, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.88G/9.98G [00:19<00:31, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.90G/9.98G [00:19<00:32, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.92G/9.98G [00:19<00:33, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.94G/9.98G [00:19<00:33, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.96G/9.98G [00:19<00:32, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.98G/9.98G [00:19<00:31, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.02G/9.98G [00:19<00:29, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.05G/9.98G [00:20<00:28, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.07G/9.98G [00:20<00:28, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.09G/9.98G [00:20<00:30, 193MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.11G/9.98G [00:20<00:31, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.14G/9.98G [00:20<00:30, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.17G/9.98G [00:20<00:28, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.19G/9.98G [00:20<00:28, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.22G/9.98G [00:20<00:31, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.24G/9.98G [00:21<00:30, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.27G/9.98G [00:21<00:31, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.29G/9.98G [00:21<00:30, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.31G/9.98G [00:21<00:30, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.33G/9.98G [00:21<00:30, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.35G/9.98G [00:21<00:30, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.38G/9.98G [00:21<00:28, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.40G/9.98G [00:21<00:28, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.44G/9.98G [00:22<00:27, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.46G/9.98G [00:22<00:26, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.48G/9.98G [00:22<00:28, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.50G/9.98G [00:22<00:27, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.52G/9.98G [00:22<00:27, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.54G/9.98G [00:22<00:27, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.56G/9.98G [00:22<00:27, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.58G/9.98G [00:22<00:29, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.60G/9.98G [00:22<00:29, 180MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.62G/9.98G [00:23<00:28, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.66G/9.98G [00:23<00:26, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.69G/9.98G [00:23<00:25, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.72G/9.98G [00:23<00:24, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.75G/9.98G [00:23<00:23, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.78G/9.98G [00:23<00:23, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.81G/9.98G [00:23<00:23, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.84G/9.98G [00:24<00:24, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.87G/9.98G [00:24<00:36, 140MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.89G/9.98G [00:24<00:33, 150MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.91G/9.98G [00:24<00:31, 161MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.94G/9.98G [00:24<00:28, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.96G/9.98G [00:24<00:27, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4.99G/9.98G [00:24<00:25, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.02G/9.98G [00:25<00:24, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.05G/9.98G [00:25<00:23, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.09G/9.98G [00:25<00:22, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.12G/9.98G [00:25<00:22, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.15G/9.98G [00:25<00:22, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.18G/9.98G [00:25<00:21, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.21G/9.98G [00:25<00:20, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.24G/9.98G [00:26<00:22, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.27G/9.98G [00:26<00:23, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.30G/9.98G [00:26<00:23, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.32G/9.98G [00:26<00:24, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.34G/9.98G [00:26<00:24, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.36G/9.98G [00:26<00:23, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.38G/9.98G [00:26<00:23, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.40G/9.98G [00:26<00:24, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.42G/9.98G [00:27<00:24, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.45G/9.98G [00:27<00:22, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.47G/9.98G [00:27<00:24, 181MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.49G/9.98G [00:27<00:24, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.53G/9.98G [00:27<00:23, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.55G/9.98G [00:27<00:24, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.57G/9.98G [00:27<00:23, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.59G/9.98G [00:28<00:24, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.61G/9.98G [00:28<00:23, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.64G/9.98G [00:28<00:22, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.66G/9.98G [00:28<00:24, 179MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.68G/9.98G [00:28<00:27, 157MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.70G/9.98G [00:28<00:25, 165MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.73G/9.98G [00:28<00:26, 163MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.75G/9.98G [00:28<00:24, 171MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.77G/9.98G [00:29<00:23, 180MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.80G/9.98G [00:29<00:21, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.83G/9.98G [00:29<00:20, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.86G/9.98G [00:29<00:19, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.89G/9.98G [00:29<00:19, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.92G/9.98G [00:29<00:18, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.96G/9.98G [00:29<00:18, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5.99G/9.98G [00:30<00:18, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.02G/9.98G [00:30<00:19, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.05G/9.98G [00:30<00:18, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.08G/9.98G [00:30<00:18, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.10G/9.98G [00:30<00:19, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.12G/9.98G [00:30<00:20, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.16G/9.98G [00:30<00:19, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.18G/9.98G [00:30<00:19, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.21G/9.98G [00:31<00:18, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.24G/9.98G [00:31<00:17, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.27G/9.98G [00:31<00:17, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.30G/9.98G [00:31<00:17, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.33G/9.98G [00:31<00:17, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.35G/9.98G [00:31<00:17, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.38G/9.98G [00:31<00:18, 193MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.40G/9.98G [00:32<00:18, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.42G/9.98G [00:32<00:18, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.45G/9.98G [00:32<00:17, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.47G/9.98G [00:32<00:17, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.50G/9.98G [00:32<00:17, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.52G/9.98G [00:32<00:17, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.54G/9.98G [00:32<00:17, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.56G/9.98G [00:32<00:19, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.59G/9.98G [00:33<00:18, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.61G/9.98G [00:33<00:18, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.64G/9.98G [00:33<00:16, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.67G/9.98G [00:33<00:15, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.70G/9.98G [00:33<00:15, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.73G/9.98G [00:33<00:14, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.76G/9.98G [00:33<00:14, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.79G/9.98G [00:34<00:14, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.83G/9.98G [00:34<00:14, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.86G/9.98G [00:34<00:14, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.89G/9.98G [00:34<00:13, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.92G/9.98G [00:34<00:14, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.95G/9.98G [00:34<00:16, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.97G/9.98G [00:34<00:16, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.99G/9.98G [00:35<00:15, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.03G/9.98G [00:35<00:14, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.05G/9.98G [00:35<00:15, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.07G/9.98G [00:35<00:15, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.09G/9.98G [00:35<00:15, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.11G/9.98G [00:35<00:16, 179MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.13G/9.98G [00:35<00:16, 175MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.15G/9.98G [00:37<01:18, 35.9MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.17G/9.98G [00:37<00:59, 47.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.19G/9.98G [00:37<00:53, 52.1MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.22G/9.98G [00:38<00:36, 75.0MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.26G/9.98G [00:38<00:27, 99.1MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.28G/9.98G [00:38<00:24, 111MB/s] [A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.31G/9.98G [00:38<00:19, 135MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.34G/9.98G [00:38<00:18, 144MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.36G/9.98G [00:38<00:19, 132MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.38G/9.98G [00:38<00:18, 144MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.40G/9.98G [00:39<00:16, 154MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.42G/9.98G [00:39<00:15, 163MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.44G/9.98G [00:39<00:14, 173MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.48G/9.98G [00:39<00:13, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.50G/9.98G [00:39<00:13, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.52G/9.98G [00:39<00:12, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.54G/9.98G [00:39<00:12, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.56G/9.98G [00:39<00:12, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.59G/9.98G [00:39<00:11, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.62G/9.98G [00:40<00:11, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.65G/9.98G [00:40<00:11, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.68G/9.98G [00:40<00:11, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.70G/9.98G [00:40<00:11, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.72G/9.98G [00:40<00:11, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.74G/9.98G [00:40<00:11, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.76G/9.98G [00:40<00:11, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.78G/9.98G [00:40<00:11, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.81G/9.98G [00:41<00:10, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.83G/9.98G [00:41<00:10, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.86G/9.98G [00:41<00:10, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.90G/9.98G [00:41<00:09, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.93G/9.98G [00:41<00:09, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.96G/9.98G [00:41<00:09, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.98G/9.98G [00:41<00:09, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.01G/9.98G [00:42<00:09, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.04G/9.98G [00:42<00:08, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.07G/9.98G [00:42<00:08, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.11G/9.98G [00:42<00:08, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.14G/9.98G [00:42<00:08, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.16G/9.98G [00:42<00:08, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.19G/9.98G [00:42<00:08, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.22G/9.98G [00:43<00:08, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.25G/9.98G [00:43<00:07, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.28G/9.98G [00:43<00:07, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.32G/9.98G [00:43<00:07, 223MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.35G/9.98G [00:43<00:07, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.38G/9.98G [00:43<00:07, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.41G/9.98G [00:43<00:07, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.44G/9.98G [00:44<00:06, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.47G/9.98G [00:44<00:06, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.50G/9.98G [00:44<00:06, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.54G/9.98G [00:44<00:06, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.57G/9.98G [00:44<00:05, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.60G/9.98G [00:44<00:05, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.63G/9.98G [00:44<00:05, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.66G/9.98G [00:45<00:06, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.69G/9.98G [00:45<00:06, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.71G/9.98G [00:45<00:06, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.75G/9.98G [00:45<00:06, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.78G/9.98G [00:45<00:05, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.81G/9.98G [00:45<00:05, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.84G/9.98G [00:45<00:05, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.87G/9.98G [00:45<00:05, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.90G/9.98G [00:46<00:04, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.93G/9.98G [00:46<00:04, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.97G/9.98G [00:46<00:04, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.00G/9.98G [00:46<00:04, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.03G/9.98G [00:46<00:04, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.06G/9.98G [00:46<00:03, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.09G/9.98G [00:46<00:03, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.12G/9.98G [00:47<00:03, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.15G/9.98G [00:47<00:03, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.19G/9.98G [00:47<00:03, 250MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.22G/9.98G [00:47<00:03, 250MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.25G/9.98G [00:47<00:02, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.28G/9.98G [00:47<00:02, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.31G/9.98G [00:47<00:02, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.34G/9.98G [00:47<00:02, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.37G/9.98G [00:48<00:02, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.41G/9.98G [00:48<00:02, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.43G/9.98G [00:48<00:02, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.45G/9.98G [00:48<00:02, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.48G/9.98G [00:48<00:02, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.51G/9.98G [00:48<00:02, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.54G/9.98G [00:49<00:02, 173MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.57G/9.98G [00:49<00:02, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.60G/9.98G [00:49<00:01, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.64G/9.98G [00:49<00:01, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.67G/9.98G [00:49<00:01, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.70G/9.98G [00:49<00:01, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.73G/9.98G [00:49<00:01, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.76G/9.98G [00:50<00:01, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.79G/9.98G [00:50<00:00, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.83G/9.98G [00:50<00:00, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.86G/9.98G [00:50<00:00, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.89G/9.98G [00:50<00:00, 207MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.91G/9.98G [00:50<00:00, 206MB/s][A
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.94G/9.98G [00:50<00:00, 211MB/s][A
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.97G/9.98G [00:51<00:00, 208MB/s][ADownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.98G/9.98G [00:51<00:00, 195MB/s]
Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:51<00:51, 51.25s/it]
Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:17, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|â–         | 52.4M/3.50G [00:00<00:16, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 73.4M/3.50G [00:00<00:16, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 94.4M/3.50G [00:00<00:17, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 115M/3.50G [00:00<00:17, 195MB/s] [A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 136M/3.50G [00:00<00:16, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 157M/3.50G [00:00<00:16, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 178M/3.50G [00:00<00:18, 181MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 199M/3.50G [00:01<00:17, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–‹         | 220M/3.50G [00:01<00:25, 127MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 252M/3.50G [00:01<00:21, 151MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 273M/3.50G [00:01<00:20, 159MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 294M/3.50G [00:01<00:19, 168MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 325M/3.50G [00:01<00:17, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 346M/3.50G [00:01<00:17, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–ˆ         | 367M/3.50G [00:02<00:16, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 388M/3.50G [00:02<00:17, 174MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 409M/3.50G [00:02<00:17, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 430M/3.50G [00:02<00:26, 117MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 451M/3.50G [00:02<00:31, 95.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 482M/3.50G [00:03<00:25, 118MB/s] [A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 503M/3.50G [00:03<00:23, 130MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 524M/3.50G [00:03<00:20, 144MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 556M/3.50G [00:03<00:17, 164MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–‹        | 577M/3.50G [00:03<00:16, 172MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 598M/3.50G [00:03<00:16, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 629M/3.50G [00:03<00:15, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–Š        | 650M/3.50G [00:04<00:24, 116MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 671M/3.50G [00:04<00:31, 90.4MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 692M/3.50G [00:04<00:27, 101MB/s] [A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 713M/3.50G [00:04<00:24, 116MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 734M/3.50G [00:04<00:21, 127MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 765M/3.50G [00:05<00:18, 149MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 797M/3.50G [00:05<00:16, 168MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 818M/3.50G [00:05<00:15, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 849M/3.50G [00:05<00:13, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 870M/3.50G [00:06<00:29, 88.5MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 891M/3.50G [00:06<00:25, 103MB/s] [A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 912M/3.50G [00:06<00:22, 117MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 933M/3.50G [00:06<00:19, 133MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 954M/3.50G [00:06<00:17, 147MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 975M/3.50G [00:06<00:16, 157MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 1.01G/3.50G [00:06<00:14, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 1.04G/3.50G [00:06<00:12, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.07G/3.50G [00:07<00:12, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.09G/3.50G [00:07<00:17, 139MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.11G/3.50G [00:07<00:28, 83.7MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.13G/3.50G [00:08<00:24, 96.4MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.15G/3.50G [00:08<00:21, 111MB/s] [A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.17G/3.50G [00:08<00:18, 123MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.21G/3.50G [00:08<00:15, 147MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.24G/3.50G [00:08<00:14, 160MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.26G/3.50G [00:08<00:13, 167MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.29G/3.50G [00:08<00:11, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.31G/3.50G [00:08<00:11, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.33G/3.50G [00:09<00:25, 83.4MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1.35G/3.50G [00:09<00:21, 97.9MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.38G/3.50G [00:09<00:17, 121MB/s] [A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.42G/3.50G [00:10<00:14, 141MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.44G/3.50G [00:10<00:16, 128MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.46G/3.50G [00:10<00:14, 138MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.49G/3.50G [00:10<00:12, 162MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.51G/3.50G [00:10<00:11, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.53G/3.50G [00:10<00:11, 172MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.55G/3.50G [00:10<00:12, 160MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.57G/3.50G [00:11<00:16, 120MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.59G/3.50G [00:11<00:14, 135MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.61G/3.50G [00:11<00:12, 147MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.64G/3.50G [00:11<00:12, 154MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.66G/3.50G [00:11<00:11, 159MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.68G/3.50G [00:11<00:11, 163MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.70G/3.50G [00:11<00:10, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.72G/3.50G [00:11<00:10, 174MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.75G/3.50G [00:12<00:09, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.77G/3.50G [00:12<00:18, 93.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.79G/3.50G [00:12<00:16, 105MB/s] [A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.81G/3.50G [00:12<00:14, 120MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.84G/3.50G [00:13<00:12, 133MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1.86G/3.50G [00:13<00:11, 146MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.89G/3.50G [00:13<00:09, 167MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.91G/3.50G [00:13<00:09, 175MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.93G/3.50G [00:13<00:09, 168MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.95G/3.50G [00:13<00:09, 168MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.98G/3.50G [00:13<00:08, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.00G/3.50G [00:14<00:15, 96.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.02G/3.50G [00:14<00:15, 95.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.04G/3.50G [00:14<00:13, 107MB/s] [A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.07G/3.50G [00:14<00:11, 121MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.09G/3.50G [00:14<00:10, 136MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.12G/3.50G [00:15<00:08, 158MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.15G/3.50G [00:15<00:07, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.17G/3.50G [00:15<00:07, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.20G/3.50G [00:15<00:06, 193MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.23G/3.50G [00:15<00:09, 135MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.25G/3.50G [00:16<00:10, 117MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.28G/3.50G [00:16<00:09, 131MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.30G/3.50G [00:16<00:08, 141MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.32G/3.50G [00:16<00:07, 153MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.34G/3.50G [00:16<00:07, 163MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.36G/3.50G [00:16<00:06, 172MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.38G/3.50G [00:16<00:06, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.41G/3.50G [00:16<00:05, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.44G/3.50G [00:16<00:05, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.46G/3.50G [00:17<00:06, 154MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.49G/3.50G [00:17<00:06, 165MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.52G/3.50G [00:17<00:05, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.54G/3.50G [00:17<00:05, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2.57G/3.50G [00:17<00:04, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.60G/3.50G [00:17<00:04, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.63G/3.50G [00:17<00:03, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.66G/3.50G [00:18<00:03, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.69G/3.50G [00:18<00:05, 157MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.72G/3.50G [00:18<00:06, 129MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.75G/3.50G [00:18<00:05, 149MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.77G/3.50G [00:19<00:05, 139MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.79G/3.50G [00:19<00:04, 148MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.81G/3.50G [00:19<00:04, 159MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.83G/3.50G [00:19<00:04, 160MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.85G/3.50G [00:19<00:03, 169MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.87G/3.50G [00:19<00:03, 170MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2.89G/3.50G [00:19<00:03, 175MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2.93G/3.50G [00:19<00:02, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.96G/3.50G [00:19<00:02, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.98G/3.50G [00:20<00:04, 116MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3.00G/3.50G [00:20<00:03, 130MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.03G/3.50G [00:20<00:03, 150MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.05G/3.50G [00:20<00:02, 154MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.08G/3.50G [00:20<00:02, 170MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.11G/3.50G [00:21<00:02, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.14G/3.50G [00:21<00:02, 131MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.17G/3.50G [00:21<00:02, 157MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.20G/3.50G [00:21<00:02, 108MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.22G/3.50G [00:22<00:02, 101MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.24G/3.50G [00:22<00:02, 112MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.27G/3.50G [00:22<00:01, 133MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.29G/3.50G [00:22<00:01, 142MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.31G/3.50G [00:22<00:01, 152MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.33G/3.50G [00:22<00:01, 162MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.37G/3.50G [00:22<00:00, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.40G/3.50G [00:23<00:00, 192MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.42G/3.50G [00:23<00:00, 114MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.44G/3.50G [00:23<00:00, 124MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.46G/3.50G [00:23<00:00, 136MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.48G/3.50G [00:23<00:00, 146MB/s][A
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50G/3.50G [00:24<00:00, 151MB/s][ADownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50G/3.50G [00:24<00:00, 146MB/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:15<00:00, 35.30s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:15<00:00, 37.70s/it]
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 20.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.91s/it]
Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]Downloading generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [00:00<00:00, 29.5kB/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 10802.41 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:09:33.693209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:09:34.572171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:09:41.834723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.844051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.846462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.861972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.864349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:41.866688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.055454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.057076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.058527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:09:42.059984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.18s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8
2024-03-08 18:12:27.522372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:12:28.369382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:12:35.903887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.916553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.919004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.935065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.937443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:35.939774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.128934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.130564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.132023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:12:36.133494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 20.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.99s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 10698.34 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.025 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:15:25.203796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:15:26.016046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:15:34.025427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.039257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.041751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.061770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.064166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.066508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.248928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.250542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.251996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:15:34.253459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.01s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8
2024-03-08 18:18:20.286905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:18:21.118834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:18:28.586251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.598230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.600710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.624021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.626558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.628907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.816215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.817852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.819311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:18:28.820785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 19.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.82s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 10844.64 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:21:13.381304: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:21:14.190597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:21:21.931563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.941428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.948440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.965843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.968444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:21.970820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.164201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.165847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.167316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:21:22.168796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.26s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2
2024-03-08 18:24:07.082678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:24:07.877641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:24:15.360456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.369539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.371956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.387514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.389948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.392514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.578853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.580467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.581921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:24:15.583393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.04s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 15126.55 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:27:03.120205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:27:03.932522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:27:11.164406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.173919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.176343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.191788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.194174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.196526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.385462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.387093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.388544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:27:11.390007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 19.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.62s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2
2024-03-08 18:29:55.091451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:29:55.920480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:30:03.327650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.337275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.339683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.355652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.358042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.360400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.549718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.551366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.553200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:30:03.554670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.01s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 14993.04 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:32:48.069796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:32:48.869540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:32:56.099852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.109501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.111947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.127949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.130497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.132862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.319051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.320710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.322172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:32:56.323635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.21s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2
2024-03-08 18:35:43.088009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:35:43.913536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:35:51.450063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.464355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.466856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.487709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.490123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.492473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.681443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.683086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.684546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:35:51.686015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.01s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 14681.21 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:38:39.985596: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:38:40.759356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:38:48.016638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.025957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.028389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.043440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.046000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.048342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.236053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.237686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.239262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:38:48.241139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.07s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.021 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4
2024-03-08 18:41:32.676495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:41:33.489706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:41:41.018722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.029830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.032291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.052623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.055162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.057485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.248623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.250297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.251756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:41:41.253221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 20.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.99s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 14105.00 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:44:26.462007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:44:27.252240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:44:34.356445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.368729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.371146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.386778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.389173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.391522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.615960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.618964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.621590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:44:34.624316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 20.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.87s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4
2024-03-08 18:47:20.636759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:47:21.503797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:47:29.209536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.220968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.223383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.240533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.242925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.245333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.431928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.433649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.435153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:47:29.436639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.39s/it]
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 187349ac-3665-483d-9d61-8bbf7a93aea5)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 187349ac-3665-483d-9d61-8bbf7a93aea5)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 13452.42 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:50:28.081366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:50:28.902993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:50:36.178935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.188405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.190788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.205896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.208263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.210587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.401292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.402864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.404307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:50:36.405753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 14936211-b65e-4c55-88fc-6577e1048d7a)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 14936211-b65e-4c55-88fc-6577e1048d7a)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 3beaef89-83d5-4985-90cd-df98bca02303)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 3beaef89-83d5-4985-90cd-df98bca02303)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7c300cfe-070e-4ea2-8bc2-ee2262371074)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7c300cfe-070e-4ea2-8bc2-ee2262371074)')' thrown while requesting HEAD https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.07s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4
2024-03-08 18:56:15.933530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:56:16.746850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:56:24.297172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.310059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.312703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.335487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.337886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.340221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.530089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.531713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.533167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:56:24.534622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.04s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 13725.80 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.026 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 18:59:18.054932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 18:59:18.835814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 18:59:25.980133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:25.989891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:25.992312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.007062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.009461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.011803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.214106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.215727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.217190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 18:59:26.218665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 19.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.85s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16
2024-03-08 19:03:21.000217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:03:21.840357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:03:29.388218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.399147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.401590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.418773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.421163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.423505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.611009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.612630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.614098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:03:29.615566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.28s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 8143.08 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7902.11 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:06:15.994141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:06:16.833107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:06:24.290652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.300318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.302732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.318795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.322683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.325029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.529872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.531489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.532932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:06:24.534394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.06s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16
2024-03-08 19:09:10.444086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:09:11.311779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:09:18.739722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.748769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.751177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.767469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.769866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.772438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.958397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.960021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.961480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:09:18.962934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.02s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 8001.77 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7742.98 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:12:03.356316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:12:04.188865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:12:11.457265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.467401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.469895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.485399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.487828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.490366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.699980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.701628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.703101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:12:11.704572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.00s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16
2024-03-08 19:14:57.719793: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:14:58.567181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:15:06.163923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.173518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.175939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.190909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.193293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.195642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.392679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.394329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.395883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:15:06.397372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.24s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7641.33 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7393.83 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-08 19:17:52.960652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-08 19:17:53.752263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-08 19:18:00.940031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.952038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.954487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.970163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.972573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:00.974952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.181782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.183422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.184888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-08 19:18:01.186371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:33<00:33, 33.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 20.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.45s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/mr/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
