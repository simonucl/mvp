1+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-8
textattack: Updating TextAttack package dependencies.
textattack: Downloading NLTK required packages.
2024-03-10 18:26:50.485484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 18:26:51.283697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package omw to /root/nltk_data...
[nltk_data] Downloading package universal_tagset to /root/nltk_data...
[nltk_data]   Unzipping taggers/universal_tagset.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.
  0%|          | 0.00/481M [00:00<?, ?B/s]  0%|          | 51.2k/481M [00:00<26:38, 301kB/s]  0%|          | 155k/481M [00:00<16:17, 492kB/s]   0%|          | 591k/481M [00:00<05:33, 1.44MB/s]  0%|          | 1.70M/481M [00:00<02:07, 3.75MB/s]  1%|          | 4.03M/481M [00:00<00:56, 8.45MB/s]  2%|â–         | 8.44M/481M [00:00<00:26, 17.9MB/s]  2%|â–         | 11.6M/481M [00:00<00:23, 20.3MB/s]  3%|â–Ž         | 15.5M/481M [00:01<00:19, 24.0MB/s]  4%|â–         | 20.0M/481M [00:01<00:15, 29.6MB/s]  5%|â–         | 23.2M/481M [00:01<00:16, 28.6MB/s]  6%|â–Œ         | 27.0M/481M [00:01<00:14, 31.3MB/s]  6%|â–‹         | 30.3M/481M [00:01<00:14, 30.5MB/s]  7%|â–‹         | 34.4M/481M [00:01<00:14, 30.8MB/s]  8%|â–Š         | 38.8M/481M [00:01<00:12, 34.2MB/s]  9%|â–‰         | 42.3M/481M [00:01<00:13, 33.0MB/s] 10%|â–‰         | 46.2M/481M [00:02<00:13, 32.0MB/s] 10%|â–ˆ         | 50.1M/481M [00:02<00:13, 32.0MB/s] 11%|â–ˆâ–        | 54.6M/481M [00:02<00:12, 35.3MB/s] 12%|â–ˆâ–        | 58.2M/481M [00:02<00:12, 33.4MB/s] 13%|â–ˆâ–Ž        | 61.9M/481M [00:02<00:12, 32.4MB/s] 14%|â–ˆâ–Ž        | 65.8M/481M [00:02<00:13, 31.6MB/s] 15%|â–ˆâ–        | 70.0M/481M [00:02<00:11, 34.3MB/s] 15%|â–ˆâ–Œ        | 73.5M/481M [00:02<00:12, 33.0MB/s] 16%|â–ˆâ–Œ        | 77.4M/481M [00:02<00:12, 32.2MB/s] 17%|â–ˆâ–‹        | 81.5M/481M [00:03<00:11, 34.5MB/s] 18%|â–ˆâ–Š        | 85.0M/481M [00:03<00:11, 33.3MB/s] 18%|â–ˆâ–Š        | 89.0M/481M [00:03<00:11, 34.8MB/s] 19%|â–ˆâ–‰        | 92.5M/481M [00:03<00:11, 33.5MB/s] 20%|â–ˆâ–ˆ        | 96.3M/481M [00:03<00:11, 34.8MB/s] 21%|â–ˆâ–ˆ        | 99.9M/481M [00:03<00:11, 33.6MB/s] 21%|â–ˆâ–ˆâ–       | 103M/481M [00:03<00:11, 34.2MB/s]  22%|â–ˆâ–ˆâ–       | 107M/481M [00:03<00:11, 33.2MB/s] 23%|â–ˆâ–ˆâ–Ž       | 111M/481M [00:03<00:10, 34.6MB/s] 24%|â–ˆâ–ˆâ–       | 115M/481M [00:04<00:10, 35.3MB/s] 25%|â–ˆâ–ˆâ–       | 118M/481M [00:04<00:10, 34.9MB/s] 25%|â–ˆâ–ˆâ–Œ       | 122M/481M [00:04<00:10, 34.0MB/s] 26%|â–ˆâ–ˆâ–Œ       | 125M/481M [00:04<00:10, 33.2MB/s] 27%|â–ˆâ–ˆâ–‹       | 129M/481M [00:04<00:10, 33.8MB/s] 27%|â–ˆâ–ˆâ–‹       | 132M/481M [00:04<00:10, 33.7MB/s] 28%|â–ˆâ–ˆâ–Š       | 136M/481M [00:04<00:09, 34.7MB/s] 29%|â–ˆâ–ˆâ–‰       | 140M/481M [00:04<00:10, 32.6MB/s] 30%|â–ˆâ–ˆâ–‰       | 144M/481M [00:04<00:09, 34.7MB/s] 31%|â–ˆâ–ˆâ–ˆ       | 147M/481M [00:04<00:09, 34.2MB/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 151M/481M [00:05<00:09, 34.0MB/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 154M/481M [00:05<00:09, 33.1MB/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 157M/481M [00:05<00:09, 33.4MB/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 161M/481M [00:05<00:09, 34.4MB/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 165M/481M [00:05<00:09, 34.1MB/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 168M/481M [00:05<00:09, 34.2MB/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 172M/481M [00:05<00:09, 33.1MB/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 175M/481M [00:05<00:09, 33.2MB/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 179M/481M [00:05<00:09, 33.4MB/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 183M/481M [00:06<00:08, 33.8MB/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 186M/481M [00:06<00:08, 32.9MB/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 190M/481M [00:06<00:08, 35.1MB/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 194M/481M [00:06<00:08, 33.6MB/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 197M/481M [00:06<00:08, 33.2MB/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200M/481M [00:06<00:08, 32.3MB/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 205M/481M [00:06<00:07, 35.0MB/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 208M/481M [00:06<00:07, 34.2MB/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212M/481M [00:06<00:07, 33.8MB/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 215M/481M [00:07<00:08, 32.9MB/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 219M/481M [00:07<00:07, 34.6MB/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 223M/481M [00:07<00:07, 33.7MB/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 226M/481M [00:07<00:07, 33.4MB/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 230M/481M [00:07<00:07, 32.4MB/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 233M/481M [00:07<00:07, 34.1MB/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 237M/481M [00:07<00:07, 33.6MB/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 241M/481M [00:07<00:07, 33.7MB/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244M/481M [00:07<00:07, 32.8MB/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248M/481M [00:07<00:06, 33.6MB/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251M/481M [00:08<00:06, 33.3MB/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 255M/481M [00:08<00:06, 33.1MB/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 258M/481M [00:08<00:06, 32.5MB/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262M/481M [00:08<00:06, 33.3MB/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 266M/481M [00:08<00:06, 33.6MB/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 269M/481M [00:08<00:06, 33.7MB/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 273M/481M [00:08<00:06, 32.9MB/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 276M/481M [00:08<00:06, 33.6MB/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 280M/481M [00:08<00:05, 33.7MB/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 284M/481M [00:09<00:05, 33.7MB/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 287M/481M [00:09<00:05, 33.0MB/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 291M/481M [00:09<00:05, 34.5MB/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 295M/481M [00:09<00:05, 33.8MB/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298M/481M [00:09<00:05, 33.8MB/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 302M/481M [00:09<00:05, 32.9MB/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 306M/481M [00:09<00:05, 35.1MB/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309M/481M [00:09<00:05, 33.3MB/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 313M/481M [00:09<00:05, 33.6MB/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 316M/481M [00:10<00:05, 32.8MB/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 320M/481M [00:10<00:04, 34.0MB/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 324M/481M [00:10<00:04, 33.1MB/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 327M/481M [00:10<00:04, 33.5MB/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 331M/481M [00:10<00:04, 32.9MB/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 334M/481M [00:10<00:04, 34.2MB/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 338M/481M [00:10<00:04, 35.2MB/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 342M/481M [00:10<00:04, 34.3MB/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 345M/481M [00:10<00:04, 33.3MB/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 349M/481M [00:10<00:04, 33.1MB/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 352M/481M [00:11<00:03, 34.2MB/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 356M/481M [00:11<00:03, 34.2MB/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 360M/481M [00:11<00:03, 32.9MB/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 363M/481M [00:11<00:03, 34.3MB/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 367M/481M [00:11<00:03, 34.1MB/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 370M/481M [00:11<00:03, 34.4MB/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 374M/481M [00:11<00:03, 34.1MB/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 377M/481M [00:11<00:03, 33.9MB/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 381M/481M [00:11<00:03, 33.5MB/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 384M/481M [00:12<00:02, 33.5MB/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 388M/481M [00:12<00:02, 34.4MB/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 391M/481M [00:12<00:02, 34.0MB/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 395M/481M [00:12<00:02, 33.7MB/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 398M/481M [00:12<00:02, 33.3MB/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 402M/481M [00:12<00:02, 33.9MB/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 405M/481M [00:12<00:02, 34.1MB/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 409M/481M [00:12<00:02, 34.0MB/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 412M/481M [00:12<00:02, 33.9MB/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 416M/481M [00:12<00:01, 34.2MB/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 419M/481M [00:13<00:01, 34.3MB/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 423M/481M [00:13<00:01, 33.9MB/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 426M/481M [00:13<00:01, 33.8MB/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 430M/481M [00:13<00:01, 34.6MB/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 433M/481M [00:13<00:01, 34.0MB/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 437M/481M [00:13<00:01, 34.4MB/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 441M/481M [00:13<00:01, 33.5MB/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 444M/481M [00:13<00:01, 34.4MB/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 448M/481M [00:13<00:00, 33.6MB/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452M/481M [00:14<00:00, 34.7MB/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 455M/481M [00:14<00:00, 33.3MB/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 459M/481M [00:14<00:00, 34.2MB/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 463M/481M [00:14<00:00, 34.3MB/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 466M/481M [00:14<00:00, 34.3MB/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 470M/481M [00:14<00:00, 34.6MB/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 473M/481M [00:14<00:00, 33.5MB/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 477M/481M [00:14<00:00, 33.8MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 480M/481M [00:14<00:00, 33.9MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481M/481M [00:14<00:00, 32.3MB/s]
textattack: Unzipping file /root/.cache/textattack/tmp52lisdz2.zip to /root/.cache/textattack/word_embeddings/paragramcf.
textattack: Successfully saved word_embeddings/paragramcf to cache.
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 21.1MB/s]                   2024-03-10 18:27:18.553032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.561730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.564165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.579615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.582006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.584376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.769713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.771345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.772811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:27:18.774291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0

Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.3k/35.3k [00:00<00:00, 488kB/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/584k [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 584k/584k [00:00<00:00, 2.04MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 584k/584k [00:00<00:00, 2.03MB/s]
Downloading data files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  3.44it/s]
Downloading data:   0%|          | 0.00/69.0k [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69.0k/69.0k [00:00<00:00, 292kB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69.0k/69.0k [00:00<00:00, 292kB/s]
Downloading data files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  3.84it/s]
Downloading data:   0%|          | 0.00/621k [00:00<?, ?B/s][A
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621k/621k [00:00<00:00, 2.64MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 621k/621k [00:00<00:00, 2.63MB/s]
Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.99it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.90it/s]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2400.40it/s]
Generating train split:   0%|          | 0/2490 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2490/2490 [00:00<00:00, 313345.84 examples/s]
Generating validation split:   0%|          | 0/277 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 158243.29 examples/s]
Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00<00:00, 598587.70 examples/s]
Map:   0%|          | 0/2490 [00:00<?, ? examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1541/2490 [00:00<00:00, 15296.92 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2490/2490 [00:00<00:00, 15340.26 examples/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13503.91 examples/s]
Map:   0%|          | 0/3000 [00:00<?, ? examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1588/3000 [00:00<00:00, 15785.36 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00<00:00, 15543.19 examples/s]
Map:   0%|          | 0/2365 [00:00<?, ? examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1007/2365 [00:00<00:00, 10008.97 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2019/2365 [00:00<00:00, 10062.97 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2365/2365 [00:00<00:00, 10013.91 examples/s]
Map:   0%|          | 0/125 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 8164.83 examples/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13401.26 examples/s]
Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 776/776 [00:00<00:00, 215kB/s]
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500k/500k [00:00<00:00, 138MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:00<00:00, 4.95MB/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:00<00:00, 4.94MB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:00<00:00, 569kB/s]
Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 609/609 [00:00<00:00, 190kB/s]
Downloading (â€¦)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading (â€¦)fetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.8k/26.8k [00:00<00:00, 24.4MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
Downloading (â€¦)of-00002.safetensors:   0%|          | 21.0M/9.98G [00:00<01:02, 159MB/s][A
Downloading (â€¦)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:54, 181MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 73.4M/9.98G [00:00<00:49, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 105M/9.98G [00:00<00:47, 207MB/s] [A
Downloading (â€¦)of-00002.safetensors:   1%|â–         | 136M/9.98G [00:00<00:46, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 168M/9.98G [00:00<00:45, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 199M/9.98G [00:00<00:45, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 231M/9.98G [00:01<00:45, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 262M/9.98G [00:01<00:45, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 294M/9.98G [00:01<00:44, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 325M/9.98G [00:01<00:44, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–Ž         | 357M/9.98G [00:01<00:44, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 388M/9.98G [00:01<00:44, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 419M/9.98G [00:01<00:44, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–         | 451M/9.98G [00:02<00:43, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–         | 482M/9.98G [00:02<00:43, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 514M/9.98G [00:02<00:43, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 545M/9.98G [00:02<00:43, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 577M/9.98G [00:02<00:42, 219MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 608M/9.98G [00:02<00:46, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–‹         | 640M/9.98G [00:03<00:44, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 671M/9.98G [00:03<00:45, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 703M/9.98G [00:03<00:44, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 734M/9.98G [00:03<00:43, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 765M/9.98G [00:03<00:43, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 797M/9.98G [00:03<00:42, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 828M/9.98G [00:03<00:42, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–Š         | 860M/9.98G [00:04<00:42, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 891M/9.98G [00:04<00:41, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 923M/9.98G [00:04<00:41, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 954M/9.98G [00:04<00:45, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 986M/9.98G [00:04<00:41, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–ˆ         | 1.02G/9.98G [00:04<00:40, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.05G/9.98G [00:04<00:38, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.08G/9.98G [00:05<00:37, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.11G/9.98G [00:05<00:38, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆâ–        | 1.14G/9.98G [00:05<00:41, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.17G/9.98G [00:05<00:39, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.21G/9.98G [00:05<00:37, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.24G/9.98G [00:05<00:36, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.27G/9.98G [00:05<00:35, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.30G/9.98G [00:05<00:35, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.33G/9.98G [00:06<00:35, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–Ž        | 1.36G/9.98G [00:06<00:34, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 1.39G/9.98G [00:06<00:34, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 1.43G/9.98G [00:06<00:34, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 1.46G/9.98G [00:06<00:34, 247MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 1.49G/9.98G [00:06<00:33, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–Œ        | 1.52G/9.98G [00:06<00:33, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.55G/9.98G [00:06<00:34, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.58G/9.98G [00:07<00:34, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.61G/9.98G [00:07<00:34, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.65G/9.98G [00:07<00:34, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.68G/9.98G [00:07<00:39, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.71G/9.98G [00:07<00:39, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.74G/9.98G [00:07<00:39, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.77G/9.98G [00:08<00:38, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.80G/9.98G [00:08<00:43, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.82G/9.98G [00:08<00:43, 187MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.85G/9.98G [00:08<00:47, 171MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–Š        | 1.87G/9.98G [00:08<00:46, 175MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.90G/9.98G [00:08<00:43, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.92G/9.98G [00:08<00:44, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.94G/9.98G [00:09<00:48, 165MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 1.96G/9.98G [00:09<00:45, 175MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 1.99G/9.98G [00:09<00:43, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 2.01G/9.98G [00:09<00:44, 180MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 2.04G/9.98G [00:09<00:39, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.08G/9.98G [00:09<00:36, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.11G/9.98G [00:09<00:34, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆâ–       | 2.14G/9.98G [00:09<00:35, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.17G/9.98G [00:10<00:37, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.20G/9.98G [00:10<00:37, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.23G/9.98G [00:10<00:37, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.26G/9.98G [00:10<00:37, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.29G/9.98G [00:10<00:39, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.31G/9.98G [00:10<00:39, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.33G/9.98G [00:10<00:38, 197MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–Ž       | 2.35G/9.98G [00:11<00:41, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.38G/9.98G [00:11<00:41, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.40G/9.98G [00:11<00:50, 149MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.43G/9.98G [00:11<00:47, 159MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 2.45G/9.98G [00:11<00:45, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 2.49G/9.98G [00:11<00:40, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 2.51G/9.98G [00:11<00:42, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 2.53G/9.98G [00:12<00:41, 179MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.55G/9.98G [00:12<00:40, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.57G/9.98G [00:12<00:40, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.59G/9.98G [00:12<00:42, 173MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 2.62G/9.98G [00:12<00:40, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 2.64G/9.98G [00:12<00:40, 182MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.66G/9.98G [00:12<00:39, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.68G/9.98G [00:12<00:39, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.72G/9.98G [00:13<00:35, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.74G/9.98G [00:13<00:35, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.76G/9.98G [00:13<00:36, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.79G/9.98G [00:13<00:34, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.81G/9.98G [00:13<00:36, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.84G/9.98G [00:13<00:33, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.87G/9.98G [00:13<00:31, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.90G/9.98G [00:13<00:31, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.94G/9.98G [00:14<00:30, 230MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 2.97G/9.98G [00:14<00:30, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 3.00G/9.98G [00:14<00:30, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 3.03G/9.98G [00:14<00:30, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.06G/9.98G [00:14<00:29, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.09G/9.98G [00:14<00:29, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 3.12G/9.98G [00:14<00:28, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.16G/9.98G [00:15<00:27, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.19G/9.98G [00:15<00:27, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.22G/9.98G [00:15<00:27, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.25G/9.98G [00:15<00:27, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.28G/9.98G [00:15<00:27, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.31G/9.98G [00:15<00:27, 247MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.34G/9.98G [00:15<00:26, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.38G/9.98G [00:15<00:26, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.41G/9.98G [00:16<00:26, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.44G/9.98G [00:16<00:32, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 3.47G/9.98G [00:16<00:32, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.50G/9.98G [00:16<00:32, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.53G/9.98G [00:16<00:30, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.57G/9.98G [00:16<00:29, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.60G/9.98G [00:16<00:28, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 3.63G/9.98G [00:17<00:27, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.66G/9.98G [00:17<00:26, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.69G/9.98G [00:17<00:26, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.72G/9.98G [00:17<00:25, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.75G/9.98G [00:17<00:25, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.79G/9.98G [00:17<00:39, 158MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.82G/9.98G [00:18<00:34, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 3.85G/9.98G [00:18<00:31, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.88G/9.98G [00:18<00:29, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.91G/9.98G [00:18<00:27, 217MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.94G/9.98G [00:18<00:26, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.97G/9.98G [00:18<00:25, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.01G/9.98G [00:18<00:25, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.04G/9.98G [00:18<00:25, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.07G/9.98G [00:19<00:24, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.10G/9.98G [00:19<00:25, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.13G/9.98G [00:19<00:27, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.16G/9.98G [00:19<00:29, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.18G/9.98G [00:19<00:30, 188MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.22G/9.98G [00:19<00:28, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.25G/9.98G [00:20<00:26, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.28G/9.98G [00:20<00:25, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.31G/9.98G [00:20<00:24, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.34G/9.98G [00:20<00:23, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.37G/9.98G [00:20<00:23, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.40G/9.98G [00:20<00:22, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.44G/9.98G [00:20<00:22, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.47G/9.98G [00:20<00:22, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.50G/9.98G [00:21<00:22, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.53G/9.98G [00:21<00:21, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.56G/9.98G [00:21<00:21, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.59G/9.98G [00:21<00:21, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.62G/9.98G [00:21<00:21, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.66G/9.98G [00:21<00:21, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.69G/9.98G [00:21<00:21, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.72G/9.98G [00:21<00:21, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.75G/9.98G [00:22<00:21, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.78G/9.98G [00:22<00:21, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.81G/9.98G [00:22<00:21, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.84G/9.98G [00:22<00:24, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.88G/9.98G [00:22<00:25, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.91G/9.98G [00:22<00:24, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.94G/9.98G [00:22<00:23, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.97G/9.98G [00:23<00:23, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.00G/9.98G [00:23<00:22, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.03G/9.98G [00:23<00:23, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.06G/9.98G [00:23<00:23, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.10G/9.98G [00:23<00:22, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.13G/9.98G [00:23<00:21, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.16G/9.98G [00:23<00:20, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.19G/9.98G [00:24<00:22, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.22G/9.98G [00:24<00:21, 223MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.25G/9.98G [00:24<00:21, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.28G/9.98G [00:24<00:20, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.32G/9.98G [00:24<00:19, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.35G/9.98G [00:24<00:19, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.38G/9.98G [00:24<00:19, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.41G/9.98G [00:25<00:23, 191MB/s][A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.43G/9.98G [00:25<00:23, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.45G/9.98G [00:25<00:25, 179MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.48G/9.98G [00:25<00:23, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.52G/9.98G [00:25<00:21, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.55G/9.98G [00:25<00:21, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.58G/9.98G [00:25<00:20, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.61G/9.98G [00:26<00:20, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.64G/9.98G [00:26<00:19, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.67G/9.98G [00:26<00:18, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.70G/9.98G [00:26<00:18, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.74G/9.98G [00:26<00:20, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.77G/9.98G [00:26<00:19, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.80G/9.98G [00:26<00:18, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.83G/9.98G [00:27<00:17, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.86G/9.98G [00:27<00:17, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.89G/9.98G [00:27<00:16, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.92G/9.98G [00:27<00:16, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.96G/9.98G [00:27<00:17, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5.99G/9.98G [00:27<00:17, 230MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.02G/9.98G [00:27<00:16, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.05G/9.98G [00:27<00:16, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.08G/9.98G [00:28<00:16, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.11G/9.98G [00:28<00:15, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.14G/9.98G [00:28<00:15, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.18G/9.98G [00:28<00:15, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.21G/9.98G [00:28<00:15, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.24G/9.98G [00:28<00:15, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.27G/9.98G [00:28<00:15, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.30G/9.98G [00:28<00:14, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.33G/9.98G [00:29<00:14, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.36G/9.98G [00:29<00:14, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.40G/9.98G [00:29<00:14, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.43G/9.98G [00:29<00:15, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.46G/9.98G [00:29<00:14, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.49G/9.98G [00:29<00:14, 240MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.52G/9.98G [00:29<00:14, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.55G/9.98G [00:29<00:13, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.59G/9.98G [00:30<00:13, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.62G/9.98G [00:30<00:13, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.65G/9.98G [00:30<00:13, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.68G/9.98G [00:30<00:13, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.71G/9.98G [00:30<00:13, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.74G/9.98G [00:30<00:13, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.77G/9.98G [00:30<00:12, 250MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.81G/9.98G [00:31<00:12, 250MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.84G/9.98G [00:31<00:12, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.87G/9.98G [00:31<00:13, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.90G/9.98G [00:31<00:13, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.93G/9.98G [00:31<00:13, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.96G/9.98G [00:31<00:12, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.99G/9.98G [00:31<00:12, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.03G/9.98G [00:31<00:12, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.06G/9.98G [00:32<00:12, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.09G/9.98G [00:32<00:11, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.12G/9.98G [00:32<00:11, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.15G/9.98G [00:32<00:11, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.18G/9.98G [00:32<00:11, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.21G/9.98G [00:32<00:16, 164MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.25G/9.98G [00:33<00:14, 184MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.28G/9.98G [00:33<00:13, 194MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.31G/9.98G [00:33<00:12, 208MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.34G/9.98G [00:33<00:12, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.37G/9.98G [00:33<00:11, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.40G/9.98G [00:33<00:11, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.43G/9.98G [00:33<00:11, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.47G/9.98G [00:33<00:10, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.50G/9.98G [00:34<00:10, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.53G/9.98G [00:34<00:10, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.56G/9.98G [00:34<00:10, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.59G/9.98G [00:34<00:10, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.62G/9.98G [00:34<00:10, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.65G/9.98G [00:34<00:10, 227MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.69G/9.98G [00:34<00:10, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.72G/9.98G [00:35<00:10, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.75G/9.98G [00:35<00:09, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.78G/9.98G [00:35<00:09, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.81G/9.98G [00:35<00:09, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.84G/9.98G [00:35<00:08, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.87G/9.98G [00:35<00:08, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.91G/9.98G [00:35<00:08, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.94G/9.98G [00:35<00:08, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.97G/9.98G [00:36<00:08, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.00G/9.98G [00:36<00:08, 236MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.03G/9.98G [00:36<00:08, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.06G/9.98G [00:36<00:08, 218MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.10G/9.98G [00:36<00:08, 228MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.13G/9.98G [00:36<00:07, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.16G/9.98G [00:36<00:07, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.19G/9.98G [00:37<00:07, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.22G/9.98G [00:37<00:07, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.25G/9.98G [00:37<00:06, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.28G/9.98G [00:37<00:06, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.32G/9.98G [00:37<00:06, 254MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.35G/9.98G [00:37<00:06, 253MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.38G/9.98G [00:37<00:06, 254MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.41G/9.98G [00:37<00:06, 254MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.44G/9.98G [00:38<00:06, 254MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.47G/9.98G [00:38<00:06, 229MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.50G/9.98G [00:38<00:06, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.54G/9.98G [00:38<00:06, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.57G/9.98G [00:38<00:06, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.60G/9.98G [00:38<00:05, 230MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.63G/9.98G [00:38<00:06, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.66G/9.98G [00:39<00:06, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.69G/9.98G [00:39<00:05, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.72G/9.98G [00:39<00:05, 235MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.76G/9.98G [00:39<00:05, 211MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.79G/9.98G [00:39<00:05, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.82G/9.98G [00:39<00:05, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.85G/9.98G [00:39<00:05, 206MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.88G/9.98G [00:40<00:05, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.91G/9.98G [00:40<00:04, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.94G/9.98G [00:40<00:04, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.98G/9.98G [00:40<00:04, 234MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.01G/9.98G [00:40<00:04, 240MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.04G/9.98G [00:40<00:03, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.07G/9.98G [00:40<00:03, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.10G/9.98G [00:40<00:03, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.13G/9.98G [00:41<00:03, 251MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.16G/9.98G [00:41<00:03, 252MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.20G/9.98G [00:41<00:03, 254MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.23G/9.98G [00:41<00:02, 256MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.26G/9.98G [00:41<00:02, 257MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.29G/9.98G [00:41<00:02, 258MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.32G/9.98G [00:41<00:02, 258MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.35G/9.98G [00:41<00:02, 259MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.38G/9.98G [00:42<00:02, 258MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.42G/9.98G [00:42<00:02, 256MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.45G/9.98G [00:42<00:02, 239MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.48G/9.98G [00:42<00:02, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.51G/9.98G [00:42<00:01, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.54G/9.98G [00:42<00:02, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.57G/9.98G [00:42<00:01, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.60G/9.98G [00:43<00:01, 220MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.64G/9.98G [00:43<00:01, 231MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.67G/9.98G [00:43<00:01, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.70G/9.98G [00:43<00:01, 244MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.73G/9.98G [00:43<00:00, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.76G/9.98G [00:43<00:00, 252MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.79G/9.98G [00:43<00:00, 222MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.83G/9.98G [00:43<00:00, 221MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.86G/9.98G [00:44<00:00, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.89G/9.98G [00:44<00:00, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.92G/9.98G [00:44<00:00, 202MB/s][A
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.94G/9.98G [00:44<00:00, 200MB/s][A
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.97G/9.98G [00:44<00:00, 206MB/s][ADownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.98G/9.98G [00:44<00:00, 223MB/s]
Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:44<00:44, 44.87s/it]
Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
Downloading (â€¦)of-00002.safetensors:   0%|          | 10.5M/3.50G [00:00<00:44, 78.0MB/s][A
Downloading (â€¦)of-00002.safetensors:   1%|          | 31.5M/3.50G [00:00<00:27, 127MB/s] [A
Downloading (â€¦)of-00002.safetensors:   2%|â–         | 62.9M/3.50G [00:00<00:19, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 94.4M/3.50G [00:00<00:16, 205MB/s][A
Downloading (â€¦)of-00002.safetensors:   4%|â–Ž         | 126M/3.50G [00:00<00:15, 218MB/s] [A
Downloading (â€¦)of-00002.safetensors:   4%|â–         | 157M/3.50G [00:00<00:14, 230MB/s][A
Downloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 189M/3.50G [00:00<00:13, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:   6%|â–‹         | 220M/3.50G [00:01<00:13, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:   7%|â–‹         | 252M/3.50G [00:01<00:19, 163MB/s][A
Downloading (â€¦)of-00002.safetensors:   8%|â–Š         | 283M/3.50G [00:01<00:17, 183MB/s][A
Downloading (â€¦)of-00002.safetensors:   9%|â–‰         | 315M/3.50G [00:01<00:16, 199MB/s][A
Downloading (â€¦)of-00002.safetensors:  10%|â–‰         | 346M/3.50G [00:01<00:14, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 377M/3.50G [00:01<00:14, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 409M/3.50G [00:01<00:13, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 440M/3.50G [00:02<00:13, 232MB/s][A
Downloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 472M/3.50G [00:02<00:12, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 503M/3.50G [00:02<00:12, 241MB/s][A
Downloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–Œ        | 535M/3.50G [00:02<00:13, 214MB/s][A
Downloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 566M/3.50G [00:02<00:13, 225MB/s][A
Downloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 598M/3.50G [00:02<00:12, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 629M/3.50G [00:02<00:12, 238MB/s][A
Downloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 661M/3.50G [00:03<00:11, 242MB/s][A
Downloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 692M/3.50G [00:03<00:11, 245MB/s][A
Downloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 724M/3.50G [00:03<00:11, 246MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 755M/3.50G [00:03<00:11, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 786M/3.50G [00:03<00:10, 247MB/s][A
Downloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 818M/3.50G [00:03<00:10, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 849M/3.50G [00:03<00:10, 248MB/s][A
Downloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 881M/3.50G [00:03<00:10, 249MB/s][A
Downloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 912M/3.50G [00:04<00:12, 213MB/s][A
Downloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 944M/3.50G [00:05<00:36, 69.5MB/s][A
Downloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 975M/3.50G [00:05<00:28, 88.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 1.01G/3.50G [00:05<00:22, 110MB/s][A
Downloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 1.04G/3.50G [00:05<00:18, 132MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.07G/3.50G [00:05<00:15, 153MB/s][A
Downloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 1.10G/3.50G [00:05<00:13, 173MB/s][A
Downloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.13G/3.50G [00:06<00:12, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.16G/3.50G [00:06<00:11, 204MB/s][A
Downloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.20G/3.50G [00:06<00:10, 215MB/s][A
Downloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.23G/3.50G [00:06<00:15, 151MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.25G/3.50G [00:07<00:32, 68.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.27G/3.50G [00:07<00:29, 75.9MB/s][A
Downloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.29G/3.50G [00:08<00:31, 70.5MB/s][A
Downloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.32G/3.50G [00:08<00:23, 94.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1.35G/3.50G [00:08<00:18, 119MB/s] [A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.38G/3.50G [00:08<00:14, 143MB/s][A
Downloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.42G/3.50G [00:08<00:12, 166MB/s][A
Downloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.45G/3.50G [00:08<00:11, 186MB/s][A
Downloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.48G/3.50G [00:08<00:09, 202MB/s][A
Downloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.51G/3.50G [00:09<00:22, 88.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.53G/3.50G [00:09<00:24, 82.0MB/s][A
Downloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.56G/3.50G [00:10<00:18, 105MB/s] [A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.59G/3.50G [00:10<00:14, 131MB/s][A
Downloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.63G/3.50G [00:10<00:13, 141MB/s][A
Downloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.65G/3.50G [00:10<00:13, 143MB/s][A
Downloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.68G/3.50G [00:10<00:10, 168MB/s][A
Downloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.71G/3.50G [00:10<00:09, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.74G/3.50G [00:10<00:08, 203MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.77G/3.50G [00:11<00:21, 80.1MB/s][A
Downloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.79G/3.50G [00:12<00:19, 85.7MB/s][A
Downloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.81G/3.50G [00:12<00:18, 91.2MB/s][A
Downloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1.85G/3.50G [00:12<00:13, 119MB/s] [A
Downloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1.88G/3.50G [00:12<00:10, 148MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.91G/3.50G [00:12<00:09, 177MB/s][A
Downloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.94G/3.50G [00:12<00:07, 198MB/s][A
Downloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.97G/3.50G [00:12<00:07, 216MB/s][A
Downloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.00G/3.50G [00:12<00:06, 230MB/s][A
Downloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.03G/3.50G [00:13<00:06, 243MB/s][A
Downloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.07G/3.50G [00:13<00:06, 237MB/s][A
Downloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.10G/3.50G [00:13<00:12, 116MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.12G/3.50G [00:14<00:17, 76.8MB/s][A
Downloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.14G/3.50G [00:14<00:18, 72.0MB/s][A
Downloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.17G/3.50G [00:14<00:13, 96.0MB/s][A
Downloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.20G/3.50G [00:14<00:10, 122MB/s] [A
Downloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.23G/3.50G [00:15<00:08, 147MB/s][A
Downloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.26G/3.50G [00:15<00:07, 172MB/s][A
Downloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.30G/3.50G [00:15<00:06, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.33G/3.50G [00:15<00:05, 200MB/s][A
Downloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.36G/3.50G [00:15<00:05, 209MB/s][A
Downloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.39G/3.50G [00:15<00:04, 226MB/s][A
Downloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.42G/3.50G [00:16<00:10, 100MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.44G/3.50G [00:16<00:12, 81.4MB/s][A
Downloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.46G/3.50G [00:17<00:12, 83.2MB/s][A
Downloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.50G/3.50G [00:17<00:09, 108MB/s] [A
Downloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.53G/3.50G [00:17<00:07, 133MB/s][A
Downloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2.56G/3.50G [00:17<00:05, 157MB/s][A
Downloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.59G/3.50G [00:17<00:05, 178MB/s][A
Downloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.62G/3.50G [00:17<00:04, 196MB/s][A
Downloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.65G/3.50G [00:17<00:03, 212MB/s][A
Downloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.68G/3.50G [00:17<00:03, 224MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.72G/3.50G [00:18<00:03, 233MB/s][A
Downloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.75G/3.50G [00:19<00:10, 74.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.77G/3.50G [00:19<00:09, 76.1MB/s][A
Downloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.80G/3.50G [00:19<00:07, 98.3MB/s][A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.82G/3.50G [00:19<00:06, 110MB/s] [A
Downloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.85G/3.50G [00:19<00:04, 133MB/s][A
Downloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.88G/3.50G [00:19<00:04, 152MB/s][A
Downloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2.92G/3.50G [00:20<00:03, 170MB/s][A
Downloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.95G/3.50G [00:20<00:02, 190MB/s][A
Downloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.98G/3.50G [00:20<00:02, 201MB/s][A
Downloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3.01G/3.50G [00:21<00:05, 88.4MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.03G/3.50G [00:21<00:05, 83.5MB/s][A
Downloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.06G/3.50G [00:21<00:04, 107MB/s] [A
Downloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.09G/3.50G [00:21<00:03, 131MB/s][A
Downloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.12G/3.50G [00:21<00:02, 155MB/s][A
Downloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.16G/3.50G [00:21<00:01, 176MB/s][A
Downloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.19G/3.50G [00:22<00:01, 195MB/s][A
Downloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.22G/3.50G [00:22<00:01, 210MB/s][A
Downloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.25G/3.50G [00:22<00:01, 223MB/s][A
Downloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.28G/3.50G [00:22<00:01, 189MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.31G/3.50G [00:22<00:01, 116MB/s][A
Downloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.33G/3.50G [00:23<00:01, 101MB/s][A
Downloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.37G/3.50G [00:23<00:01, 124MB/s][A
Downloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.39G/3.50G [00:23<00:00, 137MB/s][A
Downloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.42G/3.50G [00:23<00:00, 162MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.45G/3.50G [00:23<00:00, 185MB/s][A
Downloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.48G/3.50G [00:23<00:00, 203MB/s][ADownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50G/3.50G [00:23<00:00, 146MB/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:08<00:00, 32.65s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:08<00:00, 34.48s/it]
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:26<00:26, 26.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 16.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 18.25s/it]
Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]Downloading generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188/188 [00:00<00:00, 41.0kB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 38.8kB/s]
Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 537kB/s]
Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 1.52MB/s]Downloading vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 1.52MB/s]
Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 98.8MB/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 6280.53 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-8_bm25/icl_attack_log.csv
  0%|          | 0/277 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-03-10 18:31:39.442201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string
	 [[{{node inputs}}]]
2024-03-10 18:31:42.401456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
  0%|          | 1/277 [01:00<4:37:14, 60.27s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   0%|          | 1/277 [01:01<4:40:38, 61.01s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   1%|          | 2/277 [01:01<2:20:14, 30.60s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 1 / 2:   1%|          | 2/277 [01:01<2:20:31, 30.66s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 1 / 2:   1%|          | 3/277 [01:33<2:22:08, 31.13s/it][Succeeded / Failed / Skipped / Total] 2 / 0 / 1 / 3:   1%|          | 3/277 [01:33<2:22:21, 31.17s/it]wandb: - 0.225 MB of 0.225 MB uploadedwandb: \ 0.225 MB of 0.225 MB uploadedwandb: | 0.233 MB of 0.245 MB uploaded (0.005 MB deduped)wandb: / 0.245 MB of 0.245 MB uploaded (0.005 MB deduped)Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 228, in forward
    outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, output_hidden_states = True, output_attentions = True)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 1041, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 928, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 654, in forward
    hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 245, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 29.94 MiB is free. Process 3162708 has 39.35 GiB memory in use. Of the allocated memory 35.25 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-03-10 18:32:39.533879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 18:32:40.279940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 18:32:46.930313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:46.939585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:46.941986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:46.955716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:46.958102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:46.960472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:47.147987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:47.149571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:47.151028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:32:47.152488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:35<00:00, 16.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:35<00:00, 17.91s/it]
Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]Downloading .gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.18k/1.18k [00:00<00:00, 267kB/s]
Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]Downloading 1_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 75.8kB/s]
Downloading README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]Downloading README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.7k/10.7k [00:00<00:00, 17.5MB/s]
Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612/612 [00:00<00:00, 1.30MB/s]
Downloading (â€¦)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]Downloading (â€¦)ce_transformers.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 194kB/s]
Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]Downloading data_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39.3k/39.3k [00:00<00:00, 537kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]Downloading pytorch_model.bin:  12%|â–ˆâ–        | 10.5M/90.9M [00:00<00:01, 52.1MB/s]Downloading pytorch_model.bin:  23%|â–ˆâ–ˆâ–Ž       | 21.0M/90.9M [00:00<00:01, 55.1MB/s]Downloading pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 31.5M/90.9M [00:00<00:00, 64.0MB/s]Downloading pytorch_model.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 41.9M/90.9M [00:00<00:00, 67.0MB/s]Downloading pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 52.4M/90.9M [00:00<00:00, 67.7MB/s]Downloading pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 62.9M/90.9M [00:00<00:00, 69.7MB/s]Downloading pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 73.4M/90.9M [00:01<00:00, 70.5MB/s]Downloading pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 83.9M/90.9M [00:01<00:00, 73.2MB/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.9M/90.9M [00:01<00:00, 67.7MB/s]
Downloading (â€¦)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.0/53.0 [00:00<00:00, 15.5kB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 191kB/s]
Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 2.07MB/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 2.07MB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:00<00:00, 670kB/s]
Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]Downloading train_script.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.2k/13.2k [00:00<00:00, 29.0MB/s]
Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 36.9MB/s]
Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]Downloading modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 349/349 [00:00<00:00, 881kB/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13325.48 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
2024-03-10 18:35:16.944213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 18:35:17.666229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 18:35:24.105948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.114999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.117397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.131245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.133621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.135957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.320393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.321985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.323431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:35:24.324891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:26<00:26, 26.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:35<00:00, 16.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:35<00:00, 17.75s/it]
Downloading .gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]Downloading .gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.48k/1.48k [00:00<00:00, 279kB/s]
Downloading 1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]Downloading 1_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<00:00, 83.8kB/s]
Downloading 2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]Downloading 2_Dense/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 171kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.15M/3.15M [00:00<00:00, 54.2MB/s]
Downloading README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]Downloading README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.3k/66.3k [00:00<00:00, 32.4MB/s]
Downloading config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.53k/1.53k [00:00<00:00, 2.20MB/s]
Downloading (â€¦)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]Downloading (â€¦)ce_transformers.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [00:00<00:00, 221kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]Downloading pytorch_model.bin:   1%|          | 10.5M/1.34G [00:00<00:13, 100MB/s]Downloading pytorch_model.bin:   3%|â–Ž         | 41.9M/1.34G [00:00<00:06, 193MB/s]Downloading pytorch_model.bin:   5%|â–Œ         | 73.4M/1.34G [00:00<00:05, 230MB/s]Downloading pytorch_model.bin:   8%|â–Š         | 105M/1.34G [00:00<00:05, 218MB/s] Downloading pytorch_model.bin:  10%|â–ˆ         | 136M/1.34G [00:00<00:05, 223MB/s]Downloading pytorch_model.bin:  13%|â–ˆâ–Ž        | 168M/1.34G [00:00<00:05, 211MB/s]Downloading pytorch_model.bin:  15%|â–ˆâ–        | 199M/1.34G [00:00<00:05, 225MB/s]Downloading pytorch_model.bin:  17%|â–ˆâ–‹        | 231M/1.34G [00:01<00:04, 238MB/s]Downloading pytorch_model.bin:  20%|â–ˆâ–‰        | 262M/1.34G [00:01<00:04, 232MB/s]Downloading pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 294M/1.34G [00:01<00:04, 227MB/s]Downloading pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 325M/1.34G [00:01<00:04, 211MB/s]Downloading pytorch_model.bin:  27%|â–ˆâ–ˆâ–‹       | 357M/1.34G [00:01<00:04, 211MB/s]Downloading pytorch_model.bin:  29%|â–ˆâ–ˆâ–‰       | 388M/1.34G [00:01<00:04, 217MB/s]Downloading pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆâ–      | 419M/1.34G [00:01<00:04, 212MB/s]Downloading pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 451M/1.34G [00:02<00:04, 213MB/s]Downloading pytorch_model.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 482M/1.34G [00:02<00:03, 215MB/s]Downloading pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 514M/1.34G [00:02<00:03, 213MB/s]Downloading pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 545M/1.34G [00:02<00:03, 219MB/s]Downloading pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 577M/1.34G [00:02<00:03, 226MB/s]Downloading pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 608M/1.34G [00:02<00:03, 233MB/s]Downloading pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 640M/1.34G [00:02<00:02, 238MB/s]Downloading pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 671M/1.34G [00:03<00:02, 225MB/s]Downloading pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 703M/1.34G [00:03<00:02, 219MB/s]Downloading pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 734M/1.34G [00:03<00:02, 222MB/s]Downloading pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 765M/1.34G [00:03<00:02, 228MB/s]Downloading pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 797M/1.34G [00:03<00:02, 223MB/s]Downloading pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 828M/1.34G [00:03<00:02, 221MB/s]Downloading pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 860M/1.34G [00:03<00:02, 216MB/s]Downloading pytorch_model.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 891M/1.34G [00:04<00:02, 204MB/s]Downloading pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 923M/1.34G [00:04<00:01, 212MB/s]Downloading pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 954M/1.34G [00:04<00:01, 220MB/s]Downloading pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 986M/1.34G [00:04<00:01, 210MB/s]Downloading pytorch_model.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1.02G/1.34G [00:04<00:01, 196MB/s]Downloading pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1.04G/1.34G [00:04<00:01, 194MB/s]Downloading pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1.06G/1.34G [00:04<00:01, 196MB/s]Downloading pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1.08G/1.34G [00:05<00:01, 194MB/s]Downloading pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1.11G/1.34G [00:05<00:01, 214MB/s]Downloading pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.14G/1.34G [00:05<00:00, 229MB/s]Downloading pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.17G/1.34G [00:05<00:00, 227MB/s]Downloading pytorch_model.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.21G/1.34G [00:05<00:00, 238MB/s]Downloading pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.24G/1.34G [00:05<00:00, 215MB/s]Downloading pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.27G/1.34G [00:05<00:00, 223MB/s]Downloading pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.30G/1.34G [00:05<00:00, 226MB/s]Downloading pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.33G/1.34G [00:06<00:00, 220MB/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [00:06<00:00, 217MB/s]
Downloading (â€¦)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.0/53.0 [00:00<00:00, 15.6kB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.20k/2.20k [00:00<00:00, 3.47MB/s]
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:00<00:00, 121MB/s]
Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.42M/2.42M [00:00<00:00, 10.6MB/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.42M/2.42M [00:00<00:00, 10.6MB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.41k/2.41k [00:00<00:00, 3.46MB/s]
Downloading modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]Downloading modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 728kB/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13533.64 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
1+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-2
2024-03-10 18:38:15.933264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 18:38:16.691602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 18:38:23.356384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.365155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.367597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.381921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.384369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.386733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.576972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.578606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.580076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 18:38:23.581549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 17.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.77s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 8907.84 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-2_bm25/icl_attack_log.csv
  0%|          | 0/277 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-03-10 18:40:48.005962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string
	 [[{{node inputs}}]]
2024-03-10 18:40:50.631399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
  0%|          | 1/277 [00:09<42:33,  9.25s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   0%|          | 1/277 [00:09<45:33,  9.91s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   1%|          | 2/277 [00:11<25:50,  5.64s/it][Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:   1%|          | 2/277 [00:11<25:54,  5.65s/it][Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:   1%|          | 3/277 [00:40<1:01:23, 13.44s/it][Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:   1%|          | 3/277 [00:40<1:01:28, 13.46s/it][Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:   1%|â–         | 4/277 [00:50<57:16, 12.59s/it]  [Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:   1%|â–         | 4/277 [00:50<57:20, 12.60s/it][Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:   2%|â–         | 5/277 [01:07<1:01:07, 13.48s/it][Succeeded / Failed / Skipped / Total] 3 / 2 / 0 / 5:   2%|â–         | 5/277 [01:07<1:01:09, 13.49s/it][Succeeded / Failed / Skipped / Total] 3 / 2 / 0 / 5:   2%|â–         | 6/277 [01:22<1:01:53, 13.70s/it][Succeeded / Failed / Skipped / Total] 3 / 3 / 0 / 6:   2%|â–         | 6/277 [01:22<1:01:55, 13.71s/it][Succeeded / Failed / Skipped / Total] 3 / 3 / 0 / 6:   3%|â–Ž         | 7/277 [01:42<1:05:54, 14.65s/it][Succeeded / Failed / Skipped / Total] 3 / 4 / 0 / 7:   3%|â–Ž         | 7/277 [01:42<1:05:55, 14.65s/it][Succeeded / Failed / Skipped / Total] 3 / 4 / 0 / 7:   3%|â–Ž         | 8/277 [01:52<1:03:04, 14.07s/it][Succeeded / Failed / Skipped / Total] 3 / 5 / 0 / 8:   3%|â–Ž         | 8/277 [01:52<1:03:05, 14.07s/it][Succeeded / Failed / Skipped / Total] 3 / 5 / 0 / 8:   3%|â–Ž         | 9/277 [02:10<1:05:00, 14.56s/it][Succeeded / Failed / Skipped / Total] 3 / 6 / 0 / 9:   3%|â–Ž         | 9/277 [02:11<1:05:02, 14.56s/it][Succeeded / Failed / Skipped / Total] 3 / 6 / 0 / 9:   4%|â–Ž         | 10/277 [02:11<58:20, 13.11s/it] [Succeeded / Failed / Skipped / Total] 3 / 6 / 1 / 10:   4%|â–Ž         | 10/277 [02:11<58:22, 13.12s/it][Succeeded / Failed / Skipped / Total] 3 / 6 / 1 / 10:   4%|â–         | 11/277 [02:24<58:24, 13.17s/it][Succeeded / Failed / Skipped / Total] 3 / 7 / 1 / 11:   4%|â–         | 11/277 [02:24<58:25, 13.18s/it][Succeeded / Failed / Skipped / Total] 3 / 7 / 1 / 11:   4%|â–         | 12/277 [02:33<56:37, 12.82s/it][Succeeded / Failed / Skipped / Total] 3 / 8 / 1 / 12:   4%|â–         | 12/277 [02:33<56:39, 12.83s/it][Succeeded / Failed / Skipped / Total] 3 / 8 / 1 / 12:   5%|â–         | 13/277 [02:51<58:11, 13.23s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 1 / 13:   5%|â–         | 13/277 [02:51<58:12, 13.23s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 1 / 13:   5%|â–Œ         | 14/277 [02:52<53:51, 12.29s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 2 / 14:   5%|â–Œ         | 14/277 [02:52<53:52, 12.29s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 3 / 15:   5%|â–Œ         | 15/277 [02:52<50:07, 11.48s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 3 / 15:   6%|â–Œ         | 16/277 [02:52<46:49, 10.76s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 4 / 16:   6%|â–Œ         | 16/277 [02:52<46:50, 10.77s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 5 / 17:   6%|â–Œ         | 17/277 [02:52<43:56, 10.14s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 5 / 17:   6%|â–Œ         | 17/277 [03:04<47:08, 10.88s/it][Succeeded / Failed / Skipped / Total] 3 / 9 / 5 / 17:   6%|â–‹         | 18/277 [03:12<46:07, 10.69s/it][Succeeded / Failed / Skipped / Total] 3 / 10 / 5 / 18:   6%|â–‹         | 18/277 [03:12<46:08, 10.69s/it][Succeeded / Failed / Skipped / Total] 3 / 10 / 5 / 18:   7%|â–‹         | 19/277 [03:32<48:02, 11.17s/it][Succeeded / Failed / Skipped / Total] 3 / 11 / 5 / 19:   7%|â–‹         | 19/277 [03:32<48:02, 11.17s/it][Succeeded / Failed / Skipped / Total] 3 / 11 / 5 / 19:   7%|â–‹         | 20/277 [04:01<51:40, 12.07s/it][Succeeded / Failed / Skipped / Total] 3 / 12 / 5 / 20:   7%|â–‹         | 20/277 [04:01<51:41, 12.07s/it][Succeeded / Failed / Skipped / Total] 3 / 12 / 5 / 20:   8%|â–Š         | 21/277 [04:03<49:26, 11.59s/it][Succeeded / Failed / Skipped / Total] 4 / 12 / 5 / 21:   8%|â–Š         | 21/277 [04:03<49:26, 11.59s/it][Succeeded / Failed / Skipped / Total] 4 / 12 / 5 / 21:   8%|â–Š         | 22/277 [04:15<49:26, 11.63s/it][Succeeded / Failed / Skipped / Total] 4 / 13 / 5 / 22:   8%|â–Š         | 22/277 [04:15<49:27, 11.64s/it][Succeeded / Failed / Skipped / Total] 4 / 13 / 5 / 22:   8%|â–Š         | 23/277 [04:16<47:07, 11.13s/it][Succeeded / Failed / Skipped / Total] 4 / 13 / 6 / 23:   8%|â–Š         | 23/277 [04:16<47:08, 11.14s/it][Succeeded / Failed / Skipped / Total] 4 / 13 / 6 / 23:   9%|â–Š         | 24/277 [04:43<49:43, 11.79s/it][Succeeded / Failed / Skipped / Total] 4 / 14 / 6 / 24:   9%|â–Š         | 24/277 [04:43<49:44, 11.80s/it][Succeeded / Failed / Skipped / Total] 4 / 14 / 6 / 24:   9%|â–‰         | 25/277 [04:49<48:33, 11.56s/it][Succeeded / Failed / Skipped / Total] 5 / 14 / 6 / 25:   9%|â–‰         | 25/277 [04:49<48:34, 11.56s/it][Succeeded / Failed / Skipped / Total] 5 / 14 / 6 / 25:   9%|â–‰         | 26/277 [05:04<48:56, 11.70s/it][Succeeded / Failed / Skipped / Total] 5 / 15 / 6 / 26:   9%|â–‰         | 26/277 [05:04<48:56, 11.70s/it][Succeeded / Failed / Skipped / Total] 5 / 15 / 7 / 27:  10%|â–‰         | 27/277 [05:04<46:57, 11.27s/it][Succeeded / Failed / Skipped / Total] 5 / 15 / 7 / 27:  10%|â–ˆ         | 28/277 [05:04<45:06, 10.87s/it][Succeeded / Failed / Skipped / Total] 5 / 15 / 8 / 28:  10%|â–ˆ         | 28/277 [05:04<45:06, 10.87s/it][Succeeded / Failed / Skipped / Total] 5 / 16 / 8 / 29:  10%|â–ˆ         | 29/277 [05:12<44:32, 10.78s/it][Succeeded / Failed / Skipped / Total] 5 / 16 / 8 / 29:  10%|â–ˆ         | 29/277 [05:14<44:53, 10.86s/it][Succeeded / Failed / Skipped / Total] 5 / 16 / 8 / 29:  11%|â–ˆ         | 30/277 [05:16<43:27, 10.56s/it][Succeeded / Failed / Skipped / Total] 6 / 16 / 8 / 30:  11%|â–ˆ         | 30/277 [05:16<43:28, 10.56s/it][Succeeded / Failed / Skipped / Total] 6 / 16 / 8 / 30:  11%|â–ˆ         | 31/277 [05:31<43:47, 10.68s/it][Succeeded / Failed / Skipped / Total] 6 / 17 / 8 / 31:  11%|â–ˆ         | 31/277 [05:31<43:48, 10.68s/it][Succeeded / Failed / Skipped / Total] 6 / 17 / 8 / 31:  12%|â–ˆâ–        | 32/277 [05:41<43:35, 10.68s/it][Succeeded / Failed / Skipped / Total] 6 / 18 / 8 / 32:  12%|â–ˆâ–        | 32/277 [05:41<43:36, 10.68s/it][Succeeded / Failed / Skipped / Total] 6 / 18 / 8 / 32:  12%|â–ˆâ–        | 33/277 [05:54<43:38, 10.73s/it][Succeeded / Failed / Skipped / Total] 6 / 19 / 8 / 33:  12%|â–ˆâ–        | 33/277 [05:54<43:38, 10.73s/it][Succeeded / Failed / Skipped / Total] 6 / 19 / 8 / 33:  12%|â–ˆâ–        | 34/277 [06:14<44:34, 11.01s/it][Succeeded / Failed / Skipped / Total] 6 / 20 / 8 / 34:  12%|â–ˆâ–        | 34/277 [06:14<44:35, 11.01s/it][Succeeded / Failed / Skipped / Total] 6 / 20 / 8 / 34:  13%|â–ˆâ–Ž        | 35/277 [06:14<43:08, 10.70s/it][Succeeded / Failed / Skipped / Total] 6 / 20 / 9 / 35:  13%|â–ˆâ–Ž        | 35/277 [06:14<43:08, 10.70s/it][Succeeded / Failed / Skipped / Total] 6 / 20 / 9 / 35:  13%|â–ˆâ–Ž        | 36/277 [06:21<42:33, 10.60s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 9 / 36:  13%|â–ˆâ–Ž        | 36/277 [06:21<42:33, 10.60s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 9 / 36:  13%|â–ˆâ–Ž        | 37/277 [06:21<41:14, 10.31s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 10 / 37:  13%|â–ˆâ–Ž        | 37/277 [06:21<41:15, 10.31s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 10 / 37:  14%|â–ˆâ–Ž        | 38/277 [06:21<40:00, 10.04s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 11 / 38:  14%|â–ˆâ–Ž        | 38/277 [06:21<40:00, 10.04s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 11 / 38:  14%|â–ˆâ–        | 39/277 [06:21<38:49,  9.79s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 12 / 39:  14%|â–ˆâ–        | 39/277 [06:21<38:49,  9.79s/it][Succeeded / Failed / Skipped / Total] 7 / 20 / 12 / 39:  14%|â–ˆâ–        | 40/277 [06:40<39:33, 10.02s/it][Succeeded / Failed / Skipped / Total] 7 / 21 / 12 / 40:  14%|â–ˆâ–        | 40/277 [06:40<39:33, 10.02s/it][Succeeded / Failed / Skipped / Total] 7 / 21 / 12 / 40:  15%|â–ˆâ–        | 41/277 [06:57<40:04, 10.19s/it][Succeeded / Failed / Skipped / Total] 7 / 22 / 12 / 41:  15%|â–ˆâ–        | 41/277 [06:57<40:04, 10.19s/it][Succeeded / Failed / Skipped / Total] 7 / 22 / 12 / 41:  15%|â–ˆâ–Œ        | 42/277 [07:14<40:30, 10.34s/it][Succeeded / Failed / Skipped / Total] 7 / 23 / 12 / 42:  15%|â–ˆâ–Œ        | 42/277 [07:14<40:30, 10.34s/it][Succeeded / Failed / Skipped / Total] 7 / 23 / 12 / 42:  16%|â–ˆâ–Œ        | 43/277 [07:20<39:55, 10.24s/it][Succeeded / Failed / Skipped / Total] 8 / 23 / 12 / 43:  16%|â–ˆâ–Œ        | 43/277 [07:20<39:56, 10.24s/it][Succeeded / Failed / Skipped / Total] 8 / 23 / 12 / 43:  16%|â–ˆâ–Œ        | 44/277 [07:39<40:35, 10.45s/it][Succeeded / Failed / Skipped / Total] 8 / 24 / 12 / 44:  16%|â–ˆâ–Œ        | 44/277 [07:39<40:35, 10.45s/it][Succeeded / Failed / Skipped / Total] 8 / 24 / 12 / 44:  16%|â–ˆâ–Œ        | 45/277 [07:44<39:54, 10.32s/it][Succeeded / Failed / Skipped / Total] 8 / 25 / 12 / 45:  16%|â–ˆâ–Œ        | 45/277 [07:44<39:55, 10.32s/it][Succeeded / Failed / Skipped / Total] 8 / 25 / 12 / 45:  17%|â–ˆâ–‹        | 46/277 [08:01<40:19, 10.47s/it][Succeeded / Failed / Skipped / Total] 8 / 26 / 12 / 46:  17%|â–ˆâ–‹        | 46/277 [08:01<40:19, 10.47s/it][Succeeded / Failed / Skipped / Total] 8 / 26 / 12 / 46:  17%|â–ˆâ–‹        | 47/277 [08:01<39:18, 10.25s/it][Succeeded / Failed / Skipped / Total] 8 / 26 / 13 / 47:  17%|â–ˆâ–‹        | 47/277 [08:01<39:18, 10.25s/it][Succeeded / Failed / Skipped / Total] 8 / 26 / 13 / 47:  17%|â–ˆâ–‹        | 48/277 [08:20<39:49, 10.44s/it][Succeeded / Failed / Skipped / Total] 8 / 27 / 13 / 48:  17%|â–ˆâ–‹        | 48/277 [08:20<39:50, 10.44s/it][Succeeded / Failed / Skipped / Total] 8 / 27 / 13 / 48:  18%|â–ˆâ–Š        | 49/277 [08:30<39:34, 10.41s/it][Succeeded / Failed / Skipped / Total] 8 / 28 / 13 / 49:  18%|â–ˆâ–Š        | 49/277 [08:30<39:34, 10.41s/it][Succeeded / Failed / Skipped / Total] 8 / 28 / 13 / 49:  18%|â–ˆâ–Š        | 50/277 [08:41<39:27, 10.43s/it][Succeeded / Failed / Skipped / Total] 8 / 29 / 13 / 50:  18%|â–ˆâ–Š        | 50/277 [08:41<39:27, 10.43s/it][Succeeded / Failed / Skipped / Total] 8 / 29 / 13 / 50:  18%|â–ˆâ–Š        | 51/277 [08:52<39:18, 10.44s/it][Succeeded / Failed / Skipped / Total] 8 / 30 / 13 / 51:  18%|â–ˆâ–Š        | 51/277 [08:52<39:18, 10.44s/it][Succeeded / Failed / Skipped / Total] 8 / 30 / 13 / 51:  19%|â–ˆâ–‰        | 52/277 [08:52<38:23, 10.24s/it][Succeeded / Failed / Skipped / Total] 8 / 30 / 14 / 52:  19%|â–ˆâ–‰        | 52/277 [08:52<38:23, 10.24s/it][Succeeded / Failed / Skipped / Total] 8 / 30 / 14 / 52:  19%|â–ˆâ–‰        | 53/277 [09:10<38:46, 10.39s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 14 / 53:  19%|â–ˆâ–‰        | 53/277 [09:10<38:46, 10.39s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 14 / 53:  19%|â–ˆâ–‰        | 54/277 [09:10<37:53, 10.20s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 15 / 54:  19%|â–ˆâ–‰        | 54/277 [09:10<37:54, 10.20s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 15 / 54:  20%|â–ˆâ–‰        | 55/277 [09:10<37:03, 10.01s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 16 / 55:  20%|â–ˆâ–‰        | 55/277 [09:10<37:03, 10.02s/it][Succeeded / Failed / Skipped / Total] 8 / 31 / 16 / 55:  20%|â–ˆâ–ˆ        | 56/277 [09:33<37:41, 10.23s/it][Succeeded / Failed / Skipped / Total] 8 / 32 / 16 / 56:  20%|â–ˆâ–ˆ        | 56/277 [09:33<37:41, 10.23s/it][Succeeded / Failed / Skipped / Total] 8 / 32 / 16 / 56:  21%|â–ˆâ–ˆ        | 57/277 [09:53<38:08, 10.40s/it][Succeeded / Failed / Skipped / Total] 8 / 33 / 16 / 57:  21%|â–ˆâ–ˆ        | 57/277 [09:53<38:09, 10.40s/it][Succeeded / Failed / Skipped / Total] 8 / 33 / 16 / 57:  21%|â–ˆâ–ˆ        | 58/277 [10:12<38:32, 10.56s/it][Succeeded / Failed / Skipped / Total] 8 / 34 / 16 / 58:  21%|â–ˆâ–ˆ        | 58/277 [10:12<38:32, 10.56s/it][Succeeded / Failed / Skipped / Total] 8 / 34 / 16 / 58:  21%|â–ˆâ–ˆâ–       | 59/277 [10:28<38:42, 10.65s/it][Succeeded / Failed / Skipped / Total] 8 / 35 / 16 / 59:  21%|â–ˆâ–ˆâ–       | 59/277 [10:28<38:42, 10.65s/it][Succeeded / Failed / Skipped / Total] 8 / 35 / 16 / 59:  22%|â–ˆâ–ˆâ–       | 60/277 [10:30<38:00, 10.51s/it][Succeeded / Failed / Skipped / Total] 9 / 35 / 16 / 60:  22%|â–ˆâ–ˆâ–       | 60/277 [10:30<38:00, 10.51s/it][Succeeded / Failed / Skipped / Total] 9 / 35 / 16 / 60:  22%|â–ˆâ–ˆâ–       | 61/277 [10:30<37:12, 10.34s/it][Succeeded / Failed / Skipped / Total] 9 / 35 / 17 / 61:  22%|â–ˆâ–ˆâ–       | 61/277 [10:30<37:12, 10.34s/it][Succeeded / Failed / Skipped / Total] 9 / 35 / 17 / 61:  22%|â–ˆâ–ˆâ–       | 62/277 [10:33<36:37, 10.22s/it][Succeeded / Failed / Skipped / Total] 10 / 35 / 17 / 62:  22%|â–ˆâ–ˆâ–       | 62/277 [10:33<36:37, 10.22s/it][Succeeded / Failed / Skipped / Total] 10 / 35 / 17 / 62:  23%|â–ˆâ–ˆâ–Ž       | 63/277 [10:55<37:04, 10.40s/it][Succeeded / Failed / Skipped / Total] 10 / 36 / 17 / 63:  23%|â–ˆâ–ˆâ–Ž       | 63/277 [10:55<37:05, 10.40s/it][Succeeded / Failed / Skipped / Total] 10 / 36 / 17 / 63:  23%|â–ˆâ–ˆâ–Ž       | 64/277 [11:04<36:51, 10.38s/it][Succeeded / Failed / Skipped / Total] 10 / 37 / 17 / 64:  23%|â–ˆâ–ˆâ–Ž       | 64/277 [11:04<36:51, 10.38s/it][Succeeded / Failed / Skipped / Total] 10 / 37 / 17 / 64:  23%|â–ˆâ–ˆâ–Ž       | 65/277 [11:06<36:13, 10.25s/it][Succeeded / Failed / Skipped / Total] 11 / 37 / 17 / 65:  23%|â–ˆâ–ˆâ–Ž       | 65/277 [11:06<36:13, 10.25s/it][Succeeded / Failed / Skipped / Total] 11 / 37 / 17 / 65:  24%|â–ˆâ–ˆâ–       | 66/277 [11:06<35:30, 10.10s/it][Succeeded / Failed / Skipped / Total] 11 / 37 / 18 / 66:  24%|â–ˆâ–ˆâ–       | 66/277 [11:06<35:31, 10.10s/it][Succeeded / Failed / Skipped / Total] 11 / 37 / 18 / 66:  24%|â–ˆâ–ˆâ–       | 67/277 [11:18<35:25, 10.12s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 18 / 67:  24%|â–ˆâ–ˆâ–       | 67/277 [11:18<35:25, 10.12s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 19 / 68:  25%|â–ˆâ–ˆâ–       | 68/277 [11:18<34:44,  9.98s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 19 / 68:  25%|â–ˆâ–ˆâ–       | 69/277 [11:18<34:04,  9.83s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 20 / 69:  25%|â–ˆâ–ˆâ–       | 69/277 [11:18<34:05,  9.83s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 20 / 69:  25%|â–ˆâ–ˆâ–       | 69/277 [11:30<34:40, 10.00s/it][Succeeded / Failed / Skipped / Total] 11 / 38 / 20 / 69:  25%|â–ˆâ–ˆâ–Œ       | 70/277 [11:35<34:17,  9.94s/it][Succeeded / Failed / Skipped / Total] 11 / 39 / 20 / 70:  25%|â–ˆâ–ˆâ–Œ       | 70/277 [11:35<34:17,  9.94s/it][Succeeded / Failed / Skipped / Total] 11 / 39 / 20 / 70:  26%|â–ˆâ–ˆâ–Œ       | 71/277 [11:35<33:38,  9.80s/it][Succeeded / Failed / Skipped / Total] 11 / 39 / 21 / 71:  26%|â–ˆâ–ˆâ–Œ       | 71/277 [11:35<33:38,  9.80s/it][Succeeded / Failed / Skipped / Total] 11 / 39 / 21 / 71:  26%|â–ˆâ–ˆâ–Œ       | 72/277 [11:38<33:08,  9.70s/it][Succeeded / Failed / Skipped / Total] 12 / 39 / 21 / 72:  26%|â–ˆâ–ˆâ–Œ       | 72/277 [11:38<33:08,  9.70s/it][Succeeded / Failed / Skipped / Total] 12 / 39 / 21 / 72:  26%|â–ˆâ–ˆâ–‹       | 73/277 [11:53<33:14,  9.78s/it][Succeeded / Failed / Skipped / Total] 12 / 40 / 21 / 73:  26%|â–ˆâ–ˆâ–‹       | 73/277 [11:53<33:14,  9.78s/it][Succeeded / Failed / Skipped / Total] 12 / 40 / 21 / 73:  27%|â–ˆâ–ˆâ–‹       | 74/277 [12:13<33:32,  9.91s/it][Succeeded / Failed / Skipped / Total] 12 / 41 / 21 / 74:  27%|â–ˆâ–ˆâ–‹       | 74/277 [12:13<33:32,  9.91s/it][Succeeded / Failed / Skipped / Total] 12 / 41 / 21 / 74:  27%|â–ˆâ–ˆâ–‹       | 75/277 [12:47<34:27, 10.23s/it][Succeeded / Failed / Skipped / Total] 12 / 42 / 21 / 75:  27%|â–ˆâ–ˆâ–‹       | 75/277 [12:47<34:27, 10.23s/it][Succeeded / Failed / Skipped / Total] 12 / 42 / 21 / 75:  27%|â–ˆâ–ˆâ–‹       | 76/277 [13:07<34:42, 10.36s/it][Succeeded / Failed / Skipped / Total] 12 / 43 / 21 / 76:  27%|â–ˆâ–ˆâ–‹       | 76/277 [13:07<34:42, 10.36s/it][Succeeded / Failed / Skipped / Total] 12 / 43 / 21 / 76:  28%|â–ˆâ–ˆâ–Š       | 77/277 [13:24<34:49, 10.45s/it][Succeeded / Failed / Skipped / Total] 12 / 44 / 21 / 77:  28%|â–ˆâ–ˆâ–Š       | 77/277 [13:24<34:49, 10.45s/it][Succeeded / Failed / Skipped / Total] 12 / 44 / 21 / 77:  28%|â–ˆâ–ˆâ–Š       | 78/277 [13:43<34:59, 10.55s/it][Succeeded / Failed / Skipped / Total] 12 / 45 / 21 / 78:  28%|â–ˆâ–ˆâ–Š       | 78/277 [13:43<35:00, 10.55s/it][Succeeded / Failed / Skipped / Total] 12 / 45 / 21 / 78:  29%|â–ˆâ–ˆâ–Š       | 79/277 [14:07<35:24, 10.73s/it][Succeeded / Failed / Skipped / Total] 12 / 46 / 21 / 79:  29%|â–ˆâ–ˆâ–Š       | 79/277 [14:07<35:24, 10.73s/it][Succeeded / Failed / Skipped / Total] 12 / 46 / 21 / 79:  29%|â–ˆâ–ˆâ–‰       | 80/277 [14:37<36:00, 10.97s/it][Succeeded / Failed / Skipped / Total] 12 / 47 / 21 / 80:  29%|â–ˆâ–ˆâ–‰       | 80/277 [14:37<36:00, 10.97s/it][Succeeded / Failed / Skipped / Total] 12 / 47 / 21 / 80:  29%|â–ˆâ–ˆâ–‰       | 81/277 [15:03<36:26, 11.16s/it][Succeeded / Failed / Skipped / Total] 12 / 48 / 21 / 81:  29%|â–ˆâ–ˆâ–‰       | 81/277 [15:03<36:26, 11.16s/it][Succeeded / Failed / Skipped / Total] 12 / 48 / 22 / 82:  30%|â–ˆâ–ˆâ–‰       | 82/277 [15:03<35:49, 11.02s/it][Succeeded / Failed / Skipped / Total] 12 / 48 / 22 / 82:  30%|â–ˆâ–ˆâ–‰       | 83/277 [15:13<35:34, 11.00s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 22 / 83:  30%|â–ˆâ–ˆâ–‰       | 83/277 [15:13<35:34, 11.00s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 23 / 84:  30%|â–ˆâ–ˆâ–ˆ       | 84/277 [15:13<34:58, 10.87s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 23 / 84:  31%|â–ˆâ–ˆâ–ˆ       | 85/277 [15:13<34:23, 10.74s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 24 / 85:  31%|â–ˆâ–ˆâ–ˆ       | 85/277 [15:13<34:23, 10.75s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 25 / 86:  31%|â–ˆâ–ˆâ–ˆ       | 86/277 [15:13<33:48, 10.62s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 25 / 86:  31%|â–ˆâ–ˆâ–ˆâ–      | 87/277 [15:13<33:15, 10.50s/it][Succeeded / Failed / Skipped / Total] 12 / 49 / 26 / 87:  31%|â–ˆâ–ˆâ–ˆâ–      | 87/277 [15:13<33:15, 10.50s/it][Succeeded / Failed / Skipped / Total] 13 / 49 / 26 / 88:  32%|â–ˆâ–ˆâ–ˆâ–      | 88/277 [15:15<32:45, 10.40s/it][Succeeded / Failed / Skipped / Total] 13 / 49 / 26 / 88:  32%|â–ˆâ–ˆâ–ˆâ–      | 88/277 [15:25<33:06, 10.51s/it][Succeeded / Failed / Skipped / Total] 13 / 49 / 26 / 88:  32%|â–ˆâ–ˆâ–ˆâ–      | 89/277 [15:32<32:50, 10.48s/it][Succeeded / Failed / Skipped / Total] 13 / 50 / 26 / 89:  32%|â–ˆâ–ˆâ–ˆâ–      | 89/277 [15:32<32:50, 10.48s/it][Succeeded / Failed / Skipped / Total] 13 / 50 / 26 / 89:  32%|â–ˆâ–ˆâ–ˆâ–      | 90/277 [15:48<32:50, 10.54s/it][Succeeded / Failed / Skipped / Total] 13 / 51 / 26 / 90:  32%|â–ˆâ–ˆâ–ˆâ–      | 90/277 [15:48<32:50, 10.54s/it][Succeeded / Failed / Skipped / Total] 13 / 51 / 26 / 90:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/277 [16:10<33:02, 10.66s/it][Succeeded / Failed / Skipped / Total] 13 / 52 / 26 / 91:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/277 [16:10<33:02, 10.66s/it][Succeeded / Failed / Skipped / Total] 13 / 52 / 26 / 91:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/277 [16:31<33:12, 10.77s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 26 / 92:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 92/277 [16:31<33:12, 10.77s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 26 / 92:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 93/277 [16:31<32:40, 10.66s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 27 / 93:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 93/277 [16:31<32:41, 10.66s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 27 / 93:  34%|â–ˆâ–ˆâ–ˆâ–      | 94/277 [16:31<32:09, 10.54s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 28 / 94:  34%|â–ˆâ–ˆâ–ˆâ–      | 94/277 [16:31<32:09, 10.55s/it][Succeeded / Failed / Skipped / Total] 13 / 53 / 28 / 94:  34%|â–ˆâ–ˆâ–ˆâ–      | 95/277 [16:56<32:26, 10.70s/it][Succeeded / Failed / Skipped / Total] 13 / 54 / 28 / 95:  34%|â–ˆâ–ˆâ–ˆâ–      | 95/277 [16:56<32:27, 10.70s/it][Succeeded / Failed / Skipped / Total] 13 / 54 / 28 / 95:  35%|â–ˆâ–ˆâ–ˆâ–      | 96/277 [17:14<32:31, 10.78s/it][Succeeded / Failed / Skipped / Total] 13 / 55 / 28 / 96:  35%|â–ˆâ–ˆâ–ˆâ–      | 96/277 [17:15<32:31, 10.78s/it][Succeeded / Failed / Skipped / Total] 13 / 55 / 28 / 96:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/277 [17:31<32:30, 10.84s/it][Succeeded / Failed / Skipped / Total] 13 / 56 / 28 / 97:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/277 [17:31<32:30, 10.84s/it][Succeeded / Failed / Skipped / Total] 13 / 56 / 28 / 97:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 98/277 [17:46<32:28, 10.89s/it][Succeeded / Failed / Skipped / Total] 13 / 57 / 28 / 98:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 98/277 [17:46<32:28, 10.89s/it][Succeeded / Failed / Skipped / Total] 13 / 57 / 28 / 98:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 99/277 [17:46<31:58, 10.78s/it][Succeeded / Failed / Skipped / Total] 13 / 57 / 29 / 99:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 99/277 [17:46<31:58, 10.78s/it][Succeeded / Failed / Skipped / Total] 13 / 57 / 29 / 99:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 100/277 [18:11<32:11, 10.91s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 29 / 100:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 100/277 [18:11<32:11, 10.91s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 29 / 100:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 101/277 [18:11<31:41, 10.80s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 30 / 101:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 101/277 [18:11<31:41, 10.80s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 30 / 101:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/277 [18:11<31:12, 10.70s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 31 / 102:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 102/277 [18:11<31:12, 10.70s/it][Succeeded / Failed / Skipped / Total] 13 / 58 / 31 / 102:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/277 [18:26<31:09, 10.74s/it][Succeeded / Failed / Skipped / Total] 13 / 59 / 31 / 103:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 103/277 [18:26<31:09, 10.74s/it][Succeeded / Failed / Skipped / Total] 13 / 59 / 31 / 103:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 104/277 [18:48<31:17, 10.85s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 31 / 104:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 104/277 [18:48<31:17, 10.85s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 31 / 104:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 105/277 [18:48<30:48, 10.75s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 32 / 105:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 105/277 [18:48<30:48, 10.75s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 32 / 105:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 106/277 [18:48<30:20, 10.65s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 33 / 106:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 106/277 [18:48<30:20, 10.65s/it][Succeeded / Failed / Skipped / Total] 13 / 60 / 33 / 106:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 107/277 [19:02<30:14, 10.68s/it][Succeeded / Failed / Skipped / Total] 13 / 61 / 33 / 107:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 107/277 [19:02<30:14, 10.68s/it][Succeeded / Failed / Skipped / Total] 13 / 61 / 33 / 107:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 108/277 [19:13<30:04, 10.68s/it][Succeeded / Failed / Skipped / Total] 13 / 62 / 33 / 108:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 108/277 [19:13<30:04, 10.68s/it][Succeeded / Failed / Skipped / Total] 13 / 62 / 33 / 108:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 109/277 [19:16<29:41, 10.61s/it][Succeeded / Failed / Skipped / Total] 14 / 62 / 33 / 109:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 109/277 [19:16<29:41, 10.61s/it][Succeeded / Failed / Skipped / Total] 14 / 62 / 33 / 109:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 110/277 [19:19<29:19, 10.54s/it][Succeeded / Failed / Skipped / Total] 15 / 62 / 33 / 110:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 110/277 [19:19<29:19, 10.54s/it][Succeeded / Failed / Skipped / Total] 15 / 62 / 33 / 110:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/277 [19:19<28:53, 10.44s/it][Succeeded / Failed / Skipped / Total] 15 / 62 / 34 / 111:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/277 [19:19<28:53, 10.44s/it][Succeeded / Failed / Skipped / Total] 15 / 62 / 34 / 111:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 112/277 [19:45<29:05, 10.58s/it][Succeeded / Failed / Skipped / Total] 15 / 63 / 34 / 112:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 112/277 [19:45<29:05, 10.58s/it][Succeeded / Failed / Skipped / Total] 15 / 63 / 34 / 112:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/277 [19:45<28:40, 10.49s/it][Succeeded / Failed / Skipped / Total] 15 / 63 / 35 / 113:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/277 [19:45<28:40, 10.49s/it][Succeeded / Failed / Skipped / Total] 15 / 63 / 35 / 113:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 114/277 [20:02<28:38, 10.54s/it][Succeeded / Failed / Skipped / Total] 15 / 64 / 35 / 114:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 114/277 [20:02<28:38, 10.54s/it][Succeeded / Failed / Skipped / Total] 15 / 64 / 35 / 114:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 115/277 [20:02<28:13, 10.45s/it][Succeeded / Failed / Skipped / Total] 15 / 64 / 36 / 115:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 115/277 [20:02<28:13, 10.45s/it][Succeeded / Failed / Skipped / Total] 15 / 64 / 36 / 115:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 116/277 [20:04<27:51, 10.38s/it][Succeeded / Failed / Skipped / Total] 16 / 64 / 36 / 116:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 116/277 [20:04<27:51, 10.38s/it][Succeeded / Failed / Skipped / Total] 16 / 64 / 36 / 116:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 117/277 [20:05<27:29, 10.31s/it][Succeeded / Failed / Skipped / Total] 17 / 64 / 36 / 117:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 117/277 [20:05<27:29, 10.31s/it][Succeeded / Failed / Skipped / Total] 17 / 64 / 36 / 117:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/277 [20:38<27:48, 10.49s/it][Succeeded / Failed / Skipped / Total] 17 / 65 / 36 / 118:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/277 [20:38<27:48, 10.49s/it][Succeeded / Failed / Skipped / Total] 17 / 65 / 36 / 118:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 119/277 [20:48<27:38, 10.50s/it][Succeeded / Failed / Skipped / Total] 17 / 66 / 36 / 119:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 119/277 [20:49<27:38, 10.50s/it][Succeeded / Failed / Skipped / Total] 17 / 66 / 36 / 119:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 120/277 [20:49<27:14, 10.41s/it][Succeeded / Failed / Skipped / Total] 17 / 66 / 37 / 120:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 120/277 [20:49<27:14, 10.41s/it][Succeeded / Failed / Skipped / Total] 17 / 66 / 37 / 120:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 121/277 [20:52<26:54, 10.35s/it][Succeeded / Failed / Skipped / Total] 18 / 66 / 37 / 121:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 121/277 [20:52<26:54, 10.35s/it][Succeeded / Failed / Skipped / Total] 18 / 66 / 37 / 121:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122/277 [21:02<26:44, 10.35s/it][Succeeded / Failed / Skipped / Total] 18 / 67 / 37 / 122:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122/277 [21:02<26:44, 10.35s/it][Succeeded / Failed / Skipped / Total] 18 / 67 / 37 / 122:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/277 [21:17<26:39, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 68 / 37 / 123:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/277 [21:17<26:39, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 68 / 37 / 123:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/277 [21:17<26:16, 10.30s/it][Succeeded / Failed / Skipped / Total] 18 / 68 / 38 / 124:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/277 [21:17<26:16, 10.30s/it][Succeeded / Failed / Skipped / Total] 18 / 68 / 38 / 124:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/277 [21:29<26:08, 10.32s/it][Succeeded / Failed / Skipped / Total] 18 / 69 / 38 / 125:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125/277 [21:29<26:08, 10.32s/it][Succeeded / Failed / Skipped / Total] 18 / 69 / 38 / 125:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 126/277 [21:52<26:12, 10.41s/it][Succeeded / Failed / Skipped / Total] 18 / 70 / 38 / 126:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 126/277 [21:52<26:12, 10.41s/it][Succeeded / Failed / Skipped / Total] 18 / 70 / 38 / 126:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 127/277 [22:04<26:04, 10.43s/it][Succeeded / Failed / Skipped / Total] 18 / 71 / 38 / 127:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 127/277 [22:04<26:04, 10.43s/it][Succeeded / Failed / Skipped / Total] 18 / 71 / 38 / 127:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 128/277 [22:10<25:48, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 72 / 38 / 128:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 128/277 [22:10<25:48, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 72 / 39 / 129:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 129/277 [22:10<25:26, 10.31s/it][Succeeded / Failed / Skipped / Total] 18 / 72 / 39 / 129:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 130/277 [22:27<25:23, 10.36s/it][Succeeded / Failed / Skipped / Total] 18 / 73 / 39 / 130:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 130/277 [22:27<25:23, 10.37s/it][Succeeded / Failed / Skipped / Total] 18 / 73 / 39 / 130:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 131/277 [22:40<25:16, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 74 / 39 / 131:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 131/277 [22:40<25:16, 10.39s/it][Succeeded / Failed / Skipped / Total] 18 / 74 / 39 / 131:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 132/277 [22:40<24:54, 10.31s/it][Succeeded / Failed / Skipped / Total] 18 / 74 / 40 / 132:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 132/277 [22:40<24:54, 10.31s/it][Succeeded / Failed / Skipped / Total] 18 / 74 / 40 / 132:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 133/277 [23:08<25:03, 10.44s/it][Succeeded / Failed / Skipped / Total] 18 / 75 / 40 / 133:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 133/277 [23:08<25:03, 10.44s/it][Succeeded / Failed / Skipped / Total] 18 / 75 / 40 / 133:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 134/277 [23:08<24:41, 10.36s/it][Succeeded / Failed / Skipped / Total] 18 / 75 / 41 / 134:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 134/277 [23:08<24:41, 10.36s/it][Succeeded / Failed / Skipped / Total] 18 / 75 / 41 / 134:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 135/277 [23:18<24:31, 10.36s/it][Succeeded / Failed / Skipped / Total] 18 / 76 / 41 / 135:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 135/277 [23:18<24:31, 10.36s/it][Succeeded / Failed / Skipped / Total] 18 / 76 / 42 / 136:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136/277 [23:18<24:10, 10.29s/it][Succeeded / Failed / Skipped / Total] 18 / 76 / 42 / 136:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 137/277 [23:28<23:58, 10.28s/it][Succeeded / Failed / Skipped / Total] 19 / 76 / 42 / 137:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 137/277 [23:28<23:58, 10.28s/it][Succeeded / Failed / Skipped / Total] 19 / 76 / 42 / 137:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 137/277 [23:40<24:11, 10.37s/it][Succeeded / Failed / Skipped / Total] 19 / 76 / 42 / 137:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 138/277 [23:46<23:56, 10.33s/it][Succeeded / Failed / Skipped / Total] 19 / 77 / 42 / 138:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 138/277 [23:46<23:56, 10.33s/it][Succeeded / Failed / Skipped / Total] 19 / 77 / 42 / 138:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 139/277 [24:00<23:49, 10.36s/it][Succeeded / Failed / Skipped / Total] 19 / 78 / 42 / 139:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 139/277 [24:00<23:49, 10.36s/it][Succeeded / Failed / Skipped / Total] 19 / 78 / 42 / 139:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 140/277 [24:11<23:40, 10.37s/it][Succeeded / Failed / Skipped / Total] 20 / 78 / 42 / 140:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 140/277 [24:11<23:40, 10.37s/it][Succeeded / Failed / Skipped / Total] 20 / 78 / 42 / 140:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 141/277 [24:22<23:30, 10.37s/it][Succeeded / Failed / Skipped / Total] 20 / 79 / 42 / 141:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 141/277 [24:22<23:30, 10.37s/it][Succeeded / Failed / Skipped / Total] 20 / 79 / 42 / 141:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 142/277 [24:44<23:30, 10.45s/it][Succeeded / Failed / Skipped / Total] 20 / 80 / 42 / 142:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 142/277 [24:44<23:30, 10.45s/it][Succeeded / Failed / Skipped / Total] 20 / 80 / 42 / 142:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 143/277 [25:05<23:30, 10.53s/it][Succeeded / Failed / Skipped / Total] 20 / 81 / 42 / 143:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 143/277 [25:05<23:30, 10.53s/it][Succeeded / Failed / Skipped / Total] 20 / 81 / 42 / 143:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 144/277 [25:05<23:10, 10.45s/it][Succeeded / Failed / Skipped / Total] 20 / 81 / 43 / 144:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 144/277 [25:05<23:10, 10.45s/it][Succeeded / Failed / Skipped / Total] 20 / 81 / 43 / 144:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 145/277 [25:23<23:07, 10.51s/it][Succeeded / Failed / Skipped / Total] 20 / 82 / 43 / 145:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 145/277 [25:23<23:07, 10.51s/it][Succeeded / Failed / Skipped / Total] 20 / 82 / 43 / 145:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/277 [25:38<23:00, 10.54s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 43 / 146:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 146/277 [25:38<23:00, 10.54s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 43 / 146:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/277 [25:38<22:40, 10.47s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 44 / 147:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 147/277 [25:38<22:40, 10.47s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 44 / 147:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 148/277 [25:38<22:21, 10.40s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 45 / 148:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 148/277 [25:38<22:21, 10.40s/it][Succeeded / Failed / Skipped / Total] 20 / 83 / 45 / 148:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 149/277 [25:54<22:15, 10.43s/it][Succeeded / Failed / Skipped / Total] 20 / 84 / 45 / 149:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 149/277 [25:54<22:15, 10.43s/it][Succeeded / Failed / Skipped / Total] 20 / 84 / 45 / 149:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150/277 [25:57<21:58, 10.38s/it][Succeeded / Failed / Skipped / Total] 21 / 84 / 45 / 150:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150/277 [25:57<21:58, 10.38s/it][Succeeded / Failed / Skipped / Total] 21 / 84 / 45 / 150:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 151/277 [26:20<21:58, 10.47s/it][Succeeded / Failed / Skipped / Total] 21 / 85 / 45 / 151:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 151/277 [26:20<21:58, 10.47s/it][Succeeded / Failed / Skipped / Total] 21 / 85 / 45 / 151:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 152/277 [26:20<21:39, 10.40s/it][Succeeded / Failed / Skipped / Total] 21 / 85 / 46 / 152:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 152/277 [26:20<21:39, 10.40s/it][Succeeded / Failed / Skipped / Total] 21 / 85 / 46 / 152:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 153/277 [26:23<21:23, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 85 / 46 / 153:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 153/277 [26:23<21:23, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 85 / 46 / 153:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 154/277 [26:31<21:11, 10.34s/it][Succeeded / Failed / Skipped / Total] 22 / 86 / 46 / 154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 154/277 [26:31<21:11, 10.34s/it][Succeeded / Failed / Skipped / Total] 22 / 86 / 46 / 154:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 155/277 [26:49<21:06, 10.38s/it][Succeeded / Failed / Skipped / Total] 22 / 87 / 46 / 155:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 155/277 [26:49<21:06, 10.38s/it][Succeeded / Failed / Skipped / Total] 22 / 87 / 46 / 155:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 156/277 [26:49<20:48, 10.32s/it][Succeeded / Failed / Skipped / Total] 22 / 87 / 47 / 156:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 156/277 [26:49<20:48, 10.32s/it][Succeeded / Failed / Skipped / Total] 22 / 87 / 47 / 156:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/277 [27:14<20:49, 10.41s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 47 / 157:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 157/277 [27:14<20:49, 10.41s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 47 / 157:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/277 [27:14<20:31, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 48 / 158:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 158/277 [27:15<20:31, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 48 / 158:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 159/277 [27:15<20:13, 10.28s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 49 / 159:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 159/277 [27:15<20:13, 10.28s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 49 / 159:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 160/277 [27:15<19:55, 10.22s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 50 / 160:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 160/277 [27:15<19:55, 10.22s/it][Succeeded / Failed / Skipped / Total] 22 / 88 / 50 / 160:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 161/277 [27:33<19:51, 10.27s/it][Succeeded / Failed / Skipped / Total] 22 / 89 / 50 / 161:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 161/277 [27:33<19:51, 10.27s/it][Succeeded / Failed / Skipped / Total] 22 / 89 / 50 / 161:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 162/277 [27:33<19:33, 10.21s/it][Succeeded / Failed / Skipped / Total] 22 / 89 / 51 / 162:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 162/277 [27:33<19:33, 10.21s/it][Succeeded / Failed / Skipped / Total] 22 / 89 / 51 / 162:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 163/277 [27:38<19:20, 10.18s/it][Succeeded / Failed / Skipped / Total] 22 / 90 / 51 / 163:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 163/277 [27:38<19:20, 10.18s/it][Succeeded / Failed / Skipped / Total] 22 / 90 / 51 / 163:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 164/277 [28:17<19:29, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 91 / 51 / 164:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 164/277 [28:17<19:29, 10.35s/it][Succeeded / Failed / Skipped / Total] 22 / 91 / 51 / 164:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 165/277 [28:17<19:12, 10.29s/it][Succeeded / Failed / Skipped / Total] 22 / 91 / 52 / 165:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 165/277 [28:17<19:12, 10.29s/it][Succeeded / Failed / Skipped / Total] 22 / 91 / 52 / 165:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 166/277 [28:29<19:02, 10.30s/it][Succeeded / Failed / Skipped / Total] 22 / 92 / 52 / 166:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 166/277 [28:29<19:02, 10.30s/it][Succeeded / Failed / Skipped / Total] 22 / 92 / 52 / 166:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 167/277 [28:29<18:45, 10.24s/it][Succeeded / Failed / Skipped / Total] 22 / 92 / 53 / 167:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 167/277 [28:29<18:45, 10.24s/it][Succeeded / Failed / Skipped / Total] 22 / 92 / 53 / 167:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/277 [28:35<18:32, 10.21s/it][Succeeded / Failed / Skipped / Total] 23 / 92 / 53 / 168:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 168/277 [28:35<18:32, 10.21s/it][Succeeded / Failed / Skipped / Total] 23 / 92 / 53 / 168:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/277 [28:37<18:17, 10.16s/it][Succeeded / Failed / Skipped / Total] 24 / 92 / 53 / 169:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 169/277 [28:37<18:17, 10.16s/it][Succeeded / Failed / Skipped / Total] 24 / 92 / 53 / 169:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 170/277 [28:37<18:01, 10.11s/it][Succeeded / Failed / Skipped / Total] 24 / 92 / 54 / 170:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 170/277 [28:37<18:01, 10.11s/it][Succeeded / Failed / Skipped / Total] 24 / 92 / 54 / 170:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 171/277 [28:40<17:46, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 92 / 54 / 171:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 171/277 [28:40<17:46, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 92 / 54 / 171:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 172/277 [28:56<17:39, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 93 / 54 / 172:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 172/277 [28:56<17:39, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 93 / 54 / 172:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 173/277 [29:06<17:30, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 94 / 54 / 173:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 173/277 [29:06<17:30, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 94 / 54 / 173:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 174/277 [29:15<17:19, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 95 / 54 / 174:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 174/277 [29:15<17:19, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 95 / 54 / 174:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 175/277 [29:33<17:13, 10.13s/it][Succeeded / Failed / Skipped / Total] 25 / 96 / 54 / 175:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 175/277 [29:33<17:13, 10.13s/it][Succeeded / Failed / Skipped / Total] 25 / 96 / 54 / 175:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 176/277 [29:57<17:11, 10.21s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 54 / 176:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 176/277 [29:57<17:11, 10.21s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 54 / 176:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 177/277 [29:57<16:55, 10.16s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 55 / 177:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 177/277 [29:57<16:55, 10.16s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 55 / 177:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/277 [29:57<16:39, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 56 / 178:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/277 [29:57<16:39, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 97 / 56 / 178:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/277 [30:06<16:29, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 98 / 56 / 179:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/277 [30:06<16:29, 10.09s/it][Succeeded / Failed / Skipped / Total] 25 / 98 / 56 / 179:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 180/277 [30:18<16:19, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 56 / 180:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 180/277 [30:18<16:19, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 56 / 180:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 181/277 [30:18<16:04, 10.05s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 57 / 181:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 181/277 [30:18<16:04, 10.05s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 57 / 181:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 182/277 [30:18<15:49,  9.99s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 58 / 182:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 182/277 [30:18<15:49,  9.99s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 58 / 182:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 183/277 [30:18<15:34,  9.94s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 59 / 183:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 183/277 [30:18<15:34,  9.94s/it][Succeeded / Failed / Skipped / Total] 25 / 99 / 59 / 183:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 184/277 [30:31<15:25,  9.96s/it][Succeeded / Failed / Skipped / Total] 25 / 100 / 59 / 184:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 184/277 [30:31<15:25,  9.96s/it][Succeeded / Failed / Skipped / Total] 25 / 100 / 59 / 184:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 185/277 [30:31<15:10,  9.90s/it][Succeeded / Failed / Skipped / Total] 25 / 100 / 60 / 185:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 185/277 [30:31<15:10,  9.90s/it][Succeeded / Failed / Skipped / Total] 25 / 100 / 60 / 185:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 186/277 [30:43<15:01,  9.91s/it][Succeeded / Failed / Skipped / Total] 25 / 101 / 60 / 186:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 186/277 [30:43<15:01,  9.91s/it][Succeeded / Failed / Skipped / Total] 25 / 101 / 60 / 186:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 187/277 [31:11<15:00, 10.01s/it][Succeeded / Failed / Skipped / Total] 25 / 102 / 60 / 187:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 187/277 [31:11<15:00, 10.01s/it][Succeeded / Failed / Skipped / Total] 25 / 102 / 60 / 187:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 188/277 [31:31<14:55, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 103 / 60 / 188:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 188/277 [31:31<14:55, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 103 / 60 / 188:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 189/277 [31:42<14:45, 10.07s/it][Succeeded / Failed / Skipped / Total] 25 / 104 / 60 / 189:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 189/277 [31:42<14:45, 10.07s/it][Succeeded / Failed / Skipped / Total] 25 / 104 / 60 / 189:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 190/277 [32:01<14:39, 10.11s/it][Succeeded / Failed / Skipped / Total] 25 / 105 / 60 / 190:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 190/277 [32:01<14:39, 10.11s/it][Succeeded / Failed / Skipped / Total] 25 / 105 / 60 / 190:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/277 [32:01<14:25, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 105 / 61 / 191:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 191/277 [32:01<14:25, 10.06s/it][Succeeded / Failed / Skipped / Total] 25 / 105 / 61 / 191:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 192/277 [32:18<14:18, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 106 / 61 / 192:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 192/277 [32:18<14:18, 10.10s/it][Succeeded / Failed / Skipped / Total] 25 / 106 / 61 / 192:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 193/277 [32:18<14:03, 10.04s/it][Succeeded / Failed / Skipped / Total] 25 / 106 / 62 / 193:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 193/277 [32:18<14:03, 10.04s/it][Succeeded / Failed / Skipped / Total] 25 / 106 / 62 / 193:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 194/277 [32:35<13:56, 10.08s/it][Succeeded / Failed / Skipped / Total] 25 / 107 / 62 / 194:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 194/277 [32:35<13:56, 10.08s/it][Succeeded / Failed / Skipped / Total] 25 / 107 / 62 / 194:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 195/277 [32:49<13:48, 10.10s/it][Succeeded / Failed / Skipped / Total] 26 / 107 / 62 / 195:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 195/277 [32:49<13:48, 10.10s/it][Succeeded / Failed / Skipped / Total] 26 / 107 / 62 / 195:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 196/277 [32:49<13:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 26 / 107 / 63 / 196:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 196/277 [32:49<13:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 26 / 107 / 63 / 196:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 197/277 [33:06<13:26, 10.08s/it][Succeeded / Failed / Skipped / Total] 26 / 108 / 63 / 197:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 197/277 [33:06<13:26, 10.08s/it][Succeeded / Failed / Skipped / Total] 26 / 108 / 63 / 197:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 198/277 [33:06<13:12, 10.03s/it][Succeeded / Failed / Skipped / Total] 26 / 108 / 64 / 198:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 198/277 [33:06<13:12, 10.03s/it][Succeeded / Failed / Skipped / Total] 26 / 108 / 64 / 198:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 199/277 [33:26<13:06, 10.09s/it][Succeeded / Failed / Skipped / Total] 26 / 109 / 64 / 199:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 199/277 [33:27<13:06, 10.09s/it][Succeeded / Failed / Skipped / Total] 26 / 109 / 64 / 199:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 200/277 [33:29<12:53, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 109 / 64 / 200:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 200/277 [33:29<12:53, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 109 / 64 / 200:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/277 [33:40<12:43, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 110 / 64 / 201:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 201/277 [33:40<12:43, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 110 / 64 / 201:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/277 [33:49<12:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 111 / 64 / 202:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 202/277 [33:49<12:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 111 / 64 / 202:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 203/277 [34:07<12:26, 10.08s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 64 / 203:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 203/277 [34:07<12:26, 10.08s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 64 / 203:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 204/277 [34:07<12:12, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 65 / 204:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 204/277 [34:07<12:12, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 65 / 204:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 205/277 [34:07<11:59,  9.99s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 66 / 205:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 205/277 [34:07<11:59,  9.99s/it][Succeeded / Failed / Skipped / Total] 27 / 112 / 66 / 205:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 206/277 [34:17<11:49,  9.99s/it][Succeeded / Failed / Skipped / Total] 27 / 113 / 66 / 206:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 206/277 [34:17<11:49,  9.99s/it][Succeeded / Failed / Skipped / Total] 27 / 113 / 66 / 206:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 207/277 [34:36<11:42, 10.03s/it][Succeeded / Failed / Skipped / Total] 27 / 114 / 66 / 207:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 207/277 [34:36<11:42, 10.03s/it][Succeeded / Failed / Skipped / Total] 27 / 114 / 66 / 207:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 208/277 [34:51<11:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 115 / 66 / 208:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 208/277 [34:51<11:33, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 115 / 66 / 208:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 209/277 [35:09<11:26, 10.10s/it][Succeeded / Failed / Skipped / Total] 27 / 116 / 66 / 209:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 209/277 [35:09<11:26, 10.10s/it][Succeeded / Failed / Skipped / Total] 27 / 116 / 66 / 209:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 210/277 [35:20<11:16, 10.10s/it][Succeeded / Failed / Skipped / Total] 27 / 117 / 66 / 210:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 210/277 [35:20<11:16, 10.10s/it][Succeeded / Failed / Skipped / Total] 27 / 117 / 66 / 210:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 211/277 [35:20<11:03, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 117 / 67 / 211:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 211/277 [35:20<11:03, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 117 / 67 / 211:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/277 [35:32<10:53, 10.06s/it][Succeeded / Failed / Skipped / Total] 27 / 118 / 67 / 212:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 212/277 [35:32<10:53, 10.06s/it][Succeeded / Failed / Skipped / Total] 27 / 118 / 67 / 212:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/277 [35:32<10:40, 10.01s/it][Succeeded / Failed / Skipped / Total] 27 / 118 / 68 / 213:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 213/277 [35:32<10:40, 10.01s/it][Succeeded / Failed / Skipped / Total] 27 / 118 / 68 / 213:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 214/277 [35:49<10:32, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 119 / 68 / 214:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 214/277 [35:49<10:32, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 119 / 68 / 214:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 215/277 [36:01<10:23, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 120 / 68 / 215:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 215/277 [36:01<10:23, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 120 / 68 / 215:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 216/277 [36:14<10:14, 10.07s/it][Succeeded / Failed / Skipped / Total] 27 / 121 / 68 / 216:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 216/277 [36:14<10:14, 10.07s/it][Succeeded / Failed / Skipped / Total] 27 / 121 / 68 / 216:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 217/277 [36:15<10:01, 10.02s/it][Succeeded / Failed / Skipped / Total] 27 / 121 / 69 / 217:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 217/277 [36:15<10:01, 10.02s/it][Succeeded / Failed / Skipped / Total] 27 / 121 / 69 / 217:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 218/277 [36:31<09:53, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 122 / 69 / 218:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 218/277 [36:31<09:53, 10.05s/it][Succeeded / Failed / Skipped / Total] 27 / 122 / 69 / 218:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 219/277 [36:38<09:42, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 123 / 69 / 219:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 219/277 [36:38<09:42, 10.04s/it][Succeeded / Failed / Skipped / Total] 27 / 123 / 69 / 219:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 220/277 [36:39<09:29, 10.00s/it][Succeeded / Failed / Skipped / Total] 27 / 123 / 70 / 220:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 220/277 [36:39<09:29, 10.00s/it][Succeeded / Failed / Skipped / Total] 27 / 123 / 70 / 220:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 221/277 [36:54<09:21, 10.02s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 70 / 221:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 221/277 [36:54<09:21, 10.02s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 70 / 221:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 222/277 [36:54<09:08,  9.98s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 71 / 222:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 222/277 [36:54<09:08,  9.98s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 71 / 222:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/277 [36:54<08:56,  9.93s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 72 / 223:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 223/277 [36:54<08:56,  9.93s/it][Succeeded / Failed / Skipped / Total] 27 / 124 / 72 / 223:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/277 [37:00<08:45,  9.91s/it][Succeeded / Failed / Skipped / Total] 28 / 124 / 72 / 224:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 224/277 [37:00<08:45,  9.91s/it][Succeeded / Failed / Skipped / Total] 28 / 124 / 72 / 224:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 225/277 [37:00<08:33,  9.87s/it][Succeeded / Failed / Skipped / Total] 28 / 124 / 73 / 225:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 225/277 [37:00<08:33,  9.87s/it][Succeeded / Failed / Skipped / Total] 28 / 124 / 73 / 225:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 226/277 [37:23<08:26,  9.93s/it][Succeeded / Failed / Skipped / Total] 28 / 125 / 73 / 226:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 226/277 [37:23<08:26,  9.93s/it][Succeeded / Failed / Skipped / Total] 28 / 125 / 73 / 226:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 227/277 [37:37<08:17,  9.95s/it][Succeeded / Failed / Skipped / Total] 28 / 126 / 73 / 227:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 227/277 [37:37<08:17,  9.95s/it][Succeeded / Failed / Skipped / Total] 28 / 126 / 73 / 227:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 228/277 [38:08<08:11, 10.04s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 73 / 228:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 228/277 [38:08<08:11, 10.04s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 73 / 228:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 229/277 [38:08<07:59,  9.99s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 74 / 229:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 229/277 [38:08<07:59,  9.99s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 74 / 229:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 230/277 [38:08<07:47,  9.95s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 75 / 230:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 230/277 [38:08<07:47,  9.95s/it][Succeeded / Failed / Skipped / Total] 28 / 127 / 75 / 230:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 231/277 [38:25<07:39,  9.98s/it][Succeeded / Failed / Skipped / Total] 28 / 128 / 75 / 231:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 231/277 [38:26<07:39,  9.98s/it][Succeeded / Failed / Skipped / Total] 28 / 128 / 75 / 231:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 232/277 [38:26<07:27,  9.94s/it][Succeeded / Failed / Skipped / Total] 28 / 128 / 76 / 232:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 232/277 [38:26<07:27,  9.94s/it][Succeeded / Failed / Skipped / Total] 28 / 128 / 76 / 232:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 233/277 [38:29<07:16,  9.91s/it][Succeeded / Failed / Skipped / Total] 29 / 128 / 76 / 233:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 233/277 [38:29<07:16,  9.91s/it][Succeeded / Failed / Skipped / Total] 29 / 128 / 76 / 233:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/277 [38:41<07:06,  9.92s/it][Succeeded / Failed / Skipped / Total] 29 / 129 / 76 / 234:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234/277 [38:41<07:06,  9.92s/it][Succeeded / Failed / Skipped / Total] 29 / 129 / 76 / 234:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 235/277 [38:56<06:57,  9.94s/it][Succeeded / Failed / Skipped / Total] 29 / 130 / 76 / 235:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 235/277 [38:56<06:57,  9.94s/it][Succeeded / Failed / Skipped / Total] 29 / 130 / 76 / 235:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 236/277 [38:56<06:45,  9.90s/it][Succeeded / Failed / Skipped / Total] 29 / 130 / 77 / 236:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 236/277 [38:56<06:45,  9.90s/it][Succeeded / Failed / Skipped / Total] 29 / 130 / 77 / 236:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 237/277 [38:59<06:34,  9.87s/it][Succeeded / Failed / Skipped / Total] 30 / 130 / 77 / 237:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 237/277 [38:59<06:34,  9.87s/it][Succeeded / Failed / Skipped / Total] 30 / 130 / 77 / 237:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 238/277 [39:18<06:26,  9.91s/it][Succeeded / Failed / Skipped / Total] 30 / 131 / 77 / 238:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 238/277 [39:18<06:26,  9.91s/it][Succeeded / Failed / Skipped / Total] 30 / 131 / 77 / 238:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 239/277 [39:18<06:15,  9.87s/it][Succeeded / Failed / Skipped / Total] 30 / 131 / 78 / 239:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 239/277 [39:18<06:15,  9.87s/it][Succeeded / Failed / Skipped / Total] 30 / 131 / 78 / 239:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 240/277 [39:42<06:07,  9.93s/it][Succeeded / Failed / Skipped / Total] 30 / 132 / 78 / 240:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 240/277 [39:42<06:07,  9.93s/it][Succeeded / Failed / Skipped / Total] 30 / 132 / 78 / 240:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 241/277 [39:42<05:55,  9.88s/it][Succeeded / Failed / Skipped / Total] 30 / 132 / 79 / 241:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 241/277 [39:42<05:55,  9.89s/it][Succeeded / Failed / Skipped / Total] 30 / 132 / 79 / 241:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 242/277 [39:54<05:46,  9.90s/it][Succeeded / Failed / Skipped / Total] 30 / 133 / 79 / 242:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 242/277 [39:54<05:46,  9.90s/it][Succeeded / Failed / Skipped / Total] 30 / 133 / 79 / 242:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 243/277 [40:08<05:36,  9.91s/it][Succeeded / Failed / Skipped / Total] 31 / 133 / 79 / 243:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 243/277 [40:08<05:36,  9.91s/it][Succeeded / Failed / Skipped / Total] 31 / 133 / 79 / 243:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 244/277 [40:08<05:25,  9.87s/it][Succeeded / Failed / Skipped / Total] 31 / 133 / 80 / 244:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 244/277 [40:08<05:25,  9.87s/it][Succeeded / Failed / Skipped / Total] 31 / 133 / 80 / 244:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 245/277 [40:31<05:17,  9.92s/it][Succeeded / Failed / Skipped / Total] 31 / 134 / 80 / 245:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 245/277 [40:31<05:17,  9.92s/it][Succeeded / Failed / Skipped / Total] 31 / 134 / 80 / 245:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/277 [40:31<05:06,  9.88s/it][Succeeded / Failed / Skipped / Total] 31 / 134 / 81 / 246:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 246/277 [40:31<05:06,  9.88s/it][Succeeded / Failed / Skipped / Total] 31 / 134 / 81 / 246:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 247/277 [40:41<04:56,  9.88s/it][Succeeded / Failed / Skipped / Total] 31 / 135 / 81 / 247:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 247/277 [40:41<04:56,  9.88s/it][Succeeded / Failed / Skipped / Total] 31 / 135 / 81 / 247:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 248/277 [40:47<04:46,  9.87s/it][Succeeded / Failed / Skipped / Total] 31 / 136 / 81 / 248:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 248/277 [40:47<04:46,  9.87s/it][Succeeded / Failed / Skipped / Total] 31 / 136 / 82 / 249:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 249/277 [40:47<04:35,  9.83s/it][Succeeded / Failed / Skipped / Total] 31 / 136 / 82 / 249:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 250/277 [41:03<04:26,  9.85s/it][Succeeded / Failed / Skipped / Total] 31 / 137 / 82 / 250:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 250/277 [41:03<04:26,  9.85s/it][Succeeded / Failed / Skipped / Total] 31 / 137 / 82 / 250:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 251/277 [41:14<04:16,  9.86s/it][Succeeded / Failed / Skipped / Total] 31 / 138 / 82 / 251:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 251/277 [41:14<04:16,  9.86s/it][Succeeded / Failed / Skipped / Total] 31 / 138 / 82 / 251:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 252/277 [41:36<04:07,  9.91s/it][Succeeded / Failed / Skipped / Total] 31 / 139 / 82 / 252:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 252/277 [41:36<04:07,  9.91s/it][Succeeded / Failed / Skipped / Total] 31 / 139 / 82 / 252:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 253/277 [41:57<03:58,  9.95s/it][Succeeded / Failed / Skipped / Total] 31 / 140 / 82 / 253:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 253/277 [41:57<03:58,  9.95s/it][Succeeded / Failed / Skipped / Total] 31 / 140 / 82 / 253:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 254/277 [42:05<03:48,  9.94s/it][Succeeded / Failed / Skipped / Total] 31 / 141 / 82 / 254:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 254/277 [42:05<03:48,  9.94s/it][Succeeded / Failed / Skipped / Total] 31 / 141 / 82 / 254:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255/277 [42:24<03:39,  9.98s/it][Succeeded / Failed / Skipped / Total] 31 / 142 / 82 / 255:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 255/277 [42:24<03:39,  9.98s/it][Succeeded / Failed / Skipped / Total] 31 / 142 / 82 / 255:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 256/277 [42:39<03:29, 10.00s/it][Succeeded / Failed / Skipped / Total] 31 / 143 / 82 / 256:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 256/277 [42:39<03:29, 10.00s/it][Succeeded / Failed / Skipped / Total] 31 / 143 / 82 / 256:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/277 [43:00<03:20, 10.04s/it][Succeeded / Failed / Skipped / Total] 31 / 144 / 82 / 257:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 257/277 [43:00<03:20, 10.04s/it][Succeeded / Failed / Skipped / Total] 31 / 144 / 82 / 257:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 258/277 [43:02<03:10, 10.01s/it][Succeeded / Failed / Skipped / Total] 32 / 144 / 82 / 258:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 258/277 [43:02<03:10, 10.01s/it][Succeeded / Failed / Skipped / Total] 32 / 144 / 82 / 258:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 259/277 [43:32<03:01, 10.09s/it][Succeeded / Failed / Skipped / Total] 32 / 145 / 82 / 259:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 259/277 [43:32<03:01, 10.09s/it][Succeeded / Failed / Skipped / Total] 32 / 145 / 82 / 259:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 260/277 [43:42<02:51, 10.09s/it][Succeeded / Failed / Skipped / Total] 32 / 146 / 82 / 260:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 260/277 [43:42<02:51, 10.09s/it][Succeeded / Failed / Skipped / Total] 32 / 146 / 82 / 260:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 261/277 [44:00<02:41, 10.12s/it][Succeeded / Failed / Skipped / Total] 32 / 147 / 82 / 261:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 261/277 [44:00<02:41, 10.12s/it][Succeeded / Failed / Skipped / Total] 32 / 147 / 82 / 261:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 262/277 [44:01<02:31, 10.08s/it][Succeeded / Failed / Skipped / Total] 32 / 147 / 83 / 262:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 262/277 [44:01<02:31, 10.08s/it][Succeeded / Failed / Skipped / Total] 32 / 147 / 83 / 262:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 263/277 [44:16<02:21, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 148 / 83 / 263:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 263/277 [44:16<02:21, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 148 / 83 / 263:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 264/277 [44:27<02:11, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 149 / 83 / 264:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 264/277 [44:27<02:11, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 149 / 83 / 264:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 265/277 [44:36<02:01, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 150 / 83 / 265:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 265/277 [44:36<02:01, 10.10s/it][Succeeded / Failed / Skipped / Total] 32 / 150 / 83 / 265:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 266/277 [44:52<01:51, 10.12s/it][Succeeded / Failed / Skipped / Total] 32 / 151 / 83 / 266:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 266/277 [44:52<01:51, 10.12s/it][Succeeded / Failed / Skipped / Total] 32 / 151 / 83 / 266:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/277 [44:58<01:41, 10.11s/it][Succeeded / Failed / Skipped / Total] 33 / 151 / 83 / 267:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 267/277 [44:58<01:41, 10.11s/it][Succeeded / Failed / Skipped / Total] 33 / 151 / 83 / 267:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/277 [45:14<01:31, 10.13s/it][Succeeded / Failed / Skipped / Total] 33 / 152 / 83 / 268:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 268/277 [45:14<01:31, 10.13s/it][Succeeded / Failed / Skipped / Total] 33 / 152 / 83 / 268:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 269/277 [45:17<01:20, 10.10s/it][Succeeded / Failed / Skipped / Total] 34 / 152 / 83 / 269:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 269/277 [45:17<01:20, 10.10s/it][Succeeded / Failed / Skipped / Total] 34 / 152 / 83 / 269:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 270/277 [45:17<01:10, 10.07s/it][Succeeded / Failed / Skipped / Total] 34 / 152 / 84 / 270:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 270/277 [45:17<01:10, 10.07s/it][Succeeded / Failed / Skipped / Total] 34 / 152 / 84 / 270:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 271/277 [45:28<01:00, 10.07s/it][Succeeded / Failed / Skipped / Total] 34 / 153 / 84 / 271:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 271/277 [45:28<01:00, 10.07s/it][Succeeded / Failed / Skipped / Total] 34 / 153 / 84 / 271:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 272/277 [45:35<00:50, 10.06s/it][Succeeded / Failed / Skipped / Total] 34 / 154 / 84 / 272:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 272/277 [45:35<00:50, 10.06s/it][Succeeded / Failed / Skipped / Total] 34 / 154 / 84 / 272:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 273/277 [45:35<00:40, 10.02s/it][Succeeded / Failed / Skipped / Total] 34 / 154 / 85 / 273:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 273/277 [45:35<00:40, 10.02s/it][Succeeded / Failed / Skipped / Total] 34 / 154 / 85 / 273:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 274/277 [45:40<00:30, 10.00s/it][Succeeded / Failed / Skipped / Total] 35 / 154 / 85 / 274:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 274/277 [45:40<00:30, 10.00s/it][Succeeded / Failed / Skipped / Total] 35 / 154 / 85 / 274:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 275/277 [45:57<00:20, 10.03s/it][Succeeded / Failed / Skipped / Total] 35 / 155 / 85 / 275:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 275/277 [45:57<00:20, 10.03s/it][Succeeded / Failed / Skipped / Total] 35 / 155 / 85 / 275: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 276/277 [45:57<00:09,  9.99s/it][Succeeded / Failed / Skipped / Total] 35 / 155 / 86 / 276: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 276/277 [45:58<00:09,  9.99s/it][Succeeded / Failed / Skipped / Total] 35 / 155 / 86 / 276: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [46:19<00:00, 10.03s/it][Succeeded / Failed / Skipped / Total] 35 / 156 / 86 / 277: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [46:19<00:00, 10.03s/it][Succeeded / Failed / Skipped / Total] 35 / 156 / 86 / 277: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [46:19<00:00, 10.03s/it]
Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 544kB/s]
Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]Downloading model.safetensors:   2%|â–         | 10.5M/548M [00:00<00:08, 61.4MB/s]Downloading model.safetensors:   4%|â–         | 21.0M/548M [00:00<00:08, 61.7MB/s]Downloading model.safetensors:   6%|â–Œ         | 31.5M/548M [00:00<00:08, 62.7MB/s]Downloading model.safetensors:   8%|â–Š         | 41.9M/548M [00:00<00:08, 58.4MB/s]Downloading model.safetensors:  10%|â–‰         | 52.4M/548M [00:00<00:08, 59.7MB/s]Downloading model.safetensors:  11%|â–ˆâ–        | 62.9M/548M [00:01<00:08, 59.1MB/s]Downloading model.safetensors:  13%|â–ˆâ–Ž        | 73.4M/548M [00:01<00:08, 57.9MB/s]Downloading model.safetensors:  15%|â–ˆâ–Œ        | 83.9M/548M [00:01<00:08, 56.0MB/s]Downloading model.safetensors:  17%|â–ˆâ–‹        | 94.4M/548M [00:01<00:07, 59.9MB/s]Downloading model.safetensors:  19%|â–ˆâ–‰        | 105M/548M [00:01<00:07, 61.8MB/s] Downloading model.safetensors:  21%|â–ˆâ–ˆ        | 115M/548M [00:01<00:06, 66.4MB/s]Downloading model.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 126M/548M [00:02<00:06, 70.3MB/s]Downloading model.safetensors:  25%|â–ˆâ–ˆâ–       | 136M/548M [00:02<00:05, 69.9MB/s]Downloading model.safetensors:  27%|â–ˆâ–ˆâ–‹       | 147M/548M [00:02<00:05, 71.9MB/s]Downloading model.safetensors:  29%|â–ˆâ–ˆâ–Š       | 157M/548M [00:02<00:05, 73.0MB/s]Downloading model.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 168M/548M [00:02<00:05, 70.5MB/s]Downloading model.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 178M/548M [00:02<00:05, 66.8MB/s]Downloading model.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 189M/548M [00:02<00:05, 68.9MB/s]Downloading model.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 199M/548M [00:03<00:04, 71.6MB/s]Downloading model.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210M/548M [00:03<00:04, 68.6MB/s]Downloading model.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 220M/548M [00:03<00:04, 69.9MB/s]Downloading model.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 231M/548M [00:03<00:04, 67.4MB/s]Downloading model.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241M/548M [00:03<00:04, 62.4MB/s]Downloading model.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 252M/548M [00:03<00:04, 63.1MB/s]Downloading model.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 262M/548M [00:04<00:04, 63.8MB/s]Downloading model.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 273M/548M [00:04<00:04, 65.3MB/s]Downloading model.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 283M/548M [00:04<00:03, 67.4MB/s]Downloading model.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 294M/548M [00:04<00:03, 66.5MB/s]Downloading model.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 304M/548M [00:04<00:03, 65.4MB/s]Downloading model.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315M/548M [00:04<00:03, 68.3MB/s]Downloading model.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 325M/548M [00:04<00:03, 70.3MB/s]Downloading model.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 336M/548M [00:05<00:03, 69.2MB/s]Downloading model.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 346M/548M [00:05<00:02, 68.5MB/s]Downloading model.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 357M/548M [00:05<00:02, 68.7MB/s]Downloading model.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 367M/548M [00:05<00:02, 71.9MB/s]Downloading model.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 377M/548M [00:05<00:02, 67.7MB/s]Downloading model.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 388M/548M [00:05<00:02, 63.5MB/s]Downloading model.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 398M/548M [00:06<00:02, 66.4MB/s]Downloading model.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 409M/548M [00:06<00:02, 65.6MB/s]Downloading model.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 419M/548M [00:06<00:02, 63.3MB/s]Downloading model.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 430M/548M [00:06<00:01, 59.3MB/s]Downloading model.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 440M/548M [00:06<00:01, 62.4MB/s]Downloading model.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 451M/548M [00:06<00:01, 65.6MB/s]Downloading model.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 461M/548M [00:07<00:01, 67.1MB/s]Downloading model.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 472M/548M [00:07<00:01, 65.4MB/s]Downloading model.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 482M/548M [00:07<00:00, 68.1MB/s]Downloading model.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 493M/548M [00:07<00:00, 71.5MB/s]Downloading model.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 503M/548M [00:07<00:00, 70.0MB/s]Downloading model.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 514M/548M [00:07<00:00, 74.8MB/s]Downloading model.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 524M/548M [00:07<00:00, 73.5MB/s]Downloading model.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 535M/548M [00:08<00:00, 72.1MB/s]Downloading model.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 545M/548M [00:08<00:00, 69.2MB/s]Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:08<00:00, 66.5MB/s]
Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]Downloading generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 10.5kB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.0/26.0 [00:00<00:00, 11.6kB/s]
Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]Downloading vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 3.45MB/s]Downloading vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 3.44MB/s]
Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]Downloading merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.08MB/s]Downloading merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.08MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 3.69MB/s]Downloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 3.68MB/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (18493 > 1024). Running this sequence through the model will result in indexing errors
2024-03-10 19:27:32.619231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string
	 [[{{node inputs}}]]
wandb: - 255.165 MB of 255.166 MB uploadedwandb: \ 255.166 MB of 255.176 MB uploadedwandb: | 255.166 MB of 255.176 MB uploadedwandb: / 255.176 MB of 255.176 MB uploaded2024-03-10 19:27:47.336801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 19:27:48.083365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 19:27:54.751833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.760534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.762982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.777362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.779782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.782150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.972830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.974515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.975988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:27:54.977474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 17.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.61s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13609.26 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
2024-03-10 19:30:21.625083: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 19:30:22.351953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 19:30:28.775955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.784182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.786555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.800737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.803108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.805461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.981866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.983440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.984898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:30:28.986354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:26<00:26, 26.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 17.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.51s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13928.38 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
1+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-4
2024-03-10 19:33:14.832233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 19:33:15.605174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 19:33:22.415948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.424733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.427271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.441407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.443818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.446183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.633735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.635409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.636880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:33:22.638381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 28.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 16.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.55s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 7692.51 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/rte/meta-llama/Llama-2-7b-hf/icl_attack/retrieval_icl-seed-1-shot-4_bm25/icl_attack_log.csv
  0%|          | 0/277 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-03-10 19:35:49.642664: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string
	 [[{{node inputs}}]]
2024-03-10 19:35:52.262102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
  0%|          | 1/277 [00:11<51:50, 11.27s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   0%|          | 1/277 [00:11<54:49, 11.92s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   1%|          | 2/277 [00:12<27:30,  6.00s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 1 / 2:   1%|          | 2/277 [00:12<27:36,  6.03s/it][Succeeded / Failed / Skipped / Total] 1 / 0 / 1 / 2:   1%|          | 3/277 [01:35<2:25:52, 31.94s/it][Succeeded / Failed / Skipped / Total] 1 / 1 / 1 / 3:   1%|          | 3/277 [01:35<2:25:59, 31.97s/it][Succeeded / Failed / Skipped / Total] 1 / 1 / 1 / 3:   1%|â–         | 4/277 [01:36<1:49:15, 24.01s/it][Succeeded / Failed / Skipped / Total] 1 / 1 / 2 / 4:   1%|â–         | 4/277 [01:36<1:49:22, 24.04s/it][Succeeded / Failed / Skipped / Total] 1 / 1 / 2 / 4:   2%|â–         | 5/277 [02:30<2:16:05, 30.02s/it][Succeeded / Failed / Skipped / Total] 1 / 2 / 2 / 5:   2%|â–         | 5/277 [02:30<2:16:09, 30.03s/it][Succeeded / Failed / Skipped / Total] 1 / 2 / 2 / 5:   2%|â–         | 6/277 [03:47<2:51:10, 37.90s/it][Succeeded / Failed / Skipped / Total] 1 / 3 / 2 / 6:   2%|â–         | 6/277 [03:47<2:51:14, 37.91s/it]wandb: - 0.336 MB of 0.336 MB uploadedwandb: \ 0.336 MB of 0.336 MB uploadedwandb: | 0.336 MB of 0.348 MB uploadedwandb: / 0.348 MB of 0.348 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 450, in attack
    result = self._attack(goal_function_result)
  File "/src/textattack/textattack/attack.py", line 398, in _attack
    final_result = self.search_method(initial_result)
  File "/src/textattack/textattack/search_methods/search_method.py", line 36, in __call__
    result = self.perform_search(initial_result)
  File "/src/textattack/textattack/search_methods/greedy_word_swap_wir.py", line 136, in perform_search
    index_order, search_over = self._get_index_order(attacked_text)
  File "/src/textattack/textattack/search_methods/greedy_word_swap_wir.py", line 101, in _get_index_order
    leave_one_results, search_over = self.get_goal_results(leave_one_texts)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 228, in forward
    outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, output_hidden_states = True, output_attentions = True)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 1041, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 928, in forward
    layer_outputs = decoder_layer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 653, in forward
    hidden_states = self.post_attention_layernorm(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/llama/modeling_llama.py", line 112, in forward
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 3.94 MiB is free. Process 3188795 has 39.38 GiB memory in use. Of the allocated memory 33.43 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-03-10 19:39:44.432272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 19:39:45.207111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 19:39:51.931577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:51.940356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:51.942749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:51.957244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:51.959626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:51.961972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:52.145670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:52.147260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:52.148708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:39:52.150165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 16.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.54s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 13815.92 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
2024-03-10 19:42:21.032042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-10 19:42:21.780226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-10 19:42:28.298480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.307196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.309606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.324413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.326875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.329218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.514431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.516049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.517503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-10 19:42:28.518971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0b:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:261: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 16.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 18.13s/it]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 14117.77 examples/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3500, in _map_single
    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 201, in attacker
    my_dataset = my_dataset[split].map(lambda x: convert_to_icl(x, icl_examples, verbalizer), batched=False)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3097, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py", line 3505, in _map_single
    writer.finalize()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 586, in finalize
    self.write_examples_on_file()
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 444, in write_examples_on_file
    batch_examples[col] = [
  File "/usr/local/lib/python3.8/dist-packages/datasets/arrow_writer.py", line 445, in <listcomp>
    row[0][col].to_pylist()[0] if isinstance(row[0][col], (pa.Array, pa.ChunkedArray)) else row[0][col]
KeyError: 'Premise_0'
