1+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8
textattack: Updating TextAttack package dependencies.
textattack: Downloading NLTK required packages.
2024-03-09 12:07:03.382777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:07:04.243097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package omw to /root/nltk_data...
[nltk_data] Downloading package universal_tagset to /root/nltk_data...
[nltk_data]   Unzipping taggers/universal_tagset.zip.
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.
  0%|          | 0.00/481M [00:00<?, ?B/s]  0%|          | 60.4k/481M [00:00<23:15, 345kB/s]  0%|          | 165k/481M [00:00<16:22, 490kB/s]   0%|          | 374k/481M [00:00<09:46, 821kB/s]  0%|          | 1.31M/481M [00:00<03:03, 2.62MB/s]  1%|          | 3.67M/481M [00:00<01:01, 7.72MB/s]  2%|▏         | 7.47M/481M [00:00<00:30, 15.5MB/s]  2%|▏         | 10.1M/481M [00:01<00:29, 16.0MB/s]  3%|▎         | 13.8M/481M [00:01<00:21, 21.4MB/s]  4%|▎         | 17.7M/481M [00:01<00:17, 26.0MB/s]  4%|▍         | 20.6M/481M [00:01<00:19, 23.1MB/s]  5%|▌         | 24.3M/481M [00:01<00:17, 26.6MB/s]  6%|▌         | 28.1M/481M [00:01<00:18, 25.2MB/s]  7%|▋         | 31.8M/481M [00:01<00:16, 28.1MB/s]  7%|▋         | 35.6M/481M [00:01<00:17, 26.1MB/s]  8%|▊         | 39.3M/481M [00:02<00:15, 28.7MB/s]  9%|▉         | 43.1M/481M [00:02<00:16, 26.5MB/s] 10%|▉         | 46.7M/481M [00:02<00:15, 28.9MB/s] 10%|█         | 50.5M/481M [00:02<00:16, 26.6MB/s] 11%|█         | 54.1M/481M [00:02<00:14, 28.9MB/s] 12%|█▏        | 57.9M/481M [00:02<00:15, 26.7MB/s] 13%|█▎        | 61.6M/481M [00:02<00:14, 29.1MB/s] 14%|█▎        | 65.4M/481M [00:03<00:15, 26.7MB/s] 14%|█▍        | 69.1M/481M [00:03<00:14, 29.4MB/s] 15%|█▌        | 72.8M/481M [00:03<00:15, 26.7MB/s] 16%|█▌        | 76.5M/481M [00:03<00:13, 29.1MB/s] 17%|█▋        | 80.2M/481M [00:03<00:15, 26.7MB/s] 17%|█▋        | 83.9M/481M [00:03<00:13, 29.1MB/s] 18%|█▊        | 87.6M/481M [00:03<00:14, 26.4MB/s] 19%|█▉        | 91.3M/481M [00:03<00:13, 28.9MB/s] 20%|█▉        | 94.9M/481M [00:04<00:14, 26.3MB/s] 20%|██        | 98.6M/481M [00:04<00:13, 28.9MB/s] 21%|██        | 102M/481M [00:04<00:14, 26.3MB/s]  22%|██▏       | 106M/481M [00:04<00:13, 28.5MB/s] 23%|██▎       | 109M/481M [00:04<00:14, 26.0MB/s] 23%|██▎       | 113M/481M [00:04<00:13, 28.1MB/s] 24%|██▍       | 116M/481M [00:04<00:14, 25.7MB/s] 25%|██▍       | 120M/481M [00:04<00:12, 28.1MB/s] 26%|██▌       | 124M/481M [00:05<00:14, 25.4MB/s] 26%|██▋       | 127M/481M [00:05<00:12, 27.8MB/s] 27%|██▋       | 130M/481M [00:05<00:13, 25.1MB/s] 28%|██▊       | 134M/481M [00:05<00:12, 27.6MB/s] 29%|██▊       | 137M/481M [00:05<00:13, 25.0MB/s] 29%|██▉       | 141M/481M [00:05<00:12, 27.3MB/s] 30%|██▉       | 144M/481M [00:05<00:13, 24.7MB/s] 31%|███       | 148M/481M [00:06<00:12, 27.3MB/s] 31%|███▏      | 151M/481M [00:06<00:13, 24.8MB/s] 32%|███▏      | 155M/481M [00:06<00:12, 27.2MB/s] 33%|███▎      | 158M/481M [00:06<00:13, 24.7MB/s] 34%|███▎      | 161M/481M [00:06<00:11, 26.9MB/s] 34%|███▍      | 165M/481M [00:06<00:12, 24.6MB/s] 35%|███▍      | 168M/481M [00:06<00:11, 27.0MB/s] 36%|███▌      | 172M/481M [00:07<00:12, 24.5MB/s] 36%|███▋      | 175M/481M [00:07<00:11, 27.2MB/s] 37%|███▋      | 179M/481M [00:07<00:12, 24.6MB/s] 38%|███▊      | 182M/481M [00:07<00:11, 27.1MB/s] 39%|███▊      | 185M/481M [00:07<00:12, 24.6MB/s] 39%|███▉      | 189M/481M [00:07<00:10, 26.9MB/s] 40%|███▉      | 192M/481M [00:07<00:11, 24.4MB/s] 41%|████      | 196M/481M [00:07<00:10, 27.0MB/s] 41%|████▏     | 199M/481M [00:08<00:11, 24.5MB/s] 42%|████▏     | 203M/481M [00:08<00:10, 26.9MB/s] 43%|████▎     | 206M/481M [00:08<00:11, 24.5MB/s] 44%|████▎     | 210M/481M [00:08<00:10, 27.0MB/s] 44%|████▍     | 213M/481M [00:08<00:10, 24.5MB/s] 45%|████▍     | 216M/481M [00:08<00:09, 27.0MB/s] 46%|████▌     | 220M/481M [00:08<00:10, 24.6MB/s] 46%|████▋     | 223M/481M [00:08<00:09, 26.6MB/s] 47%|████▋     | 227M/481M [00:09<00:10, 24.5MB/s] 48%|████▊     | 230M/481M [00:09<00:09, 26.9MB/s] 48%|████▊     | 233M/481M [00:09<00:10, 24.6MB/s] 49%|████▉     | 237M/481M [00:09<00:09, 27.0MB/s] 50%|████▉     | 240M/481M [00:09<00:09, 24.6MB/s] 51%|█████     | 244M/481M [00:09<00:08, 27.0MB/s] 51%|█████▏    | 247M/481M [00:09<00:09, 24.7MB/s] 52%|█████▏    | 251M/481M [00:10<00:08, 26.9MB/s] 53%|█████▎    | 254M/481M [00:10<00:09, 24.7MB/s] 54%|█████▎    | 258M/481M [00:10<00:08, 27.2MB/s] 54%|█████▍    | 261M/481M [00:10<00:08, 24.7MB/s] 55%|█████▍    | 265M/481M [00:10<00:08, 27.0MB/s] 56%|█████▌    | 268M/481M [00:10<00:07, 30.0MB/s] 56%|█████▋    | 272M/481M [00:10<00:07, 26.4MB/s] 57%|█████▋    | 275M/481M [00:10<00:07, 29.4MB/s] 58%|█████▊    | 279M/481M [00:11<00:06, 31.1MB/s] 59%|█████▊    | 282M/481M [00:11<00:07, 27.3MB/s] 59%|█████▉    | 286M/481M [00:11<00:06, 29.4MB/s] 60%|██████    | 289M/481M [00:11<00:06, 30.9MB/s] 61%|██████    | 293M/481M [00:11<00:06, 27.3MB/s] 62%|██████▏   | 296M/481M [00:11<00:06, 29.3MB/s] 62%|██████▏   | 300M/481M [00:11<00:05, 32.2MB/s] 63%|██████▎   | 304M/481M [00:11<00:06, 28.2MB/s] 64%|██████▍   | 307M/481M [00:12<00:05, 29.5MB/s] 65%|██████▍   | 311M/481M [00:12<00:05, 32.1MB/s] 65%|██████▌   | 314M/481M [00:12<00:05, 28.1MB/s] 66%|██████▌   | 318M/481M [00:12<00:05, 29.6MB/s] 67%|██████▋   | 322M/481M [00:12<00:05, 27.9MB/s] 68%|██████▊   | 325M/481M [00:12<00:05, 30.0MB/s] 68%|██████▊   | 329M/481M [00:12<00:04, 32.2MB/s] 69%|██████▉   | 333M/481M [00:12<00:05, 28.3MB/s] 70%|██████▉   | 336M/481M [00:12<00:04, 30.1MB/s] 71%|███████   | 340M/481M [00:13<00:04, 32.8MB/s] 71%|███████▏  | 344M/481M [00:13<00:04, 28.8MB/s] 72%|███████▏  | 347M/481M [00:13<00:04, 29.9MB/s] 73%|███████▎  | 351M/481M [00:13<00:04, 32.4MB/s] 74%|███████▎  | 354M/481M [00:13<00:04, 28.5MB/s] 74%|███████▍  | 358M/481M [00:13<00:04, 29.8MB/s] 75%|███████▌  | 362M/481M [00:13<00:03, 32.5MB/s] 76%|███████▌  | 365M/481M [00:13<00:04, 28.5MB/s] 77%|███████▋  | 369M/481M [00:14<00:03, 30.0MB/s] 77%|███████▋  | 372M/481M [00:14<00:03, 32.5MB/s] 78%|███████▊  | 376M/481M [00:14<00:03, 28.5MB/s] 79%|███████▉  | 379M/481M [00:14<00:03, 29.9MB/s] 80%|███████▉  | 383M/481M [00:14<00:03, 28.1MB/s] 80%|████████  | 387M/481M [00:14<00:03, 31.1MB/s] 81%|████████▏ | 391M/481M [00:14<00:02, 33.3MB/s] 82%|████████▏ | 395M/481M [00:14<00:02, 29.2MB/s] 83%|████████▎ | 399M/481M [00:15<00:02, 32.2MB/s] 84%|████████▎ | 403M/481M [00:15<00:02, 34.0MB/s] 84%|████████▍ | 406M/481M [00:15<00:02, 29.8MB/s] 85%|████████▌ | 410M/481M [00:15<00:02, 32.8MB/s] 86%|████████▌ | 414M/481M [00:15<00:01, 34.2MB/s] 87%|████████▋ | 418M/481M [00:15<00:02, 30.0MB/s] 88%|████████▊ | 422M/481M [00:15<00:01, 32.6MB/s] 88%|████████▊ | 426M/481M [00:15<00:01, 34.1MB/s] 89%|████████▉ | 429M/481M [00:16<00:01, 29.9MB/s] 90%|████████▉ | 433M/481M [00:16<00:01, 32.5MB/s] 91%|█████████ | 437M/481M [00:16<00:01, 34.5MB/s] 92%|█████████▏| 441M/481M [00:16<00:01, 30.3MB/s] 92%|█████████▏| 445M/481M [00:16<00:01, 32.7MB/s] 93%|█████████▎| 449M/481M [00:16<00:00, 34.4MB/s] 94%|█████████▍| 452M/481M [00:16<00:00, 30.2MB/s] 95%|█████████▍| 456M/481M [00:16<00:00, 32.8MB/s] 96%|█████████▌| 460M/481M [00:16<00:00, 34.3MB/s] 96%|█████████▋| 464M/481M [00:17<00:00, 30.2MB/s] 97%|█████████▋| 468M/481M [00:17<00:00, 32.1MB/s] 98%|█████████▊| 471M/481M [00:17<00:00, 33.7MB/s] 99%|█████████▊| 475M/481M [00:17<00:00, 30.4MB/s]100%|█████████▉| 479M/481M [00:17<00:00, 32.8MB/s]100%|██████████| 481M/481M [00:17<00:00, 27.3MB/s]
textattack: Unzipping file /root/.cache/textattack/tmp5rcczer3.zip to /root/.cache/textattack/word_embeddings/paragramcf.
textattack: Successfully saved word_embeddings/paragramcf to cache.
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]Downloading builder script: 4.21kB [00:00, 15.7MB/s]                   2024-03-09 12:07:35.056836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.066839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.069214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.084879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.087194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.089483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.266525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.268024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.269384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:07:35.270745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0

Downloading builder script:   0%|          | 0.00/5.09k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 5.09k/5.09k [00:00<00:00, 35.1MB/s]
Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 54.3MB/s]
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/336k [00:00<?, ?B/s][A
Downloading data:   2%|▏         | 8.19k/336k [00:00<00:04, 80.7kB/s][A
Downloading data:  14%|█▍        | 48.1k/336k [00:00<00:01, 263kB/s] [A
Downloading data:  29%|██▊       | 96.3k/336k [00:00<00:00, 356kB/s][A
Downloading data:  45%|████▌     | 153k/336k [00:00<00:00, 431kB/s] [A
Downloading data:  62%|██████▏   | 209k/336k [00:00<00:00, 473kB/s][A
Downloading data:  77%|███████▋  | 257k/336k [00:00<00:00, 471kB/s][ADownloading data: 100%|██████████| 336k/336k [00:00<00:00, 543kB/s]
Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]
Downloading data:   0%|          | 0.00/23.4k [00:00<?, ?B/s][ADownloading data: 100%|██████████| 23.4k/23.4k [00:00<00:00, 250kB/s]
Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Generating train split:   0%|          | 0/5452 [00:00<?, ? examples/s]Generating train split:  73%|███████▎  | 3972/5452 [00:00<00:00, 39630.75 examples/s]Generating train split: 100%|██████████| 5452/5452 [00:00<00:00, 39355.82 examples/s]
Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 500/500 [00:00<00:00, 36803.76 examples/s]
Map:   0%|          | 0/5452 [00:00<?, ? examples/s]Map:  37%|███▋      | 2002/5452 [00:00<00:00, 19691.52 examples/s]Map:  74%|███████▍  | 4060/5452 [00:00<00:00, 19915.01 examples/s]Map: 100%|██████████| 5452/5452 [00:00<00:00, 19949.74 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 18194.65 examples/s]
Map:   0%|          | 0/5179 [00:00<?, ? examples/s]Map:  24%|██▎       | 1222/5179 [00:00<00:00, 12122.38 examples/s]Map:  48%|████▊     | 2489/5179 [00:00<00:00, 12436.31 examples/s]Map:  84%|████████▍ | 4344/5179 [00:00<00:00, 12390.59 examples/s]Map: 100%|██████████| 5179/5179 [00:00<00:00, 12282.94 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 18175.26 examples/s]
Map:   0%|          | 0/273 [00:00<?, ? examples/s]Map: 100%|██████████| 273/273 [00:00<00:00, 10581.50 examples/s]
Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 105kB/s]
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.6MB/s]
Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.00MB/s]Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.99MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 611kB/s]
Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 609/609 [00:00<00:00, 94.8kB/s]
Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 28.2MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   0%|          | 10.5M/9.98G [00:00<01:37, 102MB/s][A
Downloading (…)of-00002.safetensors:   0%|          | 31.5M/9.98G [00:00<01:11, 140MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 62.9M/9.98G [00:00<00:51, 193MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 94.4M/9.98G [00:00<00:44, 220MB/s][A
Downloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:41, 235MB/s] [A
Downloading (…)of-00002.safetensors:   2%|▏         | 157M/9.98G [00:00<00:40, 245MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 189M/9.98G [00:00<00:39, 251MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 220M/9.98G [00:00<00:38, 255MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 252M/9.98G [00:01<00:37, 257MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 283M/9.98G [00:01<00:37, 259MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 315M/9.98G [00:01<00:37, 259MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 346M/9.98G [00:01<00:36, 261MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 377M/9.98G [00:01<00:36, 262MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 409M/9.98G [00:01<00:36, 262MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 440M/9.98G [00:01<00:36, 262MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 472M/9.98G [00:01<00:36, 260MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 503M/9.98G [00:02<00:36, 261MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 535M/9.98G [00:02<00:35, 263MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 566M/9.98G [00:02<00:35, 262MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 598M/9.98G [00:02<00:38, 246MB/s][A
Downloading (…)of-00002.safetensors:   6%|▋         | 629M/9.98G [00:02<00:46, 199MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 661M/9.98G [00:02<00:50, 184MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:02<00:49, 187MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 703M/9.98G [00:03<00:56, 164MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 724M/9.98G [00:03<00:56, 164MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 744M/9.98G [00:03<00:58, 157MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 765M/9.98G [00:03<01:03, 144MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 786M/9.98G [00:03<01:02, 148MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 807M/9.98G [00:03<01:02, 148MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 828M/9.98G [00:03<00:57, 158MB/s][A
Downloading (…)of-00002.safetensors:   9%|▊         | 849M/9.98G [00:04<00:57, 159MB/s][A
Downloading (…)of-00002.safetensors:   9%|▊         | 870M/9.98G [00:04<01:00, 151MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 891M/9.98G [00:04<01:03, 142MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 923M/9.98G [00:04<00:55, 163MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 944M/9.98G [00:04<01:01, 148MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 965M/9.98G [00:04<00:59, 151MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 986M/9.98G [00:05<00:57, 156MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 1.01G/9.98G [00:05<00:55, 161MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:05<00:50, 176MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.06G/9.98G [00:05<00:52, 170MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.08G/9.98G [00:05<00:50, 175MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:05<00:53, 165MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 1.12G/9.98G [00:05<00:51, 171MB/s][A
Downloading (…)of-00002.safetensors:  11%|█▏        | 1.14G/9.98G [00:05<00:49, 179MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.17G/9.98G [00:06<00:45, 192MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.20G/9.98G [00:06<00:49, 179MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.22G/9.98G [00:06<00:52, 166MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 1.24G/9.98G [00:06<00:50, 173MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:06<00:53, 163MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.28G/9.98G [00:06<00:53, 161MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.30G/9.98G [00:06<00:54, 161MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.32G/9.98G [00:06<00:55, 155MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 1.34G/9.98G [00:07<00:57, 149MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:07<00:59, 146MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.38G/9.98G [00:07<00:57, 150MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:07<00:57, 148MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 1.44G/9.98G [00:07<00:53, 160MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 1.46G/9.98G [00:07<00:54, 156MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 1.48G/9.98G [00:08<00:52, 160MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:08<00:47, 178MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:08<00:52, 161MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.55G/9.98G [00:08<00:51, 162MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:08<00:50, 168MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.59G/9.98G [00:08<00:47, 175MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 1.61G/9.98G [00:08<00:49, 169MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:08<00:50, 165MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.66G/9.98G [00:09<00:50, 164MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.68G/9.98G [00:09<00:52, 158MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:09<01:02, 133MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:09<00:56, 147MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:09<00:57, 143MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.76G/9.98G [00:09<00:55, 149MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:09<00:56, 144MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.80G/9.98G [00:10<00:55, 147MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.82G/9.98G [00:10<01:00, 135MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:10<01:01, 133MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▊        | 1.87G/9.98G [00:10<01:02, 130MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.89G/9.98G [00:10<01:01, 131MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:10<00:59, 136MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 1.93G/9.98G [00:11<01:00, 133MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.95G/9.98G [00:11<01:01, 132MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.97G/9.98G [00:11<01:01, 131MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:11<01:00, 132MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.01G/9.98G [00:11<00:58, 137MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:11<01:01, 129MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.06G/9.98G [00:12<01:00, 131MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.08G/9.98G [00:12<01:00, 130MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:12<00:58, 135MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 2.12G/9.98G [00:12<00:58, 134MB/s][A
Downloading (…)of-00002.safetensors:  21%|██▏       | 2.14G/9.98G [00:12<00:56, 138MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:12<00:58, 134MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.18G/9.98G [00:12<00:56, 138MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:13<00:58, 132MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.22G/9.98G [00:13<01:05, 119MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 2.24G/9.98G [00:13<01:05, 119MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:13<01:02, 123MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.29G/9.98G [00:13<01:03, 121MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:14<01:02, 122MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 2.33G/9.98G [00:14<01:01, 124MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:14<01:00, 126MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.37G/9.98G [00:14<01:00, 127MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:14<00:57, 131MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:14<00:59, 128MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 2.43G/9.98G [00:15<00:59, 126MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 2.45G/9.98G [00:15<01:02, 121MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 2.47G/9.98G [00:15<01:02, 121MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.50G/9.98G [00:15<01:02, 121MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:15<00:59, 126MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 2.54G/9.98G [00:15<00:56, 131MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.56G/9.98G [00:15<00:55, 135MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.58G/9.98G [00:16<00:57, 129MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 2.60G/9.98G [00:16<00:54, 134MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:16<00:58, 125MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 2.64G/9.98G [00:16<00:57, 128MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.66G/9.98G [00:16<00:59, 124MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.68G/9.98G [00:17<00:57, 126MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.71G/9.98G [00:17<01:00, 121MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:17<00:56, 128MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.75G/9.98G [00:17<00:56, 128MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:17<00:54, 132MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.79G/9.98G [00:17<00:56, 126MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.81G/9.98G [00:17<00:55, 130MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 2.83G/9.98G [00:18<00:53, 134MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:18<00:54, 130MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.87G/9.98G [00:18<00:56, 126MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.89G/9.98G [00:18<00:59, 120MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:18<00:54, 129MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:18<00:55, 126MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.96G/9.98G [00:19<01:14, 94.4MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 2.99G/9.98G [00:19<00:57, 122MB/s] [A
Downloading (…)of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:19<00:49, 140MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 3.04G/9.98G [00:19<00:49, 139MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.06G/9.98G [00:19<00:49, 140MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.08G/9.98G [00:20<00:49, 139MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:20<00:51, 133MB/s][A
Downloading (…)of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:20<00:51, 133MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.15G/9.98G [00:20<00:53, 129MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.17G/9.98G [00:20<00:55, 124MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.19G/9.98G [00:20<00:56, 120MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.21G/9.98G [00:21<00:53, 127MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:21<00:53, 125MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.25G/9.98G [00:21<00:53, 126MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.27G/9.98G [00:21<00:59, 113MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.29G/9.98G [00:21<00:56, 119MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.31G/9.98G [00:21<00:54, 123MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 3.33G/9.98G [00:22<00:58, 114MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:22<00:56, 117MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.38G/9.98G [00:22<01:02, 105MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.40G/9.98G [00:22<00:55, 118MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.42G/9.98G [00:22<00:53, 122MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:23<00:48, 133MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.46G/9.98G [00:23<00:46, 139MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:23<00:45, 143MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 3.50G/9.98G [00:23<00:48, 134MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:23<00:47, 135MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:23<00:47, 136MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.57G/9.98G [00:23<00:47, 134MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.59G/9.98G [00:24<00:45, 140MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 3.61G/9.98G [00:24<00:45, 139MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▋      | 3.63G/9.98G [00:24<00:51, 123MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:24<00:52, 121MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.67G/9.98G [00:24<01:00, 104MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.69G/9.98G [00:25<00:58, 108MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.71G/9.98G [00:25<00:53, 116MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:25<00:51, 122MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.75G/9.98G [00:25<00:50, 123MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:25<00:49, 124MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.80G/9.98G [00:25<00:46, 132MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.82G/9.98G [00:26<00:46, 132MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:26<00:45, 136MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▊      | 3.86G/9.98G [00:26<00:45, 134MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:26<00:44, 136MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:26<00:43, 140MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:26<00:45, 133MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:26<00:44, 136MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.96G/9.98G [00:27<00:50, 119MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:27<00:47, 127MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 4.01G/9.98G [00:27<00:53, 112MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:27<00:51, 116MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:27<00:49, 120MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.07G/9.98G [00:28<00:48, 121MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:28<00:47, 125MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:28<00:47, 124MB/s][A
Downloading (…)of-00002.safetensors:  41%|████▏     | 4.13G/9.98G [00:28<00:46, 125MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:28<00:45, 128MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.17G/9.98G [00:28<00:47, 123MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:29<00:47, 122MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:29<00:49, 117MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:29<00:46, 123MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.26G/9.98G [00:29<00:51, 111MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.28G/9.98G [00:29<00:50, 114MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:29<00:48, 117MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 4.32G/9.98G [00:30<00:45, 123MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▎     | 4.34G/9.98G [00:30<00:48, 115MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:30<00:46, 121MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.38G/9.98G [00:30<00:45, 123MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:30<00:45, 124MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:30<00:47, 117MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.45G/9.98G [00:31<00:45, 122MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:31<00:46, 120MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 4.49G/9.98G [00:31<00:45, 120MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:31<00:44, 124MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 4.53G/9.98G [00:31<00:42, 128MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:31<00:42, 128MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.57G/9.98G [00:32<00:41, 132MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:32<00:41, 130MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:32<00:41, 130MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:32<00:41, 130MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.66G/9.98G [00:32<00:40, 132MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:32<00:41, 129MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.70G/9.98G [00:33<00:45, 116MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 4.72G/9.98G [00:33<00:45, 115MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:33<00:48, 108MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.76G/9.98G [00:33<00:46, 112MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:33<00:42, 121MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.80G/9.98G [00:34<00:40, 126MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 4.82G/9.98G [00:34<00:42, 120MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▊     | 4.84G/9.98G [00:34<00:42, 121MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:34<00:41, 123MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.89G/9.98G [00:34<00:41, 122MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:34<00:38, 130MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:35<00:51, 98.0MB/s][A
Downloading (…)of-00002.safetensors:  50%|████▉     | 4.96G/9.98G [00:35<00:40, 124MB/s] [A
Downloading (…)of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:35<00:36, 138MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 5.00G/9.98G [00:35<00:35, 138MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:35<00:35, 140MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.04G/9.98G [00:35<00:35, 140MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.06G/9.98G [00:36<00:35, 139MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:36<00:36, 134MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 5.11G/9.98G [00:36<00:36, 133MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████▏    | 5.13G/9.98G [00:36<00:37, 130MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:36<00:37, 127MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.17G/9.98G [00:36<00:39, 120MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.19G/9.98G [00:37<00:37, 128MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:37<00:37, 128MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.23G/9.98G [00:37<00:40, 117MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.25G/9.98G [00:37<00:39, 121MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.27G/9.98G [00:37<00:37, 127MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:37<00:37, 127MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.32G/9.98G [00:38<00:35, 131MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:38<00:34, 135MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▎    | 5.36G/9.98G [00:38<00:33, 137MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.38G/9.98G [00:38<00:32, 141MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.40G/9.98G [00:38<00:33, 138MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.42G/9.98G [00:38<00:34, 132MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.44G/9.98G [00:39<00:36, 125MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.46G/9.98G [00:39<00:33, 133MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.48G/9.98G [00:39<00:34, 132MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.51G/9.98G [00:39<00:33, 135MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.53G/9.98G [00:39<00:34, 129MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.55G/9.98G [00:39<00:35, 126MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.57G/9.98G [00:39<00:33, 132MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:40<00:32, 133MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.61G/9.98G [00:40<00:32, 136MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▋    | 5.63G/9.98G [00:40<00:32, 132MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.65G/9.98G [00:40<00:33, 128MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.67G/9.98G [00:40<00:33, 128MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.69G/9.98G [00:40<00:32, 131MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.71G/9.98G [00:41<00:32, 132MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.74G/9.98G [00:41<00:30, 137MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.76G/9.98G [00:41<00:30, 140MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.78G/9.98G [00:41<00:31, 134MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.80G/9.98G [00:41<00:29, 140MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.82G/9.98G [00:41<00:29, 140MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▊    | 5.84G/9.98G [00:41<00:28, 143MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:42<00:29, 139MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.88G/9.98G [00:42<00:29, 138MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.90G/9.98G [00:42<00:28, 141MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:42<00:29, 137MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.95G/9.98G [00:42<00:29, 138MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.97G/9.98G [00:42<00:30, 132MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 5.99G/9.98G [00:43<00:29, 134MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 6.01G/9.98G [00:43<00:29, 136MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:43<00:28, 141MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.05G/9.98G [00:43<00:28, 137MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.07G/9.98G [00:43<00:30, 130MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 6.09G/9.98G [00:43<00:29, 131MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 6.11G/9.98G [00:43<00:28, 136MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 6.13G/9.98G [00:44<00:29, 131MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:44<00:29, 129MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.18G/9.98G [00:44<00:30, 125MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.20G/9.98G [00:44<00:30, 123MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:44<00:30, 124MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.24G/9.98G [00:45<00:29, 127MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.26G/9.98G [00:45<00:30, 120MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.28G/9.98G [00:45<00:29, 125MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.30G/9.98G [00:45<00:28, 127MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.32G/9.98G [00:45<00:31, 118MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:45<00:30, 120MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.36G/9.98G [00:46<00:28, 126MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.39G/9.98G [00:46<00:28, 127MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.41G/9.98G [00:46<00:28, 123MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.43G/9.98G [00:46<00:31, 113MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:46<00:33, 104MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.47G/9.98G [00:47<00:33, 105MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.49G/9.98G [00:47<00:32, 106MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.51G/9.98G [00:47<00:31, 111MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.53G/9.98G [00:47<00:29, 115MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.55G/9.98G [00:47<00:28, 121MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.57G/9.98G [00:47<00:27, 124MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.60G/9.98G [00:48<00:28, 119MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▋   | 6.62G/9.98G [00:48<00:29, 114MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.64G/9.98G [00:48<00:29, 115MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.66G/9.98G [00:48<00:30, 110MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.68G/9.98G [00:48<00:32, 103MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.70G/9.98G [00:49<00:28, 114MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.72G/9.98G [00:49<00:30, 107MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.74G/9.98G [00:49<00:29, 109MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.76G/9.98G [00:49<00:26, 123MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.78G/9.98G [00:49<00:24, 132MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.81G/9.98G [00:49<00:25, 122MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.83G/9.98G [00:50<00:25, 122MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▊   | 6.85G/9.98G [00:50<00:26, 118MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.87G/9.98G [00:50<00:26, 117MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:50<00:27, 113MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:50<00:25, 119MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.93G/9.98G [00:50<00:25, 119MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:51<00:24, 124MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:51<00:23, 128MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 6.99G/9.98G [00:51<00:23, 125MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 7.01G/9.98G [00:51<00:23, 127MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.04G/9.98G [00:51<00:24, 122MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.06G/9.98G [00:51<00:23, 125MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.08G/9.98G [00:52<00:24, 121MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 7.10G/9.98G [00:52<00:24, 117MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:52<00:23, 119MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.14G/9.98G [00:52<00:22, 123MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:52<00:22, 127MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.18G/9.98G [00:52<00:22, 127MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.20G/9.98G [00:53<00:22, 121MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:53<00:22, 125MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.25G/9.98G [00:53<00:22, 119MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.27G/9.98G [00:53<00:22, 121MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.29G/9.98G [00:53<00:22, 122MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.31G/9.98G [00:54<00:22, 121MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.33G/9.98G [00:54<00:21, 123MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:54<00:21, 122MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.37G/9.98G [00:54<00:28, 92.9MB/s][A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.40G/9.98G [00:54<00:20, 123MB/s] [A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.43G/9.98G [00:55<00:19, 133MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.46G/9.98G [00:55<00:19, 129MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:55<00:19, 127MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.50G/9.98G [00:55<00:19, 129MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:55<00:19, 129MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:55<00:18, 131MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.56G/9.98G [00:56<00:18, 129MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:56<00:18, 127MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:56<00:18, 128MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▋  | 7.62G/9.98G [00:56<00:18, 124MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:56<00:18, 127MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.67G/9.98G [00:56<00:17, 129MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.69G/9.98G [00:57<00:17, 132MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.71G/9.98G [00:57<00:17, 128MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.73G/9.98G [00:57<00:17, 127MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.75G/9.98G [00:57<00:17, 126MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.77G/9.98G [00:57<00:16, 131MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.79G/9.98G [00:57<00:16, 135MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.81G/9.98G [00:57<00:15, 137MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▊  | 7.83G/9.98G [00:58<00:16, 132MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▊  | 7.85G/9.98G [00:58<00:17, 124MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:58<00:16, 125MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.90G/9.98G [00:58<00:16, 125MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:58<00:15, 132MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.94G/9.98G [00:58<00:15, 129MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:59<00:15, 132MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.98G/9.98G [00:59<00:15, 129MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 8.00G/9.98G [00:59<00:17, 116MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 8.02G/9.98G [00:59<00:17, 114MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.04G/9.98G [00:59<00:17, 114MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.06G/9.98G [01:00<00:15, 120MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.08G/9.98G [01:00<00:14, 128MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 8.11G/9.98G [01:00<00:15, 124MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████▏ | 8.13G/9.98G [01:00<00:15, 123MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.15G/9.98G [01:00<00:15, 122MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.17G/9.98G [01:00<00:14, 124MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [01:01<00:15, 117MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.21G/9.98G [01:01<00:14, 121MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.23G/9.98G [01:01<00:16, 108MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [01:01<00:15, 110MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.27G/9.98G [01:01<00:14, 116MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.29G/9.98G [01:01<00:13, 121MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [01:02<00:13, 121MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▎ | 8.34G/9.98G [01:02<00:13, 121MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.36G/9.98G [01:02<00:13, 117MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.38G/9.98G [01:02<00:13, 121MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [01:02<00:12, 129MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [01:02<00:12, 128MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [01:03<00:12, 125MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [01:03<00:13, 115MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [01:03<00:12, 116MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [01:03<00:12, 122MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [01:03<00:11, 128MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [01:04<00:11, 124MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [01:04<00:11, 125MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [01:04<00:10, 130MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [01:04<00:11, 118MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [01:04<00:11, 122MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [01:04<00:11, 119MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [01:05<00:10, 120MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [01:05<00:11, 115MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [01:05<00:11, 112MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [01:05<00:11, 110MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [01:05<00:10, 114MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [01:06<00:10, 118MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [01:06<00:09, 125MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [01:06<00:09, 126MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [01:06<00:09, 125MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [01:06<00:09, 116MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.88G/9.98G [01:06<00:08, 124MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [01:07<00:08, 121MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [01:07<00:08, 122MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [01:07<00:08, 124MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [01:07<00:07, 128MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [01:07<00:07, 129MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [01:07<00:07, 130MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [01:07<00:07, 127MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [01:08<00:07, 130MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [01:08<00:07, 129MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [01:08<00:07, 124MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████▏| 9.11G/9.98G [01:08<00:06, 130MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [01:08<00:06, 125MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [01:08<00:06, 125MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.18G/9.98G [01:09<00:06, 129MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [01:09<00:06, 128MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [01:09<00:06, 124MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [01:09<00:05, 130MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [01:09<00:05, 129MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [01:10<00:05, 118MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.30G/9.98G [01:10<00:05, 120MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.32G/9.98G [01:10<00:05, 122MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [01:10<00:05, 119MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [01:10<00:05, 112MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.38G/9.98G [01:10<00:05, 116MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [01:11<00:04, 118MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [01:11<00:04, 118MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [01:11<00:04, 122MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [01:11<00:04, 127MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [01:11<00:03, 122MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [01:11<00:03, 119MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [01:12<00:03, 114MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [01:12<00:03, 117MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [01:12<00:03, 113MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [01:12<00:03, 117MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [01:12<00:03, 120MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [01:13<00:03, 110MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [01:13<00:03, 92.6MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [01:13<00:02, 109MB/s] [A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.70G/9.98G [01:13<00:02, 116MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.72G/9.98G [01:13<00:02, 118MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [01:14<00:02, 106MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [01:14<00:01, 109MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [01:14<00:01, 110MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.80G/9.98G [01:14<00:01, 112MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.83G/9.98G [01:14<00:01, 104MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▊| 9.85G/9.98G [01:15<00:01, 103MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [01:15<00:01, 103MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [01:15<00:00, 108MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.91G/9.98G [01:15<00:00, 108MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [01:15<00:00, 116MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.95G/9.98G [01:16<00:00, 106MB/s][A
Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [01:16<00:00, 109MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [01:16<00:00, 131MB/s]
Downloading shards:  50%|█████     | 1/2 [01:16<01:16, 76.35s/it]
Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
Downloading (…)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:26, 132MB/s][A
Downloading (…)of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:25, 135MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 62.9M/3.50G [00:00<00:27, 127MB/s][A
Downloading (…)of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:27, 126MB/s][A
Downloading (…)of-00002.safetensors:   3%|▎         | 105M/3.50G [00:00<00:27, 123MB/s] [A
Downloading (…)of-00002.safetensors:   4%|▎         | 126M/3.50G [00:00<00:27, 125MB/s][A
Downloading (…)of-00002.safetensors:   4%|▍         | 147M/3.50G [00:01<00:28, 118MB/s][A
Downloading (…)of-00002.safetensors:   5%|▍         | 168M/3.50G [00:01<00:26, 126MB/s][A
Downloading (…)of-00002.safetensors:   5%|▌         | 189M/3.50G [00:01<00:25, 130MB/s][A
Downloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:01<00:26, 124MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 231M/3.50G [00:01<00:26, 124MB/s][A
Downloading (…)of-00002.safetensors:   7%|▋         | 252M/3.50G [00:02<00:26, 122MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 273M/3.50G [00:02<00:26, 120MB/s][A
Downloading (…)of-00002.safetensors:   8%|▊         | 294M/3.50G [00:02<00:26, 121MB/s][A
Downloading (…)of-00002.safetensors:   9%|▉         | 315M/3.50G [00:02<00:26, 118MB/s][A
Downloading (…)of-00002.safetensors:  10%|▉         | 336M/3.50G [00:02<00:26, 119MB/s][A
Downloading (…)of-00002.safetensors:  10%|█         | 357M/3.50G [00:02<00:26, 121MB/s][A
Downloading (…)of-00002.safetensors:  11%|█         | 377M/3.50G [00:03<00:25, 123MB/s][A
Downloading (…)of-00002.safetensors:  11%|█▏        | 398M/3.50G [00:03<00:25, 122MB/s][A
Downloading (…)of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:03<00:25, 122MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 440M/3.50G [00:03<00:25, 121MB/s][A
Downloading (…)of-00002.safetensors:  13%|█▎        | 461M/3.50G [00:03<00:24, 125MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:03<00:23, 126MB/s][A
Downloading (…)of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:04<00:23, 130MB/s][A
Downloading (…)of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:04<00:22, 130MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 545M/3.50G [00:04<00:22, 130MB/s][A
Downloading (…)of-00002.safetensors:  16%|█▌        | 566M/3.50G [00:04<00:22, 130MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 587M/3.50G [00:04<00:24, 118MB/s][A
Downloading (…)of-00002.safetensors:  17%|█▋        | 608M/3.50G [00:04<00:23, 124MB/s][A
Downloading (…)of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:05<00:22, 130MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▊        | 650M/3.50G [00:05<00:22, 127MB/s][A
Downloading (…)of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:05<00:21, 129MB/s][A
Downloading (…)of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:05<00:22, 125MB/s][A
Downloading (…)of-00002.safetensors:  20%|██        | 713M/3.50G [00:05<00:22, 123MB/s][A
Downloading (…)of-00002.safetensors:  21%|██        | 734M/3.50G [00:05<00:23, 117MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:06<00:23, 117MB/s][A
Downloading (…)of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:06<00:23, 118MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:06<00:22, 123MB/s][A
Downloading (…)of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:06<00:21, 123MB/s][A
Downloading (…)of-00002.safetensors:  24%|██▍       | 839M/3.50G [00:06<00:21, 125MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▍       | 860M/3.50G [00:06<00:21, 123MB/s][A
Downloading (…)of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:07<00:21, 122MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▌       | 902M/3.50G [00:07<00:21, 122MB/s][A
Downloading (…)of-00002.safetensors:  26%|██▋       | 923M/3.50G [00:07<00:26, 97.1MB/s][A
Downloading (…)of-00002.safetensors:  27%|██▋       | 954M/3.50G [00:07<00:20, 127MB/s] [A
Downloading (…)of-00002.safetensors:  28%|██▊       | 975M/3.50G [00:07<00:19, 128MB/s][A
Downloading (…)of-00002.safetensors:  28%|██▊       | 996M/3.50G [00:08<00:19, 128MB/s][A
Downloading (…)of-00002.safetensors:  29%|██▉       | 1.02G/3.50G [00:08<00:19, 128MB/s][A
Downloading (…)of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:08<00:18, 131MB/s][A
Downloading (…)of-00002.safetensors:  30%|███       | 1.06G/3.50G [00:08<00:18, 130MB/s][A
Downloading (…)of-00002.safetensors:  31%|███       | 1.08G/3.50G [00:08<00:21, 113MB/s][A
Downloading (…)of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:09<00:22, 109MB/s][A
Downloading (…)of-00002.safetensors:  32%|███▏      | 1.12G/3.50G [00:09<00:23, 99.9MB/s][A
Downloading (…)of-00002.safetensors:  33%|███▎      | 1.14G/3.50G [00:09<00:22, 105MB/s] [A
Downloading (…)of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:09<00:20, 112MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:09<00:21, 110MB/s][A
Downloading (…)of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:10<00:22, 103MB/s][A
Downloading (…)of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:10<00:21, 105MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 1.25G/3.50G [00:10<00:20, 110MB/s][A
Downloading (…)of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:10<00:19, 116MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:10<00:18, 121MB/s][A
Downloading (…)of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:10<00:17, 125MB/s][A
Downloading (…)of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:11<00:18, 117MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:11<00:17, 125MB/s][A
Downloading (…)of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:11<00:16, 130MB/s][A
Downloading (…)of-00002.safetensors:  40%|███▉      | 1.39G/3.50G [00:11<00:15, 133MB/s][A
Downloading (…)of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:11<00:15, 136MB/s][A
Downloading (…)of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:11<00:15, 132MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:11<00:14, 137MB/s][A
Downloading (…)of-00002.safetensors:  42%|████▏     | 1.48G/3.50G [00:12<00:14, 137MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.50G/3.50G [00:12<00:16, 124MB/s][A
Downloading (…)of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:12<00:15, 125MB/s][A
Downloading (…)of-00002.safetensors:  44%|████▍     | 1.54G/3.50G [00:12<00:15, 128MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▍     | 1.56G/3.50G [00:12<00:15, 127MB/s][A
Downloading (…)of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:12<00:14, 130MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▌     | 1.60G/3.50G [00:13<00:15, 126MB/s][A
Downloading (…)of-00002.safetensors:  46%|████▋     | 1.63G/3.50G [00:13<00:14, 129MB/s][A
Downloading (…)of-00002.safetensors:  47%|████▋     | 1.65G/3.50G [00:13<00:14, 125MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 1.67G/3.50G [00:13<00:15, 115MB/s][A
Downloading (…)of-00002.safetensors:  48%|████▊     | 1.69G/3.50G [00:13<00:14, 127MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 1.71G/3.50G [00:13<00:13, 128MB/s][A
Downloading (…)of-00002.safetensors:  49%|████▉     | 1.73G/3.50G [00:14<00:14, 125MB/s][A
Downloading (…)of-00002.safetensors:  50%|█████     | 1.75G/3.50G [00:14<00:13, 131MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:14<00:14, 122MB/s][A
Downloading (…)of-00002.safetensors:  51%|█████     | 1.79G/3.50G [00:14<00:14, 118MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:14<00:13, 125MB/s][A
Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.84G/3.50G [00:14<00:12, 130MB/s][A
Downloading (…)of-00002.safetensors:  53%|█████▎    | 1.86G/3.50G [00:15<00:12, 129MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▎    | 1.88G/3.50G [00:15<00:12, 131MB/s][A
Downloading (…)of-00002.safetensors:  54%|█████▍    | 1.90G/3.50G [00:15<00:12, 129MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▍    | 1.92G/3.50G [00:15<00:11, 132MB/s][A
Downloading (…)of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:15<00:11, 132MB/s][A
Downloading (…)of-00002.safetensors:  56%|█████▌    | 1.96G/3.50G [00:15<00:11, 132MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 1.98G/3.50G [00:16<00:11, 129MB/s][A
Downloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:16<00:11, 133MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.02G/3.50G [00:16<00:11, 134MB/s][A
Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.04G/3.50G [00:16<00:11, 130MB/s][A
Downloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:16<00:10, 131MB/s][A
Downloading (…)of-00002.safetensors:  60%|█████▉    | 2.09G/3.50G [00:16<00:10, 134MB/s][A
Downloading (…)of-00002.safetensors:  60%|██████    | 2.11G/3.50G [00:17<00:10, 127MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████    | 2.13G/3.50G [00:17<00:10, 130MB/s][A
Downloading (…)of-00002.safetensors:  61%|██████▏   | 2.15G/3.50G [00:17<00:10, 132MB/s][A
Downloading (…)of-00002.safetensors:  62%|██████▏   | 2.17G/3.50G [00:17<00:09, 134MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:17<00:09, 134MB/s][A
Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.21G/3.50G [00:17<00:09, 132MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.23G/3.50G [00:17<00:09, 130MB/s][A
Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:18<00:10, 124MB/s][A
Downloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:18<00:09, 123MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.30G/3.50G [00:18<00:09, 124MB/s][A
Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:18<00:09, 130MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.34G/3.50G [00:18<00:09, 129MB/s][A
Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.36G/3.50G [00:19<00:09, 125MB/s][A
Downloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:19<00:08, 127MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▊   | 2.40G/3.50G [00:19<00:08, 123MB/s][A
Downloading (…)of-00002.safetensors:  69%|██████▉   | 2.42G/3.50G [00:19<00:08, 129MB/s][A
Downloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:19<00:08, 128MB/s][A
Downloading (…)of-00002.safetensors:  70%|███████   | 2.46G/3.50G [00:19<00:07, 131MB/s][A
Downloading (…)of-00002.safetensors:  71%|███████   | 2.49G/3.50G [00:19<00:07, 131MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.51G/3.50G [00:20<00:08, 124MB/s][A
Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.53G/3.50G [00:20<00:07, 128MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.55G/3.50G [00:20<00:10, 94.2MB/s][A
Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:20<00:08, 113MB/s] [A
Downloading (…)of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:20<00:06, 139MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:21<00:06, 141MB/s][A
Downloading (…)of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:21<00:06, 133MB/s][A
Downloading (…)of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:21<00:06, 134MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:21<00:05, 136MB/s][A
Downloading (…)of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:21<00:06, 132MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:21<00:05, 135MB/s][A
Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:22<00:05, 130MB/s][A
Downloading (…)of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:22<00:05, 124MB/s][A
Downloading (…)of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:22<00:05, 129MB/s][A
Downloading (…)of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:22<00:05, 128MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:22<00:05, 132MB/s][A
Downloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:22<00:04, 131MB/s][A
Downloading (…)of-00002.safetensors:  82%|████████▏ | 2.87G/3.50G [00:23<00:04, 132MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:23<00:04, 130MB/s][A
Downloading (…)of-00002.safetensors:  83%|████████▎ | 2.92G/3.50G [00:23<00:04, 133MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:23<00:04, 124MB/s][A
Downloading (…)of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:23<00:04, 125MB/s][A
Downloading (…)of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:23<00:04, 127MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:24<00:04, 125MB/s][A
Downloading (…)of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:24<00:03, 130MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:24<00:03, 132MB/s][A
Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:24<00:03, 133MB/s][A
Downloading (…)of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:24<00:03, 136MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:24<00:03, 128MB/s][A
Downloading (…)of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:24<00:02, 129MB/s][A
Downloading (…)of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:25<00:02, 130MB/s][A
Downloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:25<00:02, 126MB/s][A
Downloading (…)of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:25<00:02, 126MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:25<00:02, 127MB/s][A
Downloading (…)of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:25<00:02, 131MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.25G/3.50G [00:25<00:01, 130MB/s][A
Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:26<00:01, 128MB/s][A
Downloading (…)of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:26<00:01, 130MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:26<00:01, 123MB/s][A
Downloading (…)of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:26<00:01, 127MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:26<00:01, 132MB/s][A
Downloading (…)of-00002.safetensors:  96%|█████████▋| 3.38G/3.50G [00:26<00:00, 131MB/s][A
Downloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:27<00:00, 131MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 3.42G/3.50G [00:27<00:00, 132MB/s][A
Downloading (…)of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:27<00:00, 132MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 3.46G/3.50G [00:27<00:00, 133MB/s][A
Downloading (…)of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:27<00:00, 134MB/s][A
Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:27<00:00, 133MB/s][ADownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:27<00:00, 126MB/s]
Downloading shards: 100%|██████████| 2/2 [01:44<00:00, 47.90s/it]Downloading shards: 100%|██████████| 2/2 [01:44<00:00, 52.17s/it]
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.51s/it]
Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]Downloading generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 48.3kB/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 6319.83 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:12:09.106782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:12:09.885225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:12:17.256453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.266050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.268452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.283461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.285823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.288149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.466129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.467678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.469057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:12:17.470446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.45s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8
2024-03-09 12:15:01.695710: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:15:02.459937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:15:09.642985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.652606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.654983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.669723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.672065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.674368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.852794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.854348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.856061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:15:09.857438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.17s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 5998.84 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:17:53.102884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:17:53.875584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:18:01.372912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.382917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.385256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.400691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.403018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.405306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.588580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.590066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.591416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:18:01.593130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.25s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+8+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8
2024-03-09 12:20:50.622306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:20:51.398219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:20:58.648223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.657696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.660046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.675182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.677520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.679823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.881098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.882592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.883947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:20:58.885323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.14s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 5906.02 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.026 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:23:45.716081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:23:46.476389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:23:53.887475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:53.897143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:53.899497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:53.914936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:53.917266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:53.919578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:54.100695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:54.102213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:54.103593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:23:54.104988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 22.00s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-8/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.021 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._loop_check_status(
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    local_handle = request()
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py", line 803, in deliver_internal_messages
        return self._deliver_network_status(status)return self._deliver_internal_messages(internal_message)

  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 500, in _deliver_network_status
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 506, in _deliver_internal_messages
    return self._deliver_record(record)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 449, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    return self._deliver_record(record)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 449, in _deliver_record
    self._sock_client.send_record_publish(record)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    handle = mailbox._deliver_record(record, interface=self)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    self.send_server_request(server_req)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
        interface._publish(record)self._send_message(msg)

  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
        self._sock_client.send_record_publish(record)self._sendall_with_error_handle(header + data)

  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
        self.send_server_request(server_req)sent = self._sock.send(data)

  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
BrokenPipeError: [Errno 32] Broken pipe
    self._send_message(msg)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
1+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2
2024-03-09 12:26:46.695143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:26:47.451033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:26:54.620102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.629773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.632161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.646891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.649264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.651587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.832466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.834020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.835406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:26:54.836791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.44s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 12648.15 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:29:39.796599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:29:40.578686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:29:47.883011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:47.892643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:47.895020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:47.909408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:47.911781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:47.914108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:48.102868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:48.104410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:48.105768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:29:48.107138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.23s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2
2024-03-09 12:32:34.085069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:32:34.836879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:32:42.075183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.084656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.086993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.102627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.104945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.107234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.294255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.295766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.297125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:32:42.298500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.42s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 12370.02 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:35:26.596721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:35:27.361534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:35:34.662540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.671794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.674179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.689149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.691500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.693830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.874924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.876483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.877853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:35:34.879241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.50s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+2+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2
2024-03-09 12:38:21.992390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:38:22.755145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:38:29.978861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:29.988514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:29.990896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.006062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.008425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.010753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.193067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.194674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.196389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:38:30.197772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.19s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 12108.62 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:41:16.002975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:41:16.762435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:41:24.116604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.125972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.128360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.143011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.145360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.147685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.328600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.330140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.331513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:41:24.332897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.39s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-2/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4
2024-03-09 12:44:07.893019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:44:08.660345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:44:15.902021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:15.911721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:15.914096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:15.928910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:15.931282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:15.933599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:16.115009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:16.116599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:16.117970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:44:16.119345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.08s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 9455.23 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:47:01.503715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:47:02.278575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:47:09.566012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.575477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.577853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.592273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.594619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.596944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.781458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.783014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.784414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:47:09.785812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.11s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4
2024-03-09 12:49:56.483160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:49:57.264999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:50:04.469296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.478780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.481192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.496135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.498501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.500820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.680705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.682287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.683674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:50:04.685062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.24s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 9360.53 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:52:51.196714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:52:51.956462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:52:59.250550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.260131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.262477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.277641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.279986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.282289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.464164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.465708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.467070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:52:59.468439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.35s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+4+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4
2024-03-09 12:55:50.305514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:55:51.099313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:55:58.487388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.496775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.499112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.514113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.516432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.518729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.709941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.711435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.712797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:55:58.714174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.86s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 9268.40 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.026 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 12:58:45.485089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 12:58:46.256228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 12:58:53.681833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.691938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.694281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.709568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.711884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.714177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.899084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.900626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.901987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 12:58:53.903352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.42s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-4/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
1+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16
2024-03-09 13:01:41.513511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:01:42.299213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:01:49.592214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.602260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.604670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.619978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.622357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.624685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.814271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.815832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.817232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:01:49.818648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.93s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3443.83 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3303.47 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 13:04:33.100093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:04:33.864599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:04:41.048970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.057960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.060310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.075650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.077973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.080279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.265353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.266852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.268216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:04:41.269924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.02s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-1-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
13+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16
2024-03-09 13:07:26.108323: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:07:26.878413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:07:34.057751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.067247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.069597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.084793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.087168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.089482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.272752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.274310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.275688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:07:34.277085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 19.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.81s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3596.42 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3444.57 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 13:10:25.011043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:10:25.797819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:10:33.097905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.107496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.109912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.125197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.127555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.129881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.316020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.317579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.318959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:10:33.320368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 20.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.77s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-13-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
42+16+meta-llama/Llama-2-7b-hf+mvp
./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16
2024-03-09 13:13:21.006893: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:13:21.789344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:13:29.017378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.027064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.029441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.045120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.047475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.049791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.244652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.246249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.247637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:13:29.249045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:33<00:33, 33.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 20.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.66s/it]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3477.62 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 3334.57 examples/s]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.026 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
2024-03-09 13:16:17.503400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-09 13:16:18.273798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-09 13:16:25.687204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.697440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.699831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.714971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.717330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.719639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.907932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.909494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.910869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-09 13:16:25.912269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79087 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:09:00.0, compute capability: 8.0
/mnt/data/mvp/src/utils/funcs.py:257: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('accuracy')
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:32<00:32, 32.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.01s/it]
textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.
textattack: Logging to CSV at path ./checkpoints/trec/meta-llama/Llama-2-7b-hf/swap_labels/icl-seed-42-shot-16/swap_labels_log.csv
  0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
wandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedTraceback (most recent call last):
  File "main.py", line 85, in <module>
    attacker(args)
  File "/mnt/data/mvp/src/test.py", line 301, in attacker
    attacker.attack_dataset()
  File "/src/textattack/textattack/attacker.py", line 441, in attack_dataset
    self._attack()
  File "/src/textattack/textattack/attacker.py", line 170, in _attack
    raise e
  File "/src/textattack/textattack/attacker.py", line 168, in _attack
    result = self.attack.attack(example, ground_truth_output)
  File "/src/textattack/textattack/attack.py", line 444, in attack
    goal_function_result, _ = self.goal_function.init_attack_example(
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 68, in init_attack_example
    result, _ = self.get_result(attacked_text, check_skip=True)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 79, in get_result
    results, search_over = self.get_results([attacked_text], **kwargs)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 96, in get_results
    model_outputs = self._call_model(attacked_text_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 219, in _call_model
    outputs = self._call_model_uncached(uncached_list)
  File "/src/textattack/textattack/goal_functions/goal_function.py", line 165, in _call_model_uncached
    batch_preds = self.model(batch)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 209, in forward
    input_ids, attention_mask, _  = self.get_updated_input_ids(input_ids, attention_mask, **kwargs)
  File "/mnt/data/mvp/src/models/model_wrapper.py", line 184, in get_updated_input_ids
    input_ids, attention_mask, input_indices = insert_icl_prompts(self, self.tokenizer, self.args.model_type, text_input_list, self.template, self.len_templates, use_all = (self.args.num_template != -2) or self.mode!="train", icl_examples = self.icl_examples)
  File "/mnt/data/mvp/src/utils/model_utils.py", line 328, in insert_icl_prompts
    inputs = tokenizer.batch_encode_plus(prompts, padding=True, truncation=True, return_tensors="pt")
  File "/transformers/src/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    return self._batch_encode_plus(
  File "/transformers/src/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
